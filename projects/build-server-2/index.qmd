---
title: "Building my own Server Part 2 â€” Software"
date: "2023-8-2"
categories: [linux]
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    code-block-background: true
execute:
  freeze: true
---



# Software

## Software Suite

I want an easy install, but I also want lots of features. Here are some things I have looked at:

-   Proxmox VE
-   Xen Orchestra
-   Openstack
-   [Canonicals LXD UI](https://github.com/canonical/lxd-ui)
-   Ovirt
-   Harvester
-   OpenVZ

## Openstack

Currently, openstack appeals to me a lot. Although I originally wanted to do a bare metal install, I now realize that that is too time consuming and not realistic, so I am most likely going to use one of the automated methods of installation.

[Kolla ansible](https://docs.openstack.org/kolla-ansible/latest/)

They have an easy [deployment guide](https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html) for an all in one node, perfect for my single server.

I will definitely not use every service, but I do want to use openstack because of the sheer number of services it offers. Openstack offers every single feature of other virtualization platforms, at the cost of complexity. Here are the features that made me decide I needed that complexity.

### Skyline/Horizon

Openstack has a previous generation web ui, horizon, and a newer generation web ui, skyline. These web ui's offer all the features of other web based virtualization platforms like proxmox, but they also let you configure the other things of openstack.

![](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fwww.itzgeek.com%2Fwp-content%2Fuploads%2F2015%2F07%2FOpenStack-Configure-Horizon-Instance-Console.jpg&f=1&nofb=1&ipt=c31c95fcc86ecd2a334a098fddcf51cbdad6b8c3b3d1bd1257b28dbc4b7cb17e&ipo=images)

And they have some special features, like giving you a visual layout of network topology.

![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fz6ftW7fUdp4%2Fmaxresdefault.jpg&f=1&nofb=1&ipt=4e02ee4eb9751d8f6d2ae8693ca223d0626deef5779ec2fe01e614be210a4c5e&ipo=images)

### Multi tenancy.

The most important feature of openstack, in my opinion, is it's multi tenant architechture. Unliek proxmox, which is designed for a single organization, openstack is designed in such a way that you can create extra users, which get their own allocation of resources.

So when I go to college, if anyone wants a VPS to play around in, I can just allocate them a few resources, and then they get their own web ui for playing with servers and networking.

Many public cloud services are actually using openstack in the background for it's public tenant architecture. Openstack's dashboards can be rebranded to be your own company:

![](https://www.scalecloud.co.uk/wp-content/uploads/2017/02/vnc-console.png)

### Bare metal nodes

Openstack saves a lot of trouble by immensely abstracting almost all the manual work that goes into putting together a modern cloud.

It takes it a step further, with it's ability to treat physical, bare metal machines, as virtual machines, even automating the provisioning the same way you can do so for a virtual machine.

[The docs](https://docs.openstack.org/newton/admin-guide/baremetal.html) make it sound complex, but it really isn't all that. By leveraging the [nova]() component of openstack, which abstracts the backend drivers of virtual machines (qemu-kvm, xen, or even [hyper-v](https://docs.openstack.org/ocata/config-reference/compute/hypervisor-hyper-v.html)) can be used as backend drivers for nova.

However, when combined with [ironic](https://docs.openstack.org/ironic/latest/) openstack's service to configure bare metal nodes, nova can also use bare metal as a driver for compute nodes. This integrates further with services like magnum...

### Magnum

Magnum is openstack's kubernetes-as-as-service. It provisions nodes using nova, with kubernetes clusters.

Now here is where I get greedy. Do I need kubernetes? No. Will kubernetes even be useful on a single server setup? No. Do I want kubernetes? Yes.

Here is a video demonstration, where someone uses the web ui to create a cluster using magnum.

{{< video https://www.youtube.com/watch?v=QvS4nDYE2r0&t=160s >}}

In addition to that, because openstack magnum uses openstack heat, which provisions nodes from templates, it can be used to do things like auto install nvidia drivers and container runtime.

{{< video https://www.youtube.com/embed/P4z2Hdh0l4g start="262" >}}

This video is a bit outdated, so heat and magnum are much more mature since then, and have only gained more features.

### Api and Automation

Openstack is designed from the ground up to be a cloud. It has first class support for their api, and everything that can be done from the UI can also be done from either the command line, or the [api](https://docs.openstack.org/api-quick-start/).

This advanced api makes it easier for other software to interact with openstack. For example, the rancher kubernetes cluster manager supports [openstack](https://rke.docs.rancher.com/config-options/cloud-providers/openstack) as a cloud provider. It is capable of requesting nodes, provisioning them, and then setting up a kubernetes cluster entirely from the rancher web gui.

### Zun

Openstack zun is the container service for openstack. It doesn't run them in virtual machines, but instead directly on metal. It's likely that when I want to run containerized services for actual usage, this is what I will be using instead of kubernetes since I will be using a single server, and thus won't be able to get the benefits of kubernetes. The benefit I see form using containers is that because I have an nvidia desktop gpu, I won't be able to use vgpu, a feature that lets you divide the gpu between virtual machines. However, containers have no such limitation.

## Installing Openstack

I've decided to use the kolla-ansible project to install openstack. It works by using ansible to deploy docker containers, which the openstack services run in.

They have a quickstart guide:

<https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html>

And the ansible playbook can be found here:

<https://opendev.org/openstack/kolla-ansible>

And they provide a [sample ansible inventory](https://opendev.org/openstack/kolla-ansible/src/branch/master/ansible/inventory/multinode) for the all in one node.

I do not need all of those features. I pretty much just want virtualized compute, networking, containers, and kubernetes. I don't need things like an S3 compatible block storage, a relational database, or an app store. Okay, but maybe I want them.

I will do more research into what I want and what I don't want, and edit the ansible playbook accordingly.

However, this method of deployment seems to require two NIC's (network interface cards). I think I have both, but just in case, I noted another method of deployment, [openstack ansible](https://docs.openstack.org/openstack-ansible/latest/) (yeah the naming isn't the greatest), which deploys openstack without using containers, it actaully installs and configures the services on the host operating system.

The openstack ansible [All in one](https://docs.openstack.org/openstack-ansible/latest/user/aio/quickstart.html) deployment, doesn't seem to have the same requirement of two NIC's, which I do have. 


### Operating system

But first, I do need to select on an operating system. Openstack is flexible and versatile, and it can be installed on multiple operating systems. 

I was originally going to choose a RHEL compatible distro, but then RHEL made changes put the status of those in limbo.

I am currently deciding between:

-   Ubuntu
-   Debian
-   RHEL (via a developer subscription)
-   A RHEL rebuild
    -   Rocky Linux
    -   Alma Linux
    -   One of the academic distros, like scientific linux
-   Centos Stream

The important thing is that it's a stable release distro with automatic updates. I don't want to have to do too much manual maintainence. Ideally, I also want this distro to have newerish packages, in case I want to do some tinkering with the underlying OS, and I also want the distro to have a stable release that goes on for longer than my college years. From what I've heard, upgrading from one release of an OS to another can be a frustrating process, and I don't want to have to do this while I'm in the middle of school. 

The RHEL rebuilds do appeal to me, but they also come with extra complications, like selinux that I don't really want to have to deal with. 

But after much deliberation, I've decided on Rocky Linux. Rocky Linux 9 is officially supported by [kolla ansible](https://docs.openstack.org/kolla-ansible/latest/user/support-matrix.html). In addition to that, Rock Linux 9 will be [supported for a good deal of time](https://endoflife.date/rocky-linux), with the release being officially supported for 3 years and a bit, and that release will continue to receive security updates for 5 more years after that. More than enough to last me through college. 

The install was very simple. I thought I would experience issues because of the Nvidia GPU, as I had been having issues with graphical monitor output with other distros, but I didn't. A GUI appeared for me, and the install process was exceedingly simple, even simpler than debian or other distros I've tried. Of course, the disadvantage was that I couldn't configure everything, like there was no option to set up users other than root, but it was very quick.

Now, I have RHEL installed. 

To make management easier, I will install my favorite web administration system, [cockpit](../../guides/cockpit-setup/). This will also enable me to do remote management operations with a gui, things like partitioning the disks. 

Now that I have rocky linux installed, I can install openstack using kolla-ansible.

### Kolla-ansible

I will be following the [quick start guide](https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html)

I will briefly go over what I am doing here, edited for my usecase. The first few steps are copy and pasted from the guide linked above. 

```{.default}
sudo dnf install git python3-devel libffi-devel gcc openssl-devel python3-libselinux
```

```{.default}
pip install 'ansible>=6,<8'
```

```{.default}
pip install git+https://opendev.org/openstack/kolla-ansible@master
```

```{.default}
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla
```

```{.default}
cp -r /home/moonpie/.local/share/kolla-ansible/etc_examples/kolla/* /etc/kolla
```

```{.default}
cp /home/moonpie/.local/share/kolla-ansible/ansible/inventory/all-in-one /home/moonpie/
```

Now, this is the initial setup. I need to customize these things to my liking. 

Usernames and passwords a are easy, but I need to make sure the networking is right, especially since my setup will be so unorthodox. Because I am setting this up on my home network, I won't have public internet access, as my router using [NAT](https://en.wikipedia.org/wiki/Network_address_translation)

### Networking

It requires that I set my two network interfaces to two things: default network interface for everything else, and the neutron external interface. 

The neutron external interface can be any physical interface, but internally, it is referred to as br-ex. This interface is responsible for letting the virtual machines interact with the internet. 

Anyway, I had a spare router lying around, and I flashed it with [freshtomato](https://www.freshtomato.org/) for some more advanced features. I hooked it up to my existing router, so it would be behind NAT, but now I have extra ethernet ports, so I can have a private subnet with my laptop, and the openstack stuff.

I was curious if tomato offered an easy way to expose services to the internet, from behind NAT, something like integration with Cloudfare's tunnels, but they didn't.

However, another idea has occured upon me: Why not just host the public parts of openstack... on the public. I could rent a VPS, and host openstack neutron, and maybe the openstack dashboards on the public. 

For example, my current vps provdider [webdock](https://webdock.io/en/docs/general-information/billing-and-pricing/additional-products#additional-/-reserved-ip-addresses), gives out a range (/124) of ipv6 addresses, 16 total. Based on the pricing on that page, 1.75 Euros for an additional ipv4 address, I think I can safely assume that ipv4 addresses are more expensive, out of what I am wiling to spend, because of their scarcity. 

However, if I can install openstack neutron on a cheap vps, thenH I will be able to give public ipv6 access to the virtual machines, which sounds like a very neat setup. 

Since my home server won't have public internet access I am guessing I have to start by creating a virtual network that links the two machines together, that way they can see eachother and cluster. 

Openstack has some interesting diagrams: <https://docs.openstack.org/install-guide/environment-networking.html>. But I can't find anything conclusive. 

Since I have a router running tomato, I am thinking that I can vpn the entire router into the other machine, so that the vps I am accessing can access everything in the tomato subnet, meaning I won't have to configure the server itself. The downside is that everything will be vpn'ed, but with a 4 TB upload limit, I'm not too concerned about that right now.

But sadly, webdock seems to be sold out of kvm vps's, which have better compatibility with docker, so I will probably go looking for another platform. 

Anyway, tomato seems to have a wireguard client installed, so I will use that, since wireguard is the fastest vpn client/server available. I found a [nice guide](https://golb.hplar.ch/2019/01/expose-server-vpn.html) on setting up pure wireguard. However, it doesn't discuss connecting the subnet of a router to the vps. I did find another [guide](https://gist.github.com/insdavm/b1034635ab23b8839bf957aa406b5e39) that did. Now this guide is older, and some people criticized it for various reasons. However, it does tell me the name of what I am looking for: Site to site.

I found a much simpler guide on [ubuntu's website](https://ubuntu.com/server/docs/wireguard-vpn-site2site). Rather than A-B-C, I only need the A-B which this offers. In addition to that, there are no iptables rules on this guide. 

The minimum specs required for openstack neutron are: ???. The docs suck.


VPS provider overview: 

| Name | CPU | Ram (GB) | Price (/month) | Ipv6 | OS |
|-|-|-|-|-|-|
| Contabo | 4 | 8 | $10.29 | [18 quintillion](https://contabo.com/en/customer-support/faq/#how-can-i-use-ipv6-on-my-server-at-contabo) | Rocky | 

Okay, contabo wins. I was gonna do an actual comparison between multiple vps providers, but contabo has great specs, and it is trusted by people when I have asked around. 

I don't want to spend money, but this is a nice deal. 

Also, contabo offers 32 TB out, and unlimiited in. This is definitely a very good choice. 

After visiting <https://test-ipv6.com/> and realizing that my home residential wifi does not have ipv6 enabled by default (although there is an option in the router settings), I realize that the college dorms may not have ipv6 support. In that case, vpning into the remote server, which does have ipv6 support, would give me ipv6 support. 

Now that I have a vps, I am trying to get wireguard working, by testing with my laptop I started out with the guide from ubuntu, but that doesn't work. In addition to not having access to subnets I can only access on my laptop from my server, not even the vpn is working correctly:

```{.default}
moonpie@lizard:~> curl --interface wgA ifconfig.me
curl: (7) Failed to connect to ifconfig.me port 80 after 1 ms: Couldn't connect to server
```

I found another [guide](https://www.laroberto.com/remote-lan-access-with-wireguard/) but yet again, it's another A-B-C guide. 

I found a promising [stackexchange answer](https://unix.stackexchange.com/questions/602267/wireguard-connection-between-two-lans-with-wireguard-boxes-behind-routers) with an interesting commmand I have tried:

```{.default}
ip route add 192.168.122.0/24 via 10.10.9.0 dev wgB
```

Where the 192.168.122.0/24 is the subnet I am trying to expose.

Okay, at this piont, I understand what I want to do pretty well. I think I will use the A-B-C guides as a template, except I only need A-B, and the router needs to eb the device that is configured not to have a permanent, static ip address, like the clients are. 

I got lucky when I was browsing lemmy, and I found [forum comment](https://programming.dev/comment/1943135) related to my exact issue, which then links to a [stackexchange question](https://unix.stackexchange.com/questions/638889/make-local-resources-available-when-connected-to-wireguard-vpn), in which the top answer uses information they sourced from this [guide](https://www.procustodibus.com/blog/2020/11/wireguard-point-to-site-config/)

I attempted to follow the latter guide, using a simple setup. My laptop, would be my "router", and I was attempting to expose the vlan subnet created by the libvirt virtual machine manager to my remote server, a vps hosted on contabo. 

Ironically, I got the connection to work, but in the wrong direction. My vps could ping my remote server, but my remote server could not ping the virtual machines on my laptop. Interestingl, it could ping my laptop using the ip of the virbr0 virtual network adapter that libvirt creates for a vlan. 

I'm guessing that this didn't work because I'm doing this somewhat backwards, with the device that the 

I am guessing because I am doing this somewhat backwards, where the device exposing the lan is behind nat, whereas it is the other way around in the guides that I have seen.

```{.default}
root@vmi1403809 ~]# ping 192.168.122.1
PING 192.168.122.1 (192.168.122.1) 56(84) bytes of data.
64 bytes from 192.168.122.1: icmp_seq=1 ttl=64 time=46.5 ms
[root@vmi1403809 ~]# ping 192.168.122.201
PING 192.168.122.201 (192.168.122.201) 56(84) bytes of data.
From 10.10.9.0 icmp_seq=1 Destination Port Unreachable
```

The first is the ip address of my laptop, and the second is the ip address of my debian virtual machine, which is running on my laptop. 


```{.default}
moonpie@lizard:~/vscode/moonpiedumplings.github.io> curl --insecure  http://192.168.122.201:9090
<html><head><title>Moved</title></head><body>Please use TLS</body></html>
moonpie@lizard:~/vscode/moonpiedumplings.github.io> curl --insecure  http://192.168.122.201:9090 --interface wgA
curl: (7) Failed to connect to 192.168.122.201 port 9090 after 0 ms: Couldn't connect to server
````

Curl has a feature to bind to a specific interface, and when I try to test a connection using the wgA interface, it can't connect. At first, I'm annoyed, but in hindsight, this makes sense. Only through the virbr0 interface, can I access the virtual machiens. And the virtual machines, are actually behind their own NAT. They are not public, so why would I be able to ping them? 

Libvirt offers [several networking options](https://jamielinux.com/docs/libvirt-networking-handbook/). Bridged is where all virtual machines get public ip addresses. Routed is similar, but they don't get their own interface, and it works on wirelessly connected devices. And finally, NAT, what I am using does not give virtual machines publicly accessible ip addresses. 

Instead of running it on my actual laptop, I will try to run the vpn on the debian virtual machine instead. 

I set up wireguard again, one side on the vps I am renting:

```{.ini filename="/etc/wireguard/wgB.conf"}
[Interface]
Address = 10.10.9.1/31
PostUp = wg set %i private-key /etc/wireguard/%i.key
ListenPort = 51000

[Peer]
# alpha site
PublicKey = e763iTZmmMcx7HufUOi5vzmQJ5ZhYBuuqnXh/2ViBjA=
AllowedIPs = 10.10.10.0/24,10.10.9.0/31,192.168.122.0/24

```

And another side on my virtual machine:

```{.ini filename="/etc/wireguard/wgA.conf"}
[Interface]
Address = 10.10.9.1/31
PostUp = wg set %i private-key /etc/wireguard/%i.key
ListenPort = 51000

[Peer]
# alpha site
PublicKey = e763iTZmmMcx7HufUOi5vzmQJ5ZhYBuuqnXh/2ViBjA=
AllowedIPs = 10.10.10.0/24,10.10.9.0/31,192.168.122.0/24
```

And the wireguard default route didn't work, so I removed it, and then added the route manually using the ip route tool.

When everything was done, my VPS could ping the ip address of my virtual machine, but not my laptop. 

```{.default}
[root@vmi1403809 ~]# ping 192.168.122.201 # virtual machine ip
PING 192.168.122.201 (192.168.122.201) 56(84) bytes of data.
[root@vmi1403809 ~]# ping 192.168.122.1 # laptop ip
PING 192.168.122.1 (192.168.122.1) 56(84) bytes of data.
# it just sits here forever.
```

Well, this is better than a strange error. However, I don't know where to go next from here.


Alright, I figured it out. I've sourced my information from two different links: 

The instructions for wg-quick I used are in the [arch wiki's wireguard article](https://wiki.archlinux.org/title/WireGuard).

However, I also needed to look at the arch wiki's [NAT article](https://wiki.archlinux.org/title/Internet_sharing#Enable_packet_forwarding). From that article, if you have docker enabled, you must take some extra steps that make thigns more complex. So I disabled docker, followed the instructions for NAT there. 

And on my remote server, I ran `ip route add xx.xx.xx.xx/yy via zz.zz.zz.zz` where xx is the subnet you want to access remotely, and zz is the ip address of the remote device with access to that subnet. 

Anyway, I find this deeply ironic. I originally wanted to deploy wireguard to my router to avoid any kind of complex networking, but because I didn't have physical access to my router, I was testing on machines with more complex networking, which caused things to not work. This affirms my decision to set up wireguard on my router, rather than on my server, especially since kolla-ansible uses docker to deploy. 

Anyway, I will have to look into split tunneling or whatnot, because I may not want to vpn everything. 

The [freshtomato](https://wiki.freshtomato.org/doku.php/wireguard_on_freshtomato) wiki contains info on how to configure wireguard, including doing things like having it start on boot. 

Sadly, the wg-quick command doesn't seem to be available on the router, but that's not really a big deal. I can just write a small script which sets this up, using the instructions from the arch wiki article, or even just using wg-quick, because it tells you what steps it is taking to up the wireguard interface. 


Here is the wg-quick file on my server:

```{.ini}
[Interface]
Address = 10.10.9.1/31
PostUp = wg set %i private-key /etc/wireguard/%i.key
ListenPort = 51000

[Peer]
# alpha site
PublicKey = e763iTZmmMcx7HufUOi5vzmQJ5ZhYBuuqnXh/2ViBjA=
AllowedIPs = 10.10.10.0/24,10.10.9.0/31,192.168.122.0/24
```

Interestingly, I didn't have to add a specific route using `ip`. Once nat is set up correctly on one device, the, it somehow knows where to go. I only need to add allowed ips. 

Here is configuration file for my "router" (still just testing with virtual machines). 

```{.ini filename="/etc/wireguard/wgA.conf"}
[Interface]
PreUp = sysctl net.ipv4.ip_forward=1
PreUp = iptables -t nat -A POSTROUTING -o enp1s0 -j MASQUERADE
PreUp = iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
PostUp = wg set %i private-key /etc/wireguard/%i.key
PostUp = iptables -A FORWARD -i %i -o enp1s0 -j ACCEPT
Address = 10.10.9.0/31
ListenPort = 51000

[Peer]
# Remote Server
PublicKey = pxCx+cEs6hoI4EE+XdE4lQiLkJRbG4JGQwXz6d/hZDM=
AllowedIPs = 10.10.9.0/31
Endpoint = 154.12.245.181:51000
PersistentKeepalive = 25
```

Here are the steps my "router"  takes when I use `wg-quick up wgA`


```{.default}
root@debian:/home/moonpie# wg-quick up wgA
[#] sysctl net.ipv4.ip_forward=1
net.ipv4.ip_forward = 1
[#] iptables -t nat -A POSTROUTING -o enp1s0 -j MASQUERADE
[#] iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
[#] ip link add wgA type wireguard
[#] wg setconf wgA /dev/fd/63
[#] ip -4 address add 10.10.9.0/31 dev wgA
[#] ip link set mtu 1420 up dev wgA
[#] wg set wgA private-key /etc/wireguard/wgA.key
[#] iptables -A FORWARD -i wgA -o enp1s0 -j ACCEPT
```

In this case, wgA is the wireguard interface, and enp1s0 is the interface I want to configure NAT on, so that I can access the subnet behind it. 

Actually, wg-quick appears to just be a bash script, and I might be able to transfer that over to freshtomato. 

The best feature of this wireguard setup is that, since it doesn't route everything through the vpn tunnel like I was initially expecting, meaning I won't have to worry about bandwidth or speed, since I won't be routing all traffic through my router to a remote router. 

In addition to all this, I can use nmap to make sure that my remote server can actually see the other services, as they are behind a kind of NAT.

```{.default}
root@vmi1403809 wireguard]# nmap -sV 192.168.122.1
Starting Nmap 7.91 ( https://nmap.org ) at 2023-08-18 11:29 PDT
Nmap scan report for 192.168.122.1
Host is up (0.068s latency).
Not shown: 997 closed ports
PORT     STATE SERVICE         VERSION
53/tcp   open  domain          dnsmasq 2.89
8000/tcp open  http            SimpleHTTPServer 0.6 (Python 3.11.4)
9090/tcp open  ssl/zeus-admin?
1 service unrecognized despite returning data. If you know the service/version, please submit the following fingerprint at https://nmap.org/cgi-bin/submit.cgi?new-service :
SF-Port9090-TCP:V=7.91%T=SSL%I=7%D=8/18%Time=64DFB8E1%P=x86_64-redhat-linu
...
... For some reason nmap can't detect cockpit so it's just 20 lines of this nonsense.
...
20\x20\x20\x20\x20\x20}\n
SF:\x20\x20\x20\x20\x20\x20\x20\x20p\x20");

Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 186.81 seconds
```

Nmap detects my python http server, and cockpit. 

Alright, next I want to configure firewall usage. I don't want the remote wireguard server to have access to everything on my "home" network. I am likely going to do this by creating vlan's, which can be [wifi](https://hobo.house/2016/03/10/build-secure-vlan-networks-with-shibby-tomato-router-firmware/), by creating virtual wireless, or [ethernet](https://wiki.freshtomato.org/doku.php/advanced-vlan), by making it so that certain ethernet ports are trapped in a vlan. With this setup, I can ensure that my server is kept isolated from the rest of my devices, in case it gets compromised. (Although doing it this way is kinda pointless since I may be using a VPN on my devices, running through that same server to get around college wifi restrictions, if they exist).


So I setup wireguard on my server:


```{.ini filename="/etc/wireguard/wg-stack.conf"}
[Interface]
Address = 10.10.11.1/24
PostUp = wg set %i private-key /etc/wireguard/%i.key
ListenPort = 51000

[Peer]
# My router
PublicKey = mFyQQk8/w7AhLSEtJKkcNhMNLPcyBMFHu02TI+OUj2Y=
AllowedIPs = 10.10.11.0/24,192.168.17.0/24
```


And here is the wg-stack.sh script that will be run on my router:

```{.default filename="wg-stack.sh"}
# enabled by default
#echo 1 > /proc/sys/net/ipv4/conf/br2/forwarding

#iptables -t nat -A POSTROUTING -o br2 -j MASQUERADE
#iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
modprobe wireguard
ip link add wg-stack type wireguard
wg setconf wg-stack /jffs/wg-stack-setconf
ip -4 address add 10.10.11.0/24 dev wg-stack
ip link set mtu 1420 up dev wg-stack
iptables -A FORWARD -i wg-stack -o br2 -j ACCEPT
```

Some of the rules were enabled by default, so I commented them out. 

And here is the wg-stack-setconf file that the above script calls upon:

```{.ini filename="/jffs/wg-stack-setconf"}
[Interface]
PrivateKey = NO
#Address = 10.10.0.0/24
ListenPort = 51000

[Peer]
# Contabo VPS
PublicKey = xO2fVY8uh4SDx5VH+24Mxx+WIXSnfY3Vw9CDBW7cMnY=
AllowedIPs = 10.10.11.0/24
Endpoint = 154.12.245.181:51000
PersistentKeepalive = 25
```

I have to manually specify the private key, because the `wg setconf` can't run abitrary commands. 

This works... one way.

```{.default}
root@unknown:/jffs# ping 10.10.11.1
PING 10.10.11.1 (10.10.11.1): 56 data bytes
64 bytes from 10.10.11.1: seq=0 ttl=64 time=43.491 ms
root@unknown:/jffs# curl 10.10.11.1:8000
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
....
.... I ran a simple http server to test if my router could see ports of my server.
....
</html>
```

However, when my server attempts to ping or curl my router, it simply waits forever. I think this may have something to do with firewalll rules, where the router is refusing to respond to even ping connections. 

After doing some more research, it appears my iptables rules deny all connections to my router, and I can attempt to access subnets other than my router itself, but they just redirect to my router:

```{.default}
[root@vmi1403809 ~]# ping 192.168.17.152
PING 192.168.17.152 (192.168.17.152) 56(84) bytes of data.
64 bytes from 192.168.17.152: icmp_seq=1 ttl=63 time=41.9 ms
[root@vmi1403809 ~]# curl 192.168.17.152:8000
curl: (7) Failed to connect to 192.168.17.152 port 8000: No route to host
[root@vmi1403809 ~]# nmap -sV 192.168.17.152
Starting Nmap 7.91 ( https://nmap.org ) at 2023-08-19 20:18 PDT
Nmap scan report for 192.168.17.152
Host is up (0.043s latency).
Not shown: 999 filtered ports
PORT   STATE  SERVICE VERSION
22/tcp closed ssh

Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 5.53 seconds
```

The nmap scan is especially strange, considering I am using my laptop to test, which doesn't have an ssh server running. My router however, does, but it also has an http server running (the web management interface), which nmap doesn't seem to see. I am guessing my router is not playing the role of NAT properly. 

Nope, I was wrong. The problem was not my router firewall, but rather my computer firewall. I forgot that unlike Arch Linux, the Arch Linux based linux distribution I was using, CachyOS did come with a firewall enabled. 

After disabling the firewall:

```{.default}
[root@vmi1403809 ~]# nmap -sV 192.168.17.152
Starting Nmap 7.91 ( https://nmap.org ) at 2023-08-20 03:21 PDT
Nmap scan report for 192.168.17.152
Host is up (0.042s latency).
Not shown: 999 closed ports
PORT     STATE SERVICE VERSION
8000/tcp open  http    SimpleHTTPServer 0.6 (Python 3.11.4)

Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 7.78 seconds
```

For testing purposes, I run a simple http server using `python -m http.server` which launches this service in port 8000. 










