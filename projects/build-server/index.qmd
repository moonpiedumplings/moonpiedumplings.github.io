---
title: "Building my own Server"
date: "2023-8-1"
categories: [linux]
format:
  html:
    code-fold: true
    code-summary: "Show the code"
    code-block-background: true
execute:
  freeze: true
---

# Hardware

## Selection

I wanted a machine for experimenting with devops and deep learning. That means plenty of ram, cpu, and a modest gpu.

I was very careful with my selection, and here is what I ended up with:

-   Dell Precision Tower Server 7910, with 2X intel xeon E5 2687 v4. Came with 32 GB of ram.
-   More memory, when added, I will get a total of 128 GB of ECC memory. 
-   Nvidia rtx 4070 GPU, 12 GB vram.

## Building

Here is the inside of my server:

![](images/insides.jpg)

Anyway, I need to figure out where to put the GPU. The computer has several PCIe slots, and I want the fastest one.

Up top:

![](images/toppcie.jpg)

And below:

![](images/belowpcie.jpg)

I need to figure out what each of these pcie ports is. What do the numbers and color mean?

To catalog them: 

* Three black PCIe3x16 slots (all 75W)
* One blue PCIe3x16 75W + ext 225 w
* One black PCIe2x4 25 W
* One tan PCI slot

I think it is safe to assume that the color is related to wattage, not PCIe protocol. Since the computer comes with the necessary power plug that the nvidia gpu wants, it is safe to assume that any of the PCIe3x16 are optimal, although I will try to place the gpu in the best spot for cooling.

I found a [forum post](https://www.dell.com/community/Desktops-General-Read-Only/What-is-the-meaning-of-the-blue-PCIe-slot-on-the-Dell-T7910/td-p/5153034) ([wayback archive](https://web.archive.org/web/20230804052739/https://www.dell.com/community/Desktops-General-Read-Only/What-is-the-meaning-of-the-blue-PCIe-slot-on-the-Dell-T7910/td-p/5153034)) where someone asked this exact question. 

An unsourced answer replied that the blue pcie slot was the primary gpu slot, so since I only have one gpu, that is where I put it. 

I found the [manual](https://www.dell.com/support/manuals/en-us/precision-t7910-workstation/precision_t7910_om_pub/additional-information?guid=guid-ba746105-ecbd-4bb4-9fcc-b78681607aab&lang=en-us) for my system online, but it doesn't seem to label each pci port in images. 




I also did research into some youtube videos.


[General review](https://www.youtube.com/watch?v=jPDYT0XW6Rs)

[Memory upgrading process](https://www.youtube.com/watch?v=b499mGPEwVg)

The important thing to note about the memory upgrading process is that the memory shroud (cover) does not interfere with other processes.

## GPU

However, I am trying to add the gpu, so I can get monitor output, so I can run testing suites to ensure the computer is in proper working order. 

However, the gpu does not seem to have screw slots for holding it in place. Rather than that, the entire thing rests on a single screw, as shown below.

![](images/singlescrew.jpg)

But the GPU, is inserted: 

![](images/gpuin.jpg)


After plugging the gpu power in: 

![](images/gpuwithpower.jpg)


I can turn the machine on and get monitor output:

![](images/monitoroutput.jpg)

Next step is to run the system diagnostics on the memory I currently have. Thankfully, I have an extra keyboard I can connect it to. It's a small bluetooth one, closer to a remote than something meant for typing, but it will do nicely.

Anyway, I also did more research, attempting to find more manuals and whatnot. 

Linked on [this page](https://www.dell.com/support/home/en-us/product-support/product/precision-t7910-workstation/docs) I found a [proper manual](https://dl.dell.com/topicspdf/precision-t7910-workstation_owners-manual_en-us.pdf) which gave visual instructions on how to do things like remove the memory shroud. 

Too bad it still doesn't detail how to install the gpu. I just know I'm not doing it right, the servers I worked with at cirrascale had a screw that would actually hold the gpu in place. It doesn't make sense to simply have the gpu be resting. I may simply give up, and find some nuts and attatch them to the back of the screws, locking the gpu into place. 

Later, I found out that it was very simple, and I'm frustrated I missed it. I simply had to reverse the direction of the screw, and screw it in from the other side, like so:

![](images/gpuscrewed.jpg)


## Hard Drives

Now that I had the GPU installed I could get video output and see what the BIOS is saying. I installed two hard drives, and got an error: "Alert Hard drive not found". 

I decided to pause on the hard drives, and run the built in memory test. The memory, and cpu tests ran without error, however, I got an error about not being able to find the hard drives

![](images/noharddrives.jpg)

Just to make sure that the issue was a hardware issue, and not the BIOS merely complaining about not seeing a bootable device, I booted into a live USB and ran lsblk:

![](images/manjarolsblk.jpg)

I messed around with moving around the hard drive positions, unplugging and plugging in cables, no dice. This issue is definitely at the hardware level, however, and there are several causes:

* Bad cable. I only have a single cable, an SAS connector on both sides. 
* No power to the hard drives. I will test to see if they light up later. 
* Motherboard not working. Maybe the motherboard plug isn't working, but I tried both of them...

It's definitely not defective hard drives, as this happens with both of the hard drives I am testing with, both of which are new. However, they rely on the same cable and same power supply (they attatch to a seperate board which has a cable connecting that to the motherboard and another connecting it to the power supply)

Upon doing some further research, I found a related [youtube video](https://youtu.be/Qa3qvzEjDyE?t=197). According to this video, the port I am trying to attatch this to is controlled by the RAID controller, so RAID must be enabled, although I can use RAID 0 for RAID without any of the special RAID features or complications. 

# Sofware

## Operating system

I was originally going to choose a RHEL compatible distro, but then RHEL made changes put the status of those in limbo. 

I am currently deciding between:

* Ubuntu
* Debian
* RHEL (via a developer subscription)
* A RHEL rebuild
  - Rocky Linux
  - Alma Linux
  - One of the academic distros, like scientific linux
* Centos Stream

The important thing is that it's a stable release distro with automatic updates. I don't want to have to do too much manual maintainence. 


## Software Suite

I want an easy install, but I also want lots of features. Here are some things I have looked at:

* Proxmox VE
* Xen Orchestra
* Openstack 
* [Canonicals LXD UI](https://github.com/canonical/lxd-ui)
* Ovirt
* Harvester
* OpenVZ

## Openstack


Currently, openstack appeals to me a lot. Although I originally wanted to do a bare metal install, I now realize that that is too time consuming and not realistic, so I am most likely going to use one of the automated methods of installation. 

[Kolla ansible](https://docs.openstack.org/kolla-ansible/latest/)


They have an easy [deployment guide](https://docs.openstack.org/kolla-ansible/latest/user/quickstart.html) for an all in one node, perfect for my single server. 

I will definitely not use every service, but I do want to use openstack because of the sheer number of services it offers. Openstack offers every single feature of other virtualization platforms, at the cost of complexity. Here are the features that made me decide I needed that complexity.

### Skyline/Horizon

Openstack has a previous generation web ui, horizon, and a newer generation web ui, skyline. These web ui's offer all the features of other web based virtualization platforms like proxmox, but they also let you configure the other things of openstack.

![](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Fwww.itzgeek.com%2Fwp-content%2Fuploads%2F2015%2F07%2FOpenStack-Configure-Horizon-Instance-Console.jpg&f=1&nofb=1&ipt=c31c95fcc86ecd2a334a098fddcf51cbdad6b8c3b3d1bd1257b28dbc4b7cb17e&ipo=images)

And they have some special features, like giving you a visual layout of network topology. 

![](https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fi.ytimg.com%2Fvi%2Fz6ftW7fUdp4%2Fmaxresdefault.jpg&f=1&nofb=1&ipt=4e02ee4eb9751d8f6d2ae8693ca223d0626deef5779ec2fe01e614be210a4c5e&ipo=images)


### Multi tenancy. 

The most important feature of openstack, in my opinion, is it's multi tenant architechture. Unliek proxmox, which is designed for a single organization, openstack is designed in such a way that you can create extra users, which get their own allocation of resources.

So when I go to college, if anyone wants a VPS to play around in, I can just allocate them a few resources, and then they get their own web ui for playing with servers and networking.

Many public cloud services are actually using openstack in the background for it's public tenant architecture. Openstack's dashboards can be rebranded to be your own company: 

![](https://www.scalecloud.co.uk/wp-content/uploads/2017/02/vnc-console.png)

### Bare metal nodes

Openstack saves a lot of trouble by immensely abstracting almost all the manual work that goes into putting together a modern cloud. 

It takes it a step further, with it's ability to treat physical, bare metal machines, as virtual machines, even automating the provisioning the same way you can do so for a virtual machine. 

[The docs](https://docs.openstack.org/newton/admin-guide/baremetal.html) make it sound complex, but it really isn't all that. By leveraging the [nova]() component of openstack, which abstracts the backend drivers of virtual machines (qemu-kvm, xen, or even [hyper-v](https://docs.openstack.org/ocata/config-reference/compute/hypervisor-hyper-v.html)) can be used as backend drivers for nova. 

However, when combined with [ironic](https://docs.openstack.org/ironic/latest/) openstack's service to configure bare metal nodes, nova can also use bare metal as a driver for compute nodes. This integrates further with services like magnum...


### Magnum

Magnum is openstack's kubernetes-as-as-service. It provisions nodes using nova, with kubernetes clusters.

Now here is where I get greedy. Do I need kubernetes? No. Will kubernetes even be useful on a single server setup? No. Do I want kubernetes? Yes. 

Here is a video demonstration, where someone uses the web ui to create a cluster using magnum.

{{< video https://www.youtube.com/watch?v=QvS4nDYE2r0&t=160s >}}



### An api

Openstack is much more automatable than solutions like proxmox, becuase it is designed to be a cloud. Because of this, software like terraform, or perhaps custom software written to suit the needs of the users can interact with the [openstack api](https://docs.openstack.org/api-quick-start/)







