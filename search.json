[
  {
    "objectID": "playground/include-text-in-quarto/index.html",
    "href": "playground/include-text-in-quarto/index.html",
    "title": "Can I include text from other files in quarto?",
    "section": "",
    "text": "Just playing around with embeding quarto again.\n&lt;embed src=\"data/test.txt\"&gt;\n\n&lt;object type=\"text/plain\" data=\"file.txt\"&gt;&lt;/object&gt;\n\n\nDoesn’t look quite right.\n\n\nquarto include function\n\n\n\"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n\n\n“Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.”\nThis works. But how can I get formatted text? I would like to include code from files outside the main file, for ease of editing and modularity.\n&lt;script src=\" https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js \"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js\"&gt;&lt;/script&gt;\n\n&lt;link href=\" https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css \" rel=\"stylesheet\"&gt;\n\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import numpy as np&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\n\nimport numpy as np\nBit weird, but it works. Now how can I include external content from external files with this?\n\n\n\n\n\n\n\n\ndata/test.txt\n\n\n\nSince quarto can run python, and you can force the output of that to render to markdown with a few options:\n#| output: asis\n#| echo: true\n\nbacktick = \"`\"\na = \"\"\nwith open('data/python.py', 'r') as f:\n    file_contents = f.read()\n    a = file_contents\n\nprint(f'''\n\n{backtick * 3}{{.python filename=\"data/python.py\"}}\n{a}\n{backtick * 3}\n''')\nOf course, the proper quarto syntax is:\n```{python}\n#| options here\n# python code here\n```\n\n\ndata/python.py\n\n# A function to check if a number is prime\ndef is_prime(n):\n  # If n is less than 2, it is not prime\n  if n &lt; 2:\n    return False\n  # Loop from 2 to the square root of n\n  for i in range(2, int(n**0.5) + 1):\n    # If n is divisible by i, it is not prime\n    if n % i == 0:\n      return False\n  # If no divisor is found, it is prime\n  return True\n\n# Test the function with some numbers\nprint(is_prime(2)) # True\nprint(is_prime(3)) # True\nprint(is_prime(4)) # False\nprint(is_prime(5)) # True\nprint(is_prime(6)) # False\n\nIt works! Perhaps an awkward way of doing this, but it works. I can probably even get code from remote repos, since it’s all python. There are other ways to do this, but I didn’t want to do have to rely on any extensions or external dependencies."
  },
  {
    "objectID": "playground/interactive-python-tutorial/index.html",
    "href": "playground/interactive-python-tutorial/index.html",
    "title": "Experiments with Running python in the browser",
    "section": "",
    "text": "Original\nI started on this on 2023-5-22, and paused it since nothing interesting came out of it.\nMy goal is an in browser python editor I can embed into a blog. I want to create a sort of interactive python tutorial.\nCode is ran client side, so don’t try to crash any server or anything like that.\n\n\nRecent updates\nThere was a new quarto extension, enabling you to embed pyodide python code blocks in documents: https://github.com/coatless-quarto/pyodide\n\nPlease enable JavaScript to experience the dynamic code cell content on this page.\nThis works, but the blocks don’t show up properly. In addition to that, the input function doesn’t work.\nBut still, if I want to do a simple python tutorial, this would be pretty doable. In addition to that, I could also do block posts about other interesting things in python.\n\n\nOlder work below\nThis might not render, since many of it is raw html elements.\nLots of rot happened since I first played with this, and I disabled all of this stuff since it doesn’t load, and instead prevents the newer work from loading.\n\n                        \n                    \nAwww, the input function doesn’t seem to work.\nShift + enter/return to evaluate code of the below.\n\nThis kinda works, but also has issues. Input fucntion still doesn’t work.\nShift + enter/return to evaluate below. Or double click.\n\n   import numpy as np\n   np.random.rand(5)\n\nThis is cool, but the code isn’t editable. It just resets itself for some reason.\nMaybe this will work?\n\n\n  Open the project Untitled Project in LiveCodes\n\n\n\n\n\n\n\nyeet = input(\"test\")\n\nprint(yeet)"
  },
  {
    "objectID": "projects/compiling-kasmvnc-on-nixos/index.html",
    "href": "projects/compiling-kasmvnc-on-nixos/index.html",
    "title": "Compiling KasmVNC on NixOS",
    "section": "",
    "text": "This is a living document, for the time being. Rather than a complete blogpost, this is a tracker of my progress, as well as a quick reference I can come back to.\n\nWhat is this?\nKasmweb is a software to create remote desktops, that exist within docker containers, and allow users to access them, all from a browser based GUI.\nKasmvnc is the VNC server part of kasm web, a custom fork of previous existing features, enhanced with more performance, and most importantly, a web native setup. Incompatible with existing VNC protocols, the only way to access Kasmvnc is through the http/https serive it offers, the web based VNC ui.\nKasmVNC is an amazing piece of software, but development for it is not truly, fully public. This is present in their build system. Their build system is a series of bash scripts, that call docker containers, which call more bash scripts, to finally compile the software, and package it, all in one.\nAs part of the scripts they have to compile kasm, lots of static linking happens. They do this because not all distros package the most updated, performant versions of the libraries that kasmvnc uses.\nBut Nixos, the distro I want to package KasmVNC for, does. In addition to that, it is completely incompatible with the hacked together build system that kasm uses. In order to package KasmVNC for Nixos, I must reverse engineer their build system, and bit by bit, port it to NixOS.\n\n\nNixos Build System\nHow the nixos build system works. I will do later, but here I will document, step by step, how nixos builds a package.\n\n\nKasmVNC’s Build System\nReverse engineering KasmVNC’s build system.\nThe below is what I neeed for the compiliation commands to work. I don’t know where Kasm runs these commands, or similar equivalents, this is just what I’ve figured out.\nPerhaps I need to only run make in the KasmVNC/unix directory?\n\ngit clone https://github.com/TigerVNC/tigervnc\n\ncd tigervnc\n\ngit clone https://github.com/kasmtech/KasmVNC\n\ncd KasmVNC\n\ncp ..vncserver .vncserver # this sets up build environment. I will still need to check if every single one of these things are necessary, but this works for now\nThe build script can be found here\nBut from this build script, I don’t think all of it is necessary. Below, I will extract what commands are actually needed, from all the fluff, cruft, and hacks.\n\ncmake -D CMAKE_BUILD_TYPE=RelWithDebInfo . -DBUILD_VIEWER:BOOL=OFF \\\n  -DENABLE_GNUTLS:BOOL=OFF\n\nmake\nBuilds end up in KasmVNC/unix/\nExcept the preliminary builds don’t work. They error:\n~/vscode/tigervnc/KasmVNC/unix master ?1 ❯ ./vncserver \n\nCan't locate List/MoreUtils.pm in @INC (you may need to install the List::MoreUtils module) (@INC contains: /usr/lib/perl5/5.36/site_perl /usr/share/perl5/site_perl /usr/lib/perl5/5.36/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5/5.36/core_perl /usr/share/perl5/core_perl) at ./vncserver line 38.\nBEGIN failed--compilation aborted at ./vncserver line 38.\nThe above is probably because a perl library is missing. After attempting to install the missing library using pacman -S perl-list-moreutils I get a different error.\n\n~/vscode/tigervnc/KasmVNC/unix master ?1 ❯ ./vncserver \n\nCan't locate KasmVNC/CliOption.pm in @INC (you may need to install the KasmVNC::CliOption module) (@INC contains: /usr/lib/perl5/5.36/site_perl /usr/share/perl5/site_perl /usr/lib/perl5/5.36/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5/5.36/core_perl /usr/share/perl5/core_perl) at ./vncserver line 42.\nBEGIN failed--compilation aborted at ./vncserver line 42.\nObviously, this won’t work. I must figure out why KasmVNC pacakges perl packages, where it puts them by default, and how to package them for Nix.\nAlright, the vncserver command appears to be in the git repo, and rather than being a binary, it is a perl wrapper script to start an xvnc server. from the script:\nuse KasmVNC::CliOption;\nuse KasmVNC::ConfigKey;\nuse KasmVNC::PatternValidator;\nuse KasmVNC::EnumValidator;\nuse KasmVNC::Config;\nuse KasmVNC::Users;\nuse KasmVNC::TextOption;\nuse KasmVNC::TextUI;\nuse KasmVNC::Utils;\nuse KasmVNC::Logger;\nThese perl modules/libraries can be found in KasmVNC/unix/KasmVNC\nSo that is what is necessary for the vncserver script to run. But is this script really necessary? Based on the names of the perl libraries, this script might not be adding any core functionalities to kasmvnc, only things like additional command line options, or loggers.\nI need to find where this script runs kasmvnc, and also where the actual kasmvnc binary is.\nI think their fork of xvnc is located in KasmVNC/unix/xserver/hw/vnc/xvnc.c.\nBut I get compilation errors:\n\n[nix-shell:~/vscode/tigervnc/KasmVNC]$ make \n[  3%] Built target os\n[ 13%] Built target rdr\n[ 30%] Built target network\n[ 31%] Built target Xregion\n[ 78%] Built target rfb\n[ 80%] Built target tx\n[ 81%] Built target unixcommon\n[ 81%] Linking CXX executable vncconfig\n/nix/store/178vvank67pg2ckr5ic5gmdkm3ri72f3-binutils-2.39/bin/ld: cannot find -lturbojpeg: No such file or directory\ncollect2: error: ld returned 1 exit status\nmake[2]: *** [unix/vncconfig/CMakeFiles/vncconfig.dir/build.make:157: unix/vncconfig/vncconfig] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:610: unix/vncconfig/CMakeFiles/vncconfig.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\nI don’t know why this happens. For those who don’t know, make pretty much calls a set of scripts, called Makefiles. I will need to find where in these scripts, the error commands are run.\nWeirdly, I can’t reproduce outside of the build script:\n\n~/vscode/tigervnc/KasmVNC master ?1 ❯ ld -lsdsakdfj\nld: cannot find -lsdsakdfj: No such file or directory\n~/vscode/tigervnc/KasmVNC master ?1 ❯ ld -lturbojpeg\nld: warning: cannot find entry symbol _start; not setting start address\n~/vscode/tigervnc/KasmVNC master ?2 ❯ ld -ljpeg     \nld: warning: cannot find entry symbol _start; not setting start address\n~/vscode/tigervnc/KasmVNC master ?2 ❯ ld -ljpeg-turbo\nld: cannot find -ljpeg-turbo: No such file or directory\n~/vscode/tigervnc/KasmVNC master ?1 ❯ \nI suspect the make scripts have some hacks that change working directory, or otherwise hide my installation of libjpeg.\nWhen commenting out the part of the KasmVNC/Cmakelists.txt file that appears to be related to libjpeg, the error when I run make changes.\n[ 95%] Building CXX object unix/vncconfig/CMakeFiles/vncconfig.dir/vncconfig.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/Surface.cxx:23:10: fatal error: FL/Fl_RGB_Image.H: No such file or directory\n   23 | #include &lt;FL/Fl_RGB_Image.H&gt;\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake[2]: *** [tests/CMakeFiles/fbperf.dir/build.make:104: tests/CMakeFiles/fbperf.dir/__/vncviewer/Surface.cxx.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/Surface_X11.cxx:26:10: fatal error: FL/Fl_RGB_Image.H: No such file or directory\n   26 | #include &lt;FL/Fl_RGB_Image.H&gt;\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake[2]: *** [tests/CMakeFiles/fbperf.dir/build.make:118: tests/CMakeFiles/fbperf.dir/__/vncviewer/Surface_X11.cxx.o] Error 1\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:31:10: fatal error: FL/Fl.H: No such file or directory\n   31 | #include &lt;FL/Fl.H&gt;\n      |          ^~~~~~~~~\nThis is probably missing libraries.\nAfter installing fltk (pacman -S fltk), this error goes away, and I get a new, even harder to comprehend error.\n\n[ 97%] Building CXX object tests/CMakeFiles/decperf.dir/decperf.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx: In constructor ‘PlatformPixelBuffer::PlatformPixelBuffer(int, int)’:\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:64:29: error: ‘uint8_t’ was not declared in this scope\n   64 |   setBuffer(width, height, (uint8_t*)xim-&gt;data,\n      |                             ^~~~~~~\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:38:1: note: ‘uint8_t’ is defined in header ‘&lt;cstdint&gt;’; did you forget to ‘#include &lt;cstdint&gt;’?\n   37 | #include \"PlatformPixelBuffer.h\"\n  +++ |+#include &lt;cstdint&gt;\n   38 | \n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:64:37: error: expected primary-expression before ‘)’ token\n   64 |   setBuffer(width, height, (uint8_t*)xim-&gt;data,\n      |                                     ^\nI suspect I have the wrong version of the tigervnc source code/libraries. I will need to investigate where Kasm’s build system downloads these vncviewer libraries from.\nAfter editing the errored file, to include the library:\nWithin KasmVNC/vncviewer/PlatformPixelBuffer.cxx:\n\n#include &lt;cstdint&gt;\n\nI get a different error.\n\n[ 94%] Building CXX object tests/CMakeFiles/fbperf.dir/__/vncviewer/PlatformPixelBuffer.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx: In constructor ‘PlatformPixelBuffer::PlatformPixelBuffer(int, int)’:\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:66:3: error: ‘setBuffer’ was not declared in this scope; did you mean ‘setbuffer’?\n   66 |   setBuffer(width, height, (uint8_t*)xim-&gt;d﻿tion. \nI think it’s likely that I have the wrong version of tigervnc or something. I will try to see how Kasm’s build scripts set this up.\nOkay, I will attempt to create a visual graph, documenting each step of KasmVNC’s build system, to build an ubuntu package. I will have hyperlinks to each of the scripts/dockerfiles, or other pieces of the github repo. Currently still working on figuring out how to use Graphviz.\n\nEntrybuild-tarballBuildWWWdockerfile.ubuntu.build\n\n\n\n\n\n\n\n\n\n\n\n\nEntry\n\n\nEntry.\n Starts out at KasmVNC/builder\n\n\n\n\n\nBuildPackage\n\n\nI will run the 'builder/build-package ubuntu bionic' command. \n This isn't the only command available, but for an example.\n\n\n\n\n\nEntry-&gt;BuildPackage\n\n\n\n\n\nBuildTarball\n\n\nbuild-tarball ubuntu bionic\n\n\n\n\n\nBuildPackage-&gt;BuildTarball\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildTarball\n\n\n\nwww\n\n\nif some condition, then \n build and run dockerfile.www.build\n\n\n\n\n\ndockerbuild\n\n\nBuild and run the appropiate Dockerfile, \n which in this case dockerfile.ubuntu_bionic.build\n\n\n\n\n\nwww-&gt;dockerbuild\n\n\n\n\n\nincomplete\n\n???, currrently in progress.\n\n\n\nBuildTarball\n\n\nbuild-tarball\n\n\n\n\n\nBuildTarball-&gt;www\n\n\n\n\n\ndockerbuild-&gt;incomplete\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildWWW\n\n\n\nbuildwwwsh\n\n\nENTRYPOINT [ \"/src/build_www.sh\" ]\n\n\n\n\n\nCOPY kasmweb/ /src/www/\n\nCOPY kasmweb/ /src/www/\n\n\n\nCOPY builder/build_www.sh /src/\n\nCOPY builder/build_www.sh /src/\n\n\n\nCOPY kasmweb/ /src/www/-&gt;COPY builder/build_www.sh /src/\n\n\n\n\n\nWORKDIR /src/www\n\nWORKDIR /src/www\n\n\n\nCOPY builder/build_www.sh /src/-&gt;WORKDIR /src/www\n\n\n\n\n\nRUN npm install\n\nRUN npm install\n\n\n\nWORKDIR /src/www-&gt;RUN npm install\n\n\n\n\n\nRUN npm install-&gt;buildwwwsh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildonUbuntu\n\n\n\nEntry\n\nEntry here.\nThis entire phase happens in a docker container\n\n\n\ndevpendencies\n\nInstall some dependencies.\nNotably, tightvncserver\n\n\n\nEntry-&gt;devpendencies\n\n\n\n\n\nMakeinstalls\n\n\nbuild and install webp and libjpeg turbo\n\n\n\n\n\ndevpendencies-&gt;Makeinstalls\n\n\n\n\n\nInstall some more libs\n\nInstall some more libs\n\n\n\nMakeinstalls-&gt;Install some more libs\n\n\n\n\n\nbuildsh\n\n\nbuild.sh\n This is the command the built docker container runs\n\n\n\n\n\nInstall some more libs-&gt;buildsh\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote about dependencies install\n\n\n\n\n\nFor some reason they use multiple build phases for the same step of installing packages. In addition to that, they don’t clean the apt cache between build stages.\n\n\n\n\n\n\n\n\n\nNote about webp and libjpeg-turbo\n\n\n\n\n\nThey make and install — in the build dockerfile. I will end up skipping this step, as nix packages these, but why do they do that?\n\n\n\n\n\n\nI will expand on this, and organize it further. But based on these beginnings, kasmvnc appears to be a perl script that either starts a sepreate webserver or serves a webserver (if there is a perl native way to do this?), which appears to be based on novnc, while it also starts the vnc server, which is based on tigervnc.\nHowever, I am still confused about one thing: Where does it download the vnc server source code.\n\n\nThe easy way\nAfter I packaged quarto, I realized that I can actually package the kasmvnc binary using nix. I have decided to do this for now.\nHere is first attempt for this. Now, kasmvnc’s packaging system is weird, in that they do not offer a binary tarball for their packages. So, I have decided to convert their alpine package into something I can use, because based on a cursory look into all the packages, it seems to be the easiest to package for Nix/Nixos.\n\n\nFirst try!\n\n{\n    stdenv,\n    lib,\n    fetchurl,\n    makeWrapper,\n} :\n\nstdenv.mkDerivation rec {\n  pname = \"kasmvnc\";\n  version = \"1.1.0\";\n  src = fetchurl {\n    url = \"https://github.com/kasmtech/KasmVNC/releases/download/v${version}/kasmvnc.alpine_317_x86_64.tgz\";\n    sha256 = \"sha256-j/3PUwBd8XygBKYfFdAwN15cwxDPf3vbEwbLy1laxSU=\";\n  };\n\n  nativeBuildInputs = [\n  ];\n\n  patches = [\n  ];\n\n  postPatch = ''\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n  '';\n\n  installPhase = ''\n      runHook preInstall\n\n      mkdir -p $out/bin $out/share $out/man $out/etc $out/lib\n\n      echo here\n      ls\n      ls local/bin\n\n      mv local/etc/* $out/etc\n      mv local/share/* $out/share\n      mv local/man/* $out/man\n      mv local/lib/* $out/lib\n      mv local/bin/* $out/bin\n\n      runHook preInstall\n  '';\n\n  meta = with lib; {\n    description = \"Kasmvnc\";\n    longDescription = ''\n        Long description here\n    '';\n    homepage = \"\";\n    changelog = \"https://github.com/kasmtech/KasmVNC/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ moonpiedumplings ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nThis is my first attempt at a derivation. I’ve converted this one from the quarto derivation I built, which is also on my blog.\nI’m installing it using a simple shell.nix, that uses the nix callPackage function to build the kasmvnc nix package, and make it available in my current shell.\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n    kasmvnc = pkgs.callPackage ./kasmvnctest.nix {};\nin\n    pkgs.mkShell {\n        packages = [ kasmvnc ];\n    }\nTo run kasmvnc, I run the vncserver command.\nHowever, I get an error."
  },
  {
    "objectID": "projects/nixos-vps/index.html",
    "href": "projects/nixos-vps/index.html",
    "title": "Automating my server config, first nix, then ansible",
    "section": "",
    "text": "I want to automate my install with nixos"
  },
  {
    "objectID": "projects/nixos-vps/index.html#deployment-attempts",
    "href": "projects/nixos-vps/index.html#deployment-attempts",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Deployment attempts",
    "text": "Deployment attempts\nHowever, I now want to deploy this. I am currently using webdock.io as my VPS provider, which doesn’t officially support nixos. So I want to convert an existing ubuntu install in place to a nixos installation, without physical access to the machine.\nI have tried multiple tools and they have not worked, but I am now trying nixos-anywhere for this.\nHere is my config as of 7/14/2023\n\n\nShow .nix file\n\n\n\nflake.nix\n\n\n\nflake.nix\n\n{\n  inputs.nixpkgs.url = github:NixOS/nixpkgs;\n  inputs.disko.url = github:nix-community/disko;\n  inputs.disko.inputs.nixpkgs.follows = \"nixpkgs\";\n\n  outputs = { self, nixpkgs, disko, ... }@attrs: {\n    nixosConfigurations.hetzner-cloud = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n      specialArgs = attrs;\n      modules = [\n        ({modulesPath, ... }: {\n          imports = [\n            (modulesPath + \"/installer/scan/not-detected.nix\")\n            #(modulesPath + \"/profiles/qemu-guest.nix\") #not a qemu vm\n            # try to fit the lxd-vm config in here\n            #https://github.com/mrkuz/nixos/blob/c468d9772b7a84ab8e38cc4047bc83a3a443d18f/modules/nixos/virtualization/lxd-vm.nix#L4\n            disko.nixosModules.disko\n          ];\n          disko.devices = import ./disk-config.nix {\n            lib = nixpkgs.lib;\n          };\n          boot.loader.efi.efiSysMountPoint = \"/boot/efi\";\n          boot.loader.grub = {\n            devices = [ \"nodev\" ];\n            efiSupport = true;\n            #efiInstallAsRemovable = true;\n          };\n          networking = {\n            usePredictableInterfaceNames = false;\n            interfaces = {\n              eth0 = {\n                useDHCP = false;\n                ipv4.addresses = [{ address = \"93.92.112.130\"; prefixLength = 24; }];\n              };\n            };\n            defaultGateway = {\n                interface = \"eth0\";\n                address = \"93.92.112.1\";\n            };\n          };\n          services = {\n            openssh = {\n                enable = true;\n                settings = {\n                    PasswordAuthentication = false;\n                    #PermitRootLogin = \"prohibit-password\"; # this is the default\n                };\n                openFirewall = true;\n            };\n            cockpit = {\n                enable = true;\n                openFirewall = true;\n            };\n          };\n\n          users.users.moonpie = {\n            isNormalUser = true;\n            extraGroups = [ \"wheel\" ]; # Enable ‘sudo’ for the user.\n            #packages = with pkgs; [];\n            initialHashedPassword = \"$y$j9T$ZGDLrUl6VP4AiusK96/tx0$1Xb1z61RhXYR8lDlFmJkdS8zyzTnaHL6ArNQBBrEAm0\"; # may replace this with a proper secret management scheme later\n            openssh.authorizedKeys.keys = [ \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDEQDNqf12xeaYzyBFrOM2R99ciY9i0fPMdb4J64XpU3Tjv7Z5WrYx+LSLCVolzKktTfaSgaIDN8+vswixtjaAXkGW2glvTD0IwaO0N4IQloov3cLZa84I7lkj5jIkZiXJ2zHJZ8bQmayUrSj2yBwYJ61QLs+R0hpEZHfRarBU9vphykACdM6wxSj0YVkFnGlKBxZOZipW6OoKjEkFOHOSH6DYrX3V/TqALYC62iH6jEiLIycxey1vfwkywfsP9V9GlGYHutdwgAgeaN3xUnL8+X6vkQ8cbC2jEuVopodsAAArFYZLVdfAcNc17WYq5y+FX3schGpTo89SZ4Uh9gd4b45h9Hq7h6p7hBF8UCkyqSKnFiPjDJxv5yuY+rYeZ9aJSeCJUYrb1xyOreWnJkhDuYff/1NCewWL8sfuD9IC9BXWBwhxoA/OUfV9KvDBZmYoThlh86ZCQ+uqCR1DIKa1YhPMlT6gzUY01yoMj+B93RpUBUW5LqLDVCL7Qujh/0ns= moonpie@cachyos-x8664\" ];\n          };\n          users.users.root.openssh.authorizedKeys.keys = [\n            # change this to your ssh key\n            \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCg+VqYIph1uiO243+jqdKkSOiw4hAyNWwVeIgzu7+KStY2Dji/Dzn1gu5LGj71jj/dW2Q8FpAC4WXWX5alqUJpK58HwN/d86BpnNPAxjDBOtYN2Znf3Y4108OKhEyaaKpiPSpADW5b/FW+Ki+ftMxRs9cUWuysTxoYDeX9BkTtpar5ChYaoMEkD//tUbqLx9wVFIQ4YdbVajgupUW3S2LRqCAgGBf0eTMYoZbNJHjSNlje7m9UJQOqvXJtiGdoYAqYQdHZfkCLKC1qBw6bl0ZHVkETTKr6tC89ZaZlKfZfGZqgCvyW0VzwYHwRmcOBndZgdOkEHQS/VIYmp91v1G58KMfuSBEKyUJoRVjo6lvbPHIsrGC1vNKLRiRYKGfo1lJ/qFIiq5NNfvmoYZMy+4A6jMohesTdA4yP7nwyz1o9jWmDIHeGJxZCfdYJyQ/IslesR3ACjUYAporCIk3U71f1qB7QOJAErF7+3Q6ZdOHNPPu7sURf2zMn/Q6mWktTxxU= moonpie@localhost.localdomain\"\n          ];\n        })\n      ];\n    };\n  };\n}\n\n\n\n\ndisk-config.nix\n\n\n\ndisk-config.nix\n\n{ disks ? [ \"/dev/sda\" ], ... }: {\n    disk = {\n      sda = {\n        device = builtins.elemAt disks 0;\n        type = \"disk\";\n        content = {\n          type = \"gpt\";\n          partitions = {\n            ESP = {\n              type = \"EF00\";\n              size = \"100M\";\n              content = {\n                type = \"filesystem\";\n                format = \"vfat\";\n                mountpoint = \"/boot/efi\";\n              };\n            };\n            root = {\n              size = \"100%\";\n              content = {\n                type = \"filesystem\";\n                format = \"ext4\";\n                mountpoint = \"/\";\n              };\n            };\n          };\n        };\n      };\n    };\n}\n\n\n\nIn the direcotry where this is, I run the nixos-anywhere command:\nnix run github:numtide/nixos-anywhere -- --flake .#hetzner-cloud moonpie@ip -i nixos-vps\nMy ssh identity file is named nixos-vps.\nBut this config doesn’t work. Although my effort to set up the grub bootloader seemed to have prevailed, and the terminal output said it had succeeded, I cannot access the device. I think it is an issue with network connection.\nFor those who may be attempting to help me, or look at my work, here is a copy of the files I am working with, updated live on every push to the github repo for this blog.\n\n\nShow files\n\n\n\nflake.nix\n\n{\n  inputs.nixpkgs.url = github:NixOS/nixpkgs;\n  inputs.disko.url = github:nix-community/disko;\n  inputs.disko.inputs.nixpkgs.follows = \"nixpkgs\";\n\n  outputs = { self, nixpkgs, disko, ... }@attrs: {\n    nixosConfigurations.hetzner-cloud = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n      specialArgs = attrs;\n      modules = [\n        ({modulesPath, ... }: {\n          imports = [\n            (modulesPath + \"/installer/scan/not-detected.nix\")\n            #(modulesPath + \"/profiles/qemu-guest.nix\") #not a qemu vm\n            # try to fit the lxd-vm config in here\n            #https://github.com/mrkuz/nixos/blob/c468d9772b7a84ab8e38cc4047bc83a3a443d18f/modules/nixos/virtualization/lxd-vm.nix#L4\n            disko.nixosModules.disko\n          ];\n          disko.devices = import ./disk-config.nix {\n            lib = nixpkgs.lib;\n          };\n          boot.loader.efi.efiSysMountPoint = \"/boot/efi\";\n          boot.loader.grub = {\n            devices = [ \"nodev\" ];\n            efiSupport = true;\n            #efiInstallAsRemovable = true;\n          };\n          networking = {\n            usePredictableInterfaceNames = false;\n            interfaces = {\n              eth0 = {\n                useDHCP = false;\n                ipv4.addresses = [{ address = \"93.92.112.130\"; prefixLength = 24; }];\n              };\n            };\n            defaultGateway = {\n                interface = \"eth0\";\n                address = \"93.92.112.1\";\n            };\n          };\n          services = {\n            openssh = {\n                enable = true;\n                settings = {\n                    PasswordAuthentication = false;\n                    #PermitRootLogin = \"prohibit-password\"; # this is the default\n                };\n                openFirewall = true;\n            };\n            cockpit = {\n                enable = true;\n                openFirewall = true;\n            };\n          };\n\n          users.users.moonpie = {\n            isNormalUser = true;\n            extraGroups = [ \"wheel\" ]; # Enable ‘sudo’ for the user.\n            #packages = with pkgs; [];\n            initialHashedPassword = \"$y$j9T$ZGDLrUl6VP4AiusK96/tx0$1Xb1z61RhXYR8lDlFmJkdS8zyzTnaHL6ArNQBBrEAm0\"; # may replace this with a proper secret management scheme later\n            openssh.authorizedKeys.keys = [ \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDEQDNqf12xeaYzyBFrOM2R99ciY9i0fPMdb4J64XpU3Tjv7Z5WrYx+LSLCVolzKktTfaSgaIDN8+vswixtjaAXkGW2glvTD0IwaO0N4IQloov3cLZa84I7lkj5jIkZiXJ2zHJZ8bQmayUrSj2yBwYJ61QLs+R0hpEZHfRarBU9vphykACdM6wxSj0YVkFnGlKBxZOZipW6OoKjEkFOHOSH6DYrX3V/TqALYC62iH6jEiLIycxey1vfwkywfsP9V9GlGYHutdwgAgeaN3xUnL8+X6vkQ8cbC2jEuVopodsAAArFYZLVdfAcNc17WYq5y+FX3schGpTo89SZ4Uh9gd4b45h9Hq7h6p7hBF8UCkyqSKnFiPjDJxv5yuY+rYeZ9aJSeCJUYrb1xyOreWnJkhDuYff/1NCewWL8sfuD9IC9BXWBwhxoA/OUfV9KvDBZmYoThlh86ZCQ+uqCR1DIKa1YhPMlT6gzUY01yoMj+B93RpUBUW5LqLDVCL7Qujh/0ns= moonpie@cachyos-x8664\" ];\n          };\n          users.users.root.openssh.authorizedKeys.keys = [\n            # change this to your ssh key\n            \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCg+VqYIph1uiO243+jqdKkSOiw4hAyNWwVeIgzu7+KStY2Dji/Dzn1gu5LGj71jj/dW2Q8FpAC4WXWX5alqUJpK58HwN/d86BpnNPAxjDBOtYN2Znf3Y4108OKhEyaaKpiPSpADW5b/FW+Ki+ftMxRs9cUWuysTxoYDeX9BkTtpar5ChYaoMEkD//tUbqLx9wVFIQ4YdbVajgupUW3S2LRqCAgGBf0eTMYoZbNJHjSNlje7m9UJQOqvXJtiGdoYAqYQdHZfkCLKC1qBw6bl0ZHVkETTKr6tC89ZaZlKfZfGZqgCvyW0VzwYHwRmcOBndZgdOkEHQS/VIYmp91v1G58KMfuSBEKyUJoRVjo6lvbPHIsrGC1vNKLRiRYKGfo1lJ/qFIiq5NNfvmoYZMy+4A6jMohesTdA4yP7nwyz1o9jWmDIHeGJxZCfdYJyQ/IslesR3ACjUYAporCIk3U71f1qB7QOJAErF7+3Q6ZdOHNPPu7sURf2zMn/Q6mWktTxxU= moonpie@localhost.localdomain\"\n          ];\n        })\n      ];\n    };\n  };\n}\n\n\n\ndisk-config.nix\n\n{ disks ? [ \"/dev/sda\" ], ... }: {\n    disk = {\n      sda = {\n        device = builtins.elemAt disks 0;\n        type = \"disk\";\n        content = {\n          type = \"gpt\";\n          partitions = {\n            ESP = {\n              type = \"EF00\";\n              size = \"100M\";\n              content = {\n                type = \"filesystem\";\n                format = \"vfat\";\n                mountpoint = \"/boot/efi\";\n              };\n            };\n            root = {\n              size = \"100%\";\n              content = {\n                type = \"filesystem\";\n                format = \"ext4\";\n                mountpoint = \"/\";\n              };\n            };\n          };\n        };\n      };\n    };\n}"
  },
  {
    "objectID": "projects/nixos-vps/index.html#scaffolding-and-beginnings",
    "href": "projects/nixos-vps/index.html#scaffolding-and-beginnings",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Scaffolding and Beginnings",
    "text": "Scaffolding and Beginnings\nAn example docker-compose, from the official ansible documentation:\ntasks:\n    - name: Remove flask project\n      community.docker.docker_compose:\n        project_src: flask\n        state: absent\n\n    - name: Start flask project with inline definition\n      community.docker.docker_compose:\n        pull: true # Not default. Will pull images upon every rerun.\n        \n        # rebuilds if a change to the dockerfile contents are detected. If a rebuild, then will attempt to pull a newer version fo the image, but not otherwise\n        build: always \n\n        state: present # default, but to be noted. \n\n        #Docker compose goes here. But can I have multiple projects?\n        project_name: flask\n        definition:\n            db:\n              image: postgres\n            web:\n              build: \"{{ playbook_dir }}/flask\"\n              command: \"python manage.py runserver 0.0.0.0:8000\"\n              volumes:\n                - \"{{ playbook_dir }}/flask:/code\"\n              ports:\n                - \"8000:8000\"\n              links:\n                - db\nAnsible also offers an image for managing direct parts of docker, like images.\n- name: Pull an image\n  community.docker.docker_image:\n    name: pacur/centos-7\n    source: pull\n    # Select platform for pulling. If not specified, will pull whatever docker prefers.\n    pull:\n      platform: amd64\n- name: Build image and with build args\n  community.docker.docker_image:\n    name: myimage\n    build:\n      path: /path/to/build/dir\n      args: # key value args \n        log_volume: /var/log/myapp\n        listen_port: 8080\n        file: Dockerfile_name\n    source: build\nAnsible also seems to support managing git repos, which I can use to automate. I’ve decided to write an example using the features that I would utilize.\n- name: Git checkout\n  ansible.builtin.git:\n    repo: 'https://foosball.example.com'\n    dest: /home/moonpie\n    version: release-0.22 # can be branch, tag, or sha1 hashshum of the repo.\nThis will enable me to write docker-compose’s, Dockerfiles, or other things and put them in a git repo, which I can then clone and use later.\nIn addition to that, I also need a way to keep the system updated, for security purposes. Because I am using ubuntu, I will use the ansible apt module.\n- name: update the system to latest distro packages\n  ansible.builtin.apt: \n    update-cache: yes # equivlaent of apt-get update\n    upgrade: safe # conservative, safe upprade.full/dist upgrades all packages to latest, but I will have to research the difference between the two. \n    autoclean: yes # cleans not installed packages from the cache\n    autoremove: yes # delete uneeded dependencies\n    clean: yes # deletes all packages from the cache\n\n  \n- name: update the distribution itself? Still working on this one\n  ansible.builtin.apt:\n  # Hmmm, the format for apt repos is currently changing.\n\n- name: manage apt_repo #since apt key is deprecated, this an an alternative around it.\n  block:\n    - name: somerepo |no apt key\n      ansible.builtin.get_url:\n        url: https://download.example.com/linux/ubuntu/gpg\n        dest: /etc/apt/keyrings/somerepo.asc\n\n    - name: somerepo | apt source\n      ansible.builtin.apt_repository:\n        repo: \"deb [arch=amd64 signed-by=/etc/apt/keyrings/myrepo.asc] https://download.example.com/linux/ubuntu {{ ansible_distribution_release }} stable\"\n        state: present\n\n- name: The newer deb822 format is better # however, this isn't used by my ubuntu install. \n  deb822_repository:\n    name: example\n    types: deb\n    uris: https://download.example.com/linux/ubuntu\n    suites: '{{ ansible_distribution_release }}'\n    components: stable\n    architectures: amd64\n    signed_by: https://download.example.com/linux/ubuntu/gpg\n\n\n\n\n- name: Reboot the system\n  reboot: # However, when I tested this for my current project at my internship, it didn't work. The ssh did not reconnect."
  },
  {
    "objectID": "projects/nixos-vps/index.html#ansible-inventory",
    "href": "projects/nixos-vps/index.html#ansible-inventory",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Ansible Inventory",
    "text": "Ansible Inventory\nAnsible has multiple ways to configure ssh keys. One way is to explicitly specify a ssh private key file in your inventory file:\nall:\n  hosts:\n    your_remote_host1:\n      ansible_user: your_username1\n      ansible_password: your_password1\n      ansible_ssh_private_key_file: /path/to/your_private_key1\n    your_remote_host2:\n      ansible_user: your_username2\n      # and so on\n\n  vars:\n    ansible_ssh_common_args: '-o StrictHostKeyChecking=no' # Disables auto reject of unkown hosts.\nAnother way is to specify the private key in your ~/.config/ssh/config file.\nHost your_remote_host1\n  HostName your_remote_host1.example.com\n  User your_username\n  IdentityFile /path/to/your_private_key\n\nHost your_remote_host2\n  HostName your_remote_host2.example.com\n  User your_username\n  IdentityFile /path/to/another_private_key\nYou can also specify the private key on the command line, with the –private-key=/path/to option.\nI am searching for the most CI/CD friendly way to do this. Tbh, it may be lazy, but for a single machine, I may simply ssh into the machine, git clone the repo, and use ansible’s local mode, which runs an ansible playbook on the local machine.\nHere is my current ansible inventory. Because I am only configuring one host, it is extremely simple.\n\n\ninventory.yml\n\n---\nall:\n  hosts:\n   office:\n\nAnd here is my ssh config file, censored of course.\n\n\n~/.ssh/config\n\nHost office\n        HostName {server ip or hostname goes here}\n        port 22\n        user moonpie\n        IdentityFile /home/moonpie/.ssh/office-vps\n\nNow, I can test if my hosts are up with a simple ansible command.\n[nix-shell:~/vscode/ansible-vps-config]$ ansible all -i inventory.yml -m ping\noffice | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nThe “ping: pong” part means that it discoverd my server, and it is ready for me to configure it if I so wish."
  },
  {
    "objectID": "projects/nixos-vps/index.html#dry-runs-and-testing",
    "href": "projects/nixos-vps/index.html#dry-runs-and-testing",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Dry runs and Testing",
    "text": "Dry runs and Testing\nNow, after some tinkering with syntax, I’ve gotten ansible to do a dry run without complaining:\n[nix-shell:~/vscode/ansible-vps-config]$ ansible-playbook --check -i inventory.yml main.yml --ask-become-pass\nBECOME password: \n\nPLAY [all] *********************************************************************************************************\n\nTASK [Gathering Facts] *********************************************************************************************\nok: [office]\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/meshcentral.yml for office\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nok: [office]\n\nTASK [docker-compose : npm] ****************************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/npm.yml for office\n\nTASK [docker-compose : npm] ****************************************************************************************\nchanged: [office]\n\nTASK [sys-maintain : update the system to latest distro packages] **************************************************\nchanged: [office]\n\nPLAY RECAP *********************************************************************************************************\noffice                     : ok=6    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nThe --ask-become-pass asks me for the sudo password on my server, in case my user doesn’t have permission to do passwordless sudo.\nSince this is a dry run, it simply tells me what will be changed, but doesn’t actually change it. For some reason, it didn’t update one of the docker containers although it did update the other. I discoverd this was because I was missing something in my compose file.\n---\n- name: npm\n  community.docker.docker_compose:\n    pull: true\n    build: true \n    project_name: npm\n    definition:\n      version: '3'\n      services:\n        app:\n          image: 'jc21/nginx-proxy-manager'\n          restart: unless-stopped\n          ports:\n            - '80:80'\n            - '81:81'\n            - '443:443'\n          # - '53:53'\n          volumes: \n            - /home/{{ ansible_user_id }}/npm/data:/data\n            - /home/{{ ansible_user_id }}/npm/letsencrypt:/etc/letsencrypt\n      networks:\n        default:\n          external: true\n          name: mine\nFor the image portion, I needed image: 'jc21/nginx-proxy-manager:latest'. this will ensure that every time I rerun the ansible playbook, docker will attempt to update the container versions.\nNow, I can run just the docker-compose part of my using a the tags feature.\n[nix-shell:~/vscode/ansible-vps-config]$ ansible-playbook --check -i inventory.yml main.yml --ask-become-pass --tags docker-compose\nBECOME password: \n\nPLAY [all] *********************************************************************************************************\n\nTASK [Gathering Facts] *********************************************************************************************\nok: [office]\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/meshcentral.yml for office\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nchanged: [office]\n\nTASK [docker-compose : npm] ****************************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/npm.yml for office\n\nTASK [docker-compose : npm] ****************************************************************************************\nchanged: [office]\n\nPLAY RECAP *********************************************************************************************************\noffice                     : ok=5    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0"
  },
  {
    "objectID": "playground/flashcards/index.html",
    "href": "playground/flashcards/index.html",
    "title": "A notes format that I can automatically convert to flashcards?",
    "section": "",
    "text": "My goal is to have a notes format that I can automatically convert to flashcards, for studying.\nSo quarto has support for a format called revealjs.\nRevealjs is basically presentations, like powerpoint or google slides, but in your browser. Although you can also print them to pdf.\nAnyway, I was in need of some flashcards for my class and I wanted to see if I could use revealjs for this.\nto create slides, just use a second level header:\n## Getting up\n\n- Turn off alarm\n- Get out of bed\n\n## Going to sleep\n\n- Get in bed\n- Count sheep\nHowever, I don’t really want a title, so I can simply have the second level header (##), without a title, and it still creates another slide.\nBy default, lists are revealed all at once. To change this, in your quarto heading options:\nformat: \n    revealjs:\n        incremental: true\nActually, I like content pauses better.\n## Slide with a pause\n\ncontent before the pause\n\n. . .\n\ncontent after the pause\nNow, how can I randomize slide orders?\nAccording to the revealjs pull where this feature was implemented:\nI can have a range of slides, something like:\n// for everything\n// Reveal.configure({ random: true });\n\nReveal.configure({\n                random: {\n                    rangeStart: 5,\n                    rangeEnd: 8\n                }\n            });\nNo wait, quarto has an option to shuffle the slides.\nshuffle: true\nNow, is there a way to randomize the the list? I want the list options to sometimes have the name, and sometimes have the content, and you have to match which with which.\nchatgpt gave me this:\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n    &lt;meta charset=\"UTF-8\"&gt;\n    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n    &lt;title&gt;Randomized List on Page Load&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Randomized List&lt;/h1&gt;\n    &lt;ul id=\"randomList\"&gt;\n        &lt;li&gt;Item 1&lt;/li&gt;\n        &lt;li&gt;Item 2&lt;/li&gt;\n        &lt;li&gt;Item 3&lt;/li&gt;\n        &lt;li&gt;Item 4&lt;/li&gt;\n        &lt;li&gt;Item 5&lt;/li&gt;\n    &lt;/ul&gt;\n\n    &lt;script&gt;\n        document.addEventListener(\"DOMContentLoaded\", function() {\n            const list = document.getElementById(\"randomList\");\n            for (let i = list.children.length; i &gt;= 0; i--) {\n                list.appendChild(list.children[Math.random() * i | 0]);\n            }\n        });\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\nBut does it work? :\n\n\n\n    \n    \n    Randomized List on Page Load\n\n\n    Randomized List\n    \n        Item 1\n        Item 2\n        Item 3\n        Item 4\n        Item 5\n    \n\n    \n\n\nBut it doesn’t work in revealjs, as revealjs won’t run show thing incrementally.\nRather than lists, can I just have javascript manipulate classes and divs created by pandoc?\n::: {#myCustomBlock .custom-class}\nThis is a custom block that can be manipulated by JavaScript.\n:::\nThis should be able to be changed by pandoc.\nAnd then I can shuffle them with something like:\n    function shuffleBlocks() {\n    const container = document.getElementById(\"blocksContainer\");\n    const blocks = Array.from(container.children);\n    for (let i = blocks.length - 1; i &gt; 0; i--) {\n        const j = Math.floor(Math.random() * (i + 1));\n        [blocks[i], blocks[j]] = [blocks[j], blocks[i]];\n    }\n    container.innerHTML = \"\";\n    blocks.forEach(block =&gt; container.appendChild(block));\n}\n\n// Shuffle the blocks when the page loads\nwindow.addEventListener(\"DOMContentLoaded\", shuffleBlocks);\nSo to test:\nShuffle blocks:\n\n\nThis is Block 1.\n\n\nThis is Block 2.\n\n\nThis is Block 3.\n\n\n&lt;script&gt;\n    function shuffleBlocks() {\n        const container = document.querySelector(\"#blockContainer\");\n        const blockDivs = Array.from(container.querySelectorAll(\".block\"));\n        \n        // Shuffle the order of the block divs\n        for (let i = blockDivs.length - 1; i &gt; 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            const temp = blockDivs[i];\n            blockDivs[i] = blockDivs[j];\n            blockDivs[j] = temp;\n        }\n        \n        // Append shuffled block divs back to the container\n        blockDivs.forEach(blockDiv =&gt; {\n            container.appendChild(blockDiv);\n        });\n    }\n\n    // Shuffle the blocks when the page loads\n    window.addEventListener(\"DOMContentLoaded\", shuffleBlocks);\n&lt;/script&gt;\n\nAfter many iterations with chatgpt, I finally got it to shuffle content.\nNow is there a way to scale this across multiple slides? I don’t want to have to copy and paste this for the every single block. In fact, can I just make a custom html element that shuffles them?\nSo chatgpt gave me a custom html element class. It gave me a lot, actually, and none of them worked. So I’m giving up on that for now.\nHowever, revealjs does support a grid slide layout, and it groups vertical slides together while randomizing their order, but it also randomizes the horizontal slides as well.\nformat: \n    revealjs:\n        incremental: true\n        theme: dark\n        shuffle: true\n        navigation-mode: grid\nEventually, I gave up on the revealjs format, after I found out that this flashcards app, anki\nIn my research, I found some tools which can convert other formats to anki:\n\n\n\nSoftware\nLast update?\nPackaged?\n\n\n\n\nmd2anki\n4 years ago\npypi\n\n\nmarkdown2anki\n2 weeks ago\npypi\n\n\nmarkdown-anki-decks\n1 year ago\npypi, but also in nixpkgs\n\n\n\nI did some testing. markdown-anki-decks is exceedingly simply, with something like:\n# Deck title\n\n## card front\n\ncard back\nto create a deck.\nI also experimented with markdown2anki. It’s way, way more complex. Unlike the previous option, it has many, many options.\nTo create a simple flashcard:\n```{.markdown}"
  },
  {
    "objectID": "projects/build-server-2/index.html",
    "href": "projects/build-server-2/index.html",
    "title": "Building my own Server Part 2 — Software",
    "section": "",
    "text": "STATUS: see part 3, where I don’t start with openstack.\nI have other stuff to do that needs my server working, and since I already have remote libvirt setup, I will just stick to that for now.\nBut I don’t think I am giving up on openstack, since what I want seems to be possible, just very difficult."
  },
  {
    "objectID": "projects/build-server-2/index.html#software-suite",
    "href": "projects/build-server-2/index.html#software-suite",
    "title": "Building my own Server Part 2 — Software",
    "section": "Software Suite",
    "text": "Software Suite\nI want an easy install, but I also want lots of features. Here are some things I have looked at:\n\nProxmox VE\nXen Orchestra\nOpenstack\nCanonicals LXD UI\nOvirt\nHarvester\nOpenVZ"
  },
  {
    "objectID": "projects/build-server-2/index.html#openstack",
    "href": "projects/build-server-2/index.html#openstack",
    "title": "Building my own Server Part 2 — Software",
    "section": "Openstack",
    "text": "Openstack\nCurrently, openstack appeals to me a lot. Although I originally wanted to do a bare metal install, I now realize that that is too time consuming and not realistic, so I am most likely going to use one of the automated methods of installation.\nKolla ansible\nThey have an easy deployment guide for an all in one node, perfect for my single server.\nI will definitely not use every service, but I do want to use openstack because of the sheer number of services it offers. Openstack offers every single feature of other virtualization platforms, at the cost of complexity. Here are the features that made me decide I needed that complexity.\n\nSkyline/Horizon\nOpenstack has a previous generation web ui, horizon, and a newer generation web ui, skyline. These web ui’s offer all the features of other web based virtualization platforms like proxmox, but they also let you configure the other things of openstack.\n\nAnd they have some special features, like giving you a visual layout of network topology.\n\n\n\nMulti tenancy.\nThe most important feature of openstack, in my opinion, is it’s multi tenant architechture. Unliek proxmox, which is designed for a single organization, openstack is designed in such a way that you can create extra users, which get their own allocation of resources.\nSo when I go to college, if anyone wants a VPS to play around in, I can just allocate them a few resources, and then they get their own web ui for playing with servers and networking.\nMany public cloud services are actually using openstack in the background for it’s public tenant architecture. Openstack’s dashboards can be rebranded to be your own company:\n\n\n\nBare metal nodes\nOpenstack saves a lot of trouble by immensely abstracting almost all the manual work that goes into putting together a modern cloud.\nIt takes it a step further, with it’s ability to treat physical, bare metal machines, as virtual machines, even automating the provisioning the same way you can do so for a virtual machine.\nThe docs make it sound complex, but it really isn’t all that. By leveraging the nova component of openstack, which abstracts the backend drivers of virtual machines (qemu-kvm, xen, or even hyper-v) can be used as backend drivers for nova.\nHowever, when combined with ironic openstack’s service to configure bare metal nodes, nova can also use bare metal as a driver for compute nodes. This integrates further with services like magnum…\n\n\nMagnum\nMagnum is openstack’s kubernetes-as-as-service. It provisions nodes using nova, with kubernetes clusters.\nNow here is where I get greedy. Do I need kubernetes? No. Will kubernetes even be useful on a single server setup? No. Do I want kubernetes? Yes.\nHere is a video demonstration, where someone uses the web ui to create a cluster using magnum.\n\nIn addition to that, because openstack magnum uses openstack heat, which provisions nodes from templates, it can be used to do things like auto install nvidia drivers and container runtime.\n\nThis video is a bit outdated, so heat and magnum are much more mature since then, and have only gained more features.\n\n\nApi and Automation\nOpenstack is designed from the ground up to be a cloud. It has first class support for their api, and everything that can be done from the UI can also be done from either the command line, or the api.\nThis advanced api makes it easier for other software to interact with openstack. For example, the rancher kubernetes cluster manager supports openstack as a cloud provider. It is capable of requesting nodes, provisioning them, and then setting up a kubernetes cluster entirely from the rancher web gui.\n\n\nZun\nOpenstack zun is the container service for openstack. It doesn’t run them in virtual machines, but instead directly on metal. It’s likely that when I want to run containerized services for actual usage, this is what I will be using instead of kubernetes since I will be using a single server, and thus won’t be able to get the benefits of kubernetes. The benefit I see form using containers is that because I have an nvidia desktop gpu, I won’t be able to use vgpu, a feature that lets you divide the gpu between virtual machines. However, containers have no such limitation."
  },
  {
    "objectID": "projects/build-server-2/index.html#installing-openstack",
    "href": "projects/build-server-2/index.html#installing-openstack",
    "title": "Building my own Server Part 2 — Software",
    "section": "Installing Openstack",
    "text": "Installing Openstack\nI’ve decided to use the kolla-ansible project to install openstack. It works by using ansible to deploy docker containers, which the openstack services run in.\nThey have a quickstart guide:\nhttps://docs.openstack.org/kolla-ansible/latest/user/quickstart.html\nAnd the ansible playbook can be found here:\nhttps://opendev.org/openstack/kolla-ansible\nAnd they provide a sample ansible inventory for the all in one node.\nI do not need all of those features. I pretty much just want virtualized compute, networking, containers, and kubernetes. I don’t need things like an S3 compatible block storage, a relational database, or an app store. Okay, but maybe I want them.\nI will do more research into what I want and what I don’t want, and edit the ansible playbook accordingly.\nHowever, this method of deployment seems to require two NIC’s (network interface cards). I think I have both, but just in case, I noted another method of deployment, openstack ansible (yeah the naming isn’t the greatest), which deploys openstack without using containers, it actaully installs and configures the services on the host operating system.\nThe openstack ansible All in one deployment, doesn’t seem to have the same requirement of two NIC’s, which I do have.\n\nOperating system\nBut first, I do need to select on an operating system. Openstack is flexible and versatile, and it can be installed on multiple operating systems.\nI was originally going to choose a RHEL compatible distro, but then RHEL made changes put the status of those in limbo.\nI am currently deciding between:\n\nUbuntu\nDebian\nRHEL (via a developer subscription)\nA RHEL rebuild\n\nRocky Linux\nAlma Linux\nOne of the academic distros, like scientific linux\n\nCentos Stream\n\nThe important thing is that it’s a stable release distro with automatic updates. I don’t want to have to do too much manual maintainence. Ideally, I also want this distro to have newerish packages, in case I want to do some tinkering with the underlying OS, and I also want the distro to have a stable release that goes on for longer than my college years. From what I’ve heard, upgrading from one release of an OS to another can be a frustrating process, and I don’t want to have to do this while I’m in the middle of school.\nThe RHEL rebuilds do appeal to me, but they also come with extra complications, like selinux that I don’t really want to have to deal with.\nBut after much deliberation, I’ve decided on Rocky Linux. Rocky Linux 9 is officially supported by kolla ansible. In addition to that, Rock Linux 9 will be supported for a good deal of time, with the release being officially supported for 3 years and a bit, and that release will continue to receive security updates for 5 more years after that. More than enough to last me through college.\nThe install was very simple. I thought I would experience issues because of the Nvidia GPU, as I had been having issues with graphical monitor output with other distros, but I didn’t. A GUI appeared for me, and the install process was exceedingly simple, even simpler than debian or other distros I’ve tried. Of course, the disadvantage was that I couldn’t configure everything, like there was no option to set up users other than root, but it was very quick.\nNow, I have RHEL installed.\nTo make management easier, I will install my favorite web administration system, cockpit. This will also enable me to do remote management operations with a gui, things like partitioning the disks.\nNow that I have rocky linux installed, I can install openstack using kolla-ansible.\n\n\nKolla-ansible\nI will be following the quick start guide\nI will briefly go over what I am doing here, edited for my usecase. The first few steps are copy and pasted from the guide linked above.\nsudo dnf install git python3-devel libffi-devel gcc openssl-devel python3-libselinux\npip install 'ansible&gt;=6,&lt;8'\npip install git+https://opendev.org/openstack/kolla-ansible@master\nsudo mkdir -p /etc/kolla\nsudo chown $USER:$USER /etc/kolla\ncp -r /home/moonpie/.local/share/kolla-ansible/etc_examples/kolla/* /etc/kolla\ncp /home/moonpie/.local/share/kolla-ansible/ansible/inventory/all-in-one /home/moonpie/\nNow, this is the initial setup. I need to customize these things to my liking.\nUsernames and passwords a are easy, but I need to make sure the networking is right, especially since my setup will be so unorthodox. Because I am setting this up on my home network, I won’t have public internet access, as my router using NAT\n\n\nRouter Networking\nIt requires that I set my two network interfaces to two things: default network interface for everything else, and the neutron external interface.\nThe neutron external interface can be any physical interface, but internally, it is referred to as br-ex. This interface is responsible for letting the virtual machines interact with the internet.\nAnyway, I had a spare router lying around, and I flashed it with freshtomato for some more advanced features. I hooked it up to my existing router, so it would be behind NAT, but now I have extra ethernet ports, so I can have a private subnet with my laptop, and the openstack stuff.\nI was curious if tomato offered an easy way to expose services to the internet, from behind NAT, something like integration with Cloudfare’s tunnels, but they didn’t.\nHowever, another idea has occured upon me: Why not just host the public parts of openstack… on the public. I could rent a VPS, and host openstack neutron, and maybe the openstack dashboards on the public.\nFor example, my current vps provdider webdock, gives out a range (/124) of ipv6 addresses, 16 total. Based on the pricing on that page, 1.75 Euros for an additional ipv4 address, I think I can safely assume that ipv4 addresses are more expensive, out of what I am wiling to spend, because of their scarcity.\nHowever, if I can install openstack neutron on a cheap vps, thenH I will be able to give public ipv6 access to the virtual machines, which sounds like a very neat setup.\nSince my home server won’t have public internet access I am guessing I have to start by creating a virtual network that links the two machines together, that way they can see eachother and cluster.\nOpenstack has some interesting diagrams: https://docs.openstack.org/install-guide/environment-networking.html. But I can’t find anything conclusive.\nSince I have a router running tomato, I am thinking that I can vpn the entire router into the other machine, so that the vps I am accessing can access everything in the tomato subnet, meaning I won’t have to configure the server itself. The downside is that everything will be vpn’ed, but with a 4 TB upload limit, I’m not too concerned about that right now.\nBut sadly, webdock seems to be sold out of kvm vps’s, which have better compatibility with docker, so I will probably go looking for another platform.\nAnyway, tomato seems to have a wireguard client installed, so I will use that, since wireguard is the fastest vpn client/server available. I found a nice guide on setting up pure wireguard. However, it doesn’t discuss connecting the subnet of a router to the vps. I did find another guide that did. Now this guide is older, and some people criticized it for various reasons. However, it does tell me the name of what I am looking for: Site to site.\nI found a much simpler guide on ubuntu’s website. Rather than A-B-C, I only need the A-B which this offers. In addition to that, there are no iptables rules on this guide.\nThe minimum specs required for openstack neutron are: ???. The docs suck.\nVPS provider overview:\n\n\n\nName\nCPU\nRam (GB)\nPrice (/month)\nIpv6\nOS\n\n\n\n\nContabo\n4\n8\n$10.29\n18 quintillion\nRocky\n\n\n\nOkay, contabo wins. I was gonna do an actual comparison between multiple vps providers, but contabo has great specs, and it is trusted by people when I have asked around.\nI don’t want to spend money, but this is a nice deal.\nAlso, contabo offers 32 TB out, and unlimiited in. This is definitely a very good choice.\nAfter visiting https://test-ipv6.com/ and realizing that my home residential wifi does not have ipv6 enabled by default (although there is an option in the router settings), I realize that the college dorms may not have ipv6 support. In that case, vpning into the remote server, which does have ipv6 support, would give me ipv6 support.\nNow that I have a vps, I am trying to get wireguard working, by testing with my laptop I started out with the guide from ubuntu, but that doesn’t work. In addition to not having access to subnets I can only access on my laptop from my server, not even the vpn is working correctly:\nmoonpie@lizard:~&gt; curl --interface wgA ifconfig.me\ncurl: (7) Failed to connect to ifconfig.me port 80 after 1 ms: Couldn't connect to server\nI found another guide but yet again, it’s another A-B-C guide.\nI found a promising stackexchange answer with an interesting commmand I have tried:\nip route add 192.168.122.0/24 via 10.10.9.0 dev wgB\nWhere the 192.168.122.0/24 is the subnet I am trying to expose.\nOkay, at this piont, I understand what I want to do pretty well. I think I will use the A-B-C guides as a template, except I only need A-B, and the router needs to eb the device that is configured not to have a permanent, static ip address, like the clients are.\nI got lucky when I was browsing lemmy, and I found forum comment related to my exact issue, which then links to a stackexchange question, in which the top answer uses information they sourced from this guide\nI attempted to follow the latter guide, using a simple setup. My laptop, would be my “router”, and I was attempting to expose the vlan subnet created by the libvirt virtual machine manager to my remote server, a vps hosted on contabo.\nIronically, I got the connection to work, but in the wrong direction. My vps could ping my remote server, but my remote server could not ping the virtual machines on my laptop. Interestingl, it could ping my laptop using the ip of the virbr0 virtual network adapter that libvirt creates for a vlan.\nI’m guessing that this didn’t work because I’m doing this somewhat backwards, with the device that the\nI am guessing because I am doing this somewhat backwards, where the device exposing the lan is behind nat, whereas it is the other way around in the guides that I have seen.\nroot@vmi1403809 ~]# ping 192.168.122.1\nPING 192.168.122.1 (192.168.122.1) 56(84) bytes of data.\n64 bytes from 192.168.122.1: icmp_seq=1 ttl=64 time=46.5 ms\n[root@vmi1403809 ~]# ping 192.168.122.201\nPING 192.168.122.201 (192.168.122.201) 56(84) bytes of data.\nFrom 10.10.9.0 icmp_seq=1 Destination Port Unreachable\nThe first is the ip address of my laptop, and the second is the ip address of my debian virtual machine, which is running on my laptop.\nmoonpie@lizard:~/vscode/moonpiedumplings.github.io&gt; curl --insecure  http://192.168.122.201:9090\n&lt;html&gt;&lt;head&gt;&lt;title&gt;Moved&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Please use TLS&lt;/body&gt;&lt;/html&gt;\nmoonpie@lizard:~/vscode/moonpiedumplings.github.io&gt; curl --insecure  http://192.168.122.201:9090 --interface wgA\ncurl: (7) Failed to connect to 192.168.122.201 port 9090 after 0 ms: Couldn't connect to server\nCurl has a feature to bind to a specific interface, and when I try to test a connection using the wgA interface, it can’t connect. At first, I’m annoyed, but in hindsight, this makes sense. Only through the virbr0 interface, can I access the virtual machiens. And the virtual machines, are actually behind their own NAT. They are not public, so why would I be able to ping them?\nLibvirt offers several networking options. Bridged is where all virtual machines get public ip addresses. Routed is similar, but they don’t get their own interface, and it works on wirelessly connected devices. And finally, NAT, what I am using does not give virtual machines publicly accessible ip addresses.\nInstead of running it on my actual laptop, I will try to run the vpn on the debian virtual machine instead.\nI set up wireguard again, one side on the vps I am renting:\n\n\n/etc/wireguard/wgB.conf\n\n[Interface]\nAddress = 10.10.9.1/31\nPostUp = wg set %i private-key /etc/wireguard/%i.key\nListenPort = 51000\n\n[Peer]\n# alpha site\nPublicKey = e763iTZmmMcx7HufUOi5vzmQJ5ZhYBuuqnXh/2ViBjA=\nAllowedIPs = 10.10.10.0/24,10.10.9.0/31,192.168.122.0/24\n\nAnd another side on my virtual machine:\n\n\n/etc/wireguard/wgA.conf\n\n[Interface]\nAddress = 10.10.9.1/31\nPostUp = wg set %i private-key /etc/wireguard/%i.key\nListenPort = 51000\n\n[Peer]\n# alpha site\nPublicKey = e763iTZmmMcx7HufUOi5vzmQJ5ZhYBuuqnXh/2ViBjA=\nAllowedIPs = 10.10.10.0/24,10.10.9.0/31,192.168.122.0/24\n\nAnd the wireguard default route didn’t work, so I removed it, and then added the route manually using the ip route tool.\nWhen everything was done, my VPS could ping the ip address of my virtual machine, but not my laptop.\n[root@vmi1403809 ~]# ping 192.168.122.201 # virtual machine ip\nPING 192.168.122.201 (192.168.122.201) 56(84) bytes of data.\n[root@vmi1403809 ~]# ping 192.168.122.1 # laptop ip\nPING 192.168.122.1 (192.168.122.1) 56(84) bytes of data.\n# it just sits here forever.\nWell, this is better than a strange error. However, I don’t know where to go next from here.\nAlright, I figured it out. I’ve sourced my information from two different links:\nThe instructions for wg-quick I used are in the arch wiki’s wireguard article.\nHowever, I also needed to look at the arch wiki’s NAT article. From that article, if you have docker enabled, you must take some extra steps that make thigns more complex. So I disabled docker, followed the instructions for NAT there.\nAnd on my remote server, I ran ip route add xx.xx.xx.xx/yy via zz.zz.zz.zz where xx is the subnet you want to access remotely, and zz is the ip address of the remote device with access to that subnet.\nAnyway, I find this deeply ironic. I originally wanted to deploy wireguard to my router to avoid any kind of complex networking, but because I didn’t have physical access to my router, I was testing on machines with more complex networking, which caused things to not work. This affirms my decision to set up wireguard on my router, rather than on my server, especially since kolla-ansible uses docker to deploy.\nAnyway, I will have to look into split tunneling or whatnot, because I may not want to vpn everything.\nThe freshtomato wiki contains info on how to configure wireguard, including doing things like having it start on boot.\nSadly, the wg-quick command doesn’t seem to be available on the router, but that’s not really a big deal. I can just write a small script which sets this up, using the instructions from the arch wiki article, or even just using wg-quick, because it tells you what steps it is taking to up the wireguard interface.\nHere is the wg-quick file on my server:\n[Interface]\nAddress = 10.10.9.1/31\nPostUp = wg set %i private-key /etc/wireguard/%i.key\nListenPort = 51000\n\n[Peer]\n# alpha site\nPublicKey = e763iTZmmMcx7HufUOi5vzmQJ5ZhYBuuqnXh/2ViBjA=\nAllowedIPs = 10.10.10.0/24,10.10.9.0/31,192.168.122.0/24\nInterestingly, I didn’t have to add a specific route using ip. Once nat is set up correctly on one device, the, it somehow knows where to go. I only need to add allowed ips.\nHere is configuration file for my “router” (still just testing with virtual machines).\n\n\n/etc/wireguard/wgA.conf\n\n[Interface]\nPreUp = sysctl net.ipv4.ip_forward=1\nPreUp = iptables -t nat -A POSTROUTING -o enp1s0 -j MASQUERADE\nPreUp = iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\nPostUp = wg set %i private-key /etc/wireguard/%i.key\nPostUp = iptables -A FORWARD -i %i -o enp1s0 -j ACCEPT\nAddress = 10.10.9.0/31\nListenPort = 51000\n\n[Peer]\n# Remote Server\nPublicKey = pxCx+cEs6hoI4EE+XdE4lQiLkJRbG4JGQwXz6d/hZDM=\nAllowedIPs = 10.10.9.0/31\nEndpoint = &lt;VPS IP&gt;:51000\nPersistentKeepalive = 25\n\nHere are the steps my “router” takes when I use wg-quick up wgA\nroot@debian:/home/moonpie# wg-quick up wgA\n[#] sysctl net.ipv4.ip_forward=1\nnet.ipv4.ip_forward = 1\n[#] iptables -t nat -A POSTROUTING -o enp1s0 -j MASQUERADE\n[#] iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\n[#] ip link add wgA type wireguard\n[#] wg setconf wgA /dev/fd/63\n[#] ip -4 address add 10.10.9.0/31 dev wgA\n[#] ip link set mtu 1420 up dev wgA\n[#] wg set wgA private-key /etc/wireguard/wgA.key\n[#] iptables -A FORWARD -i wgA -o enp1s0 -j ACCEPT\nIn this case, wgA is the wireguard interface, and enp1s0 is the interface I want to configure NAT on, so that I can access the subnet behind it.\nActually, wg-quick appears to just be a bash script, and I might be able to transfer that over to freshtomato.\nThe best feature of this wireguard setup is that, since it doesn’t route everything through the vpn tunnel like I was initially expecting, meaning I won’t have to worry about bandwidth or speed, since I won’t be routing all traffic through my router to a remote router.\nIn addition to all this, I can use nmap to make sure that my remote server can actually see the other services, as they are behind a kind of NAT.\nroot@vmi1403809 wireguard]# nmap -sV 192.168.122.1\nStarting Nmap 7.91 ( https://nmap.org ) at 2023-08-18 11:29 PDT\nNmap scan report for 192.168.122.1\nHost is up (0.068s latency).\nNot shown: 997 closed ports\nPORT     STATE SERVICE         VERSION\n53/tcp   open  domain          dnsmasq 2.89\n8000/tcp open  http            SimpleHTTPServer 0.6 (Python 3.11.4)\n9090/tcp open  ssl/zeus-admin?\n1 service unrecognized despite returning data. If you know the service/version, please submit the following fingerprint at https://nmap.org/cgi-bin/submit.cgi?new-service :\nSF-Port9090-TCP:V=7.91%T=SSL%I=7%D=8/18%Time=64DFB8E1%P=x86_64-redhat-linu\n...\n... For some reason nmap can't detect cockpit so it's just 20 lines of this nonsense.\n...\n20\\x20\\x20\\x20\\x20\\x20}\\n\nSF:\\x20\\x20\\x20\\x20\\x20\\x20\\x20\\x20p\\x20\");\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 186.81 seconds\nNmap detects my python http server, and cockpit.\nAlright, next I want to configure firewall usage. I don’t want the remote wireguard server to have access to everything on my “home” network. I am likely going to do this by creating vlan’s, which can be wifi, by creating virtual wireless, or ethernet, by making it so that certain ethernet ports are trapped in a vlan. With this setup, I can ensure that my server is kept isolated from the rest of my devices, in case it gets compromised. (Although doing it this way is kinda pointless since I may be using a VPN on my devices, running through that same server to get around college wifi restrictions, if they exist).\nSo I setup wireguard on my server:\n\n\n/etc/wireguard/wg-stack.conf\n\n[Interface]\nAddress = 10.10.11.1/24\nPostUp = wg set %i private-key /etc/wireguard/%i.key\nListenPort = 51000\n\n[Peer]\n# My router\nPublicKey = mFyQQk8/w7AhLSEtJKkcNhMNLPcyBMFHu02TI+OUj2Y=\nAllowedIPs = 10.10.11.0/24,192.168.17.0/24\n\nAnd here is the wg-stack.sh script that will be run on my router:\n\n\nwg-stack.sh\n\n# enabled by default\n#echo 1 &gt; /proc/sys/net/ipv4/conf/br2/forwarding\n\n#iptables -t nat -A POSTROUTING -o br2 -j MASQUERADE\n#iptables -A FORWARD -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT\nmodprobe wireguard\nip link add wg-stack type wireguard\nwg setconf wg-stack /jffs/wg-stack-setconf\nip -4 address add 10.10.11.0/24 dev wg-stack\nip link set mtu 1420 up dev wg-stack\niptables -A FORWARD -i wg-stack -o br2 -j ACCEPT\n\nSome of the rules were enabled by default, so I commented them out.\nAnd here is the wg-stack-setconf file that the above script calls upon:\n\n\n/jffs/wg-stack-setconf\n\n[Interface]\nPrivateKey = NO\n#Address = 10.10.0.0/24\nListenPort = 51000\n\n[Peer]\n# Contabo VPS\nPublicKey = xO2fVY8uh4SDx5VH+24Mxx+WIXSnfY3Vw9CDBW7cMnY=\nAllowedIPs = 10.10.11.0/24\nEndpoint = &lt;VPS IP&gt;:51000\nPersistentKeepalive = 25\n\nI have to manually specify the private key, because the wg setconf can’t run abitrary commands.\nThis works… one way.\nroot@unknown:/jffs# ping 10.10.11.1\nPING 10.10.11.1 (10.10.11.1): 56 data bytes\n64 bytes from 10.10.11.1: seq=0 ttl=64 time=43.491 ms\nroot@unknown:/jffs# curl 10.10.11.1:8000\n&lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01//EN\" \"http://www.w3.org/TR/html4/strict.dtd\"&gt;\n&lt;html&gt;\n....\n.... I ran a simple http server to test if my router could see ports of my server.\n....\n&lt;/html&gt;\nHowever, when my server attempts to ping or curl my router, it simply waits forever. I think this may have something to do with firewalll rules, where the router is refusing to respond to even ping connections.\nAfter doing some more research, it appears my iptables rules deny all connections to my router, and I can attempt to access subnets other than my router itself, but they just redirect to my router:\n[root@vmi1403809 ~]# ping 192.168.17.152\nPING 192.168.17.152 (192.168.17.152) 56(84) bytes of data.\n64 bytes from 192.168.17.152: icmp_seq=1 ttl=63 time=41.9 ms\n[root@vmi1403809 ~]# curl 192.168.17.152:8000\ncurl: (7) Failed to connect to 192.168.17.152 port 8000: No route to host\n[root@vmi1403809 ~]# nmap -sV 192.168.17.152\nStarting Nmap 7.91 ( https://nmap.org ) at 2023-08-19 20:18 PDT\nNmap scan report for 192.168.17.152\nHost is up (0.043s latency).\nNot shown: 999 filtered ports\nPORT   STATE  SERVICE VERSION\n22/tcp closed ssh\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 5.53 seconds\nThe nmap scan is especially strange, considering I am using my laptop to test, which doesn’t have an ssh server running. My router however, does, but it also has an http server running (the web management interface), which nmap doesn’t seem to see. I am guessing my router is not playing the role of NAT properly.\nNope, I was wrong. The problem was not my router firewall, but rather my computer firewall. I forgot that unlike Arch Linux, the Arch Linux based linux distribution I was using, CachyOS did come with a firewall enabled.\nAfter disabling the firewall:\n[root@vmi1403809 ~]# nmap -sV 192.168.17.152\nStarting Nmap 7.91 ( https://nmap.org ) at 2023-08-20 03:21 PDT\nNmap scan report for 192.168.17.152\nHost is up (0.042s latency).\nNot shown: 999 closed ports\nPORT     STATE SERVICE VERSION\n8000/tcp open  http    SimpleHTTPServer 0.6 (Python 3.11.4)\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 7.78 seconds\nFor testing purposes, I run a simple http server using python -m http.server which launches this service in port 8000.\nAlright, upon later investigation, it appears I was both right and wrong.\nThe iptables -A FORWARD -i wg-stack -o br2 -j ACCEPT only accepts forwarded packets, giving it access to devices in the br2 vlan, whereas input in the one that makes it respond to ping probes or show servcies. This is actually good, because my router won’t be exposed to my rented server, only the subnet behind it will. Without a policy to accept input packets, the router won’t even respond to ping probes.\nHowever, I do have one final thing to handle:\nroot@unknown:/jffs# ./wg-stack.sh \nRTNETLINK answers: Network is unreachable\nHowever, when I run all the steps individually, this error doesn’t occur. This error occurs when I run the ip link set mtu 1420 up dev wg-stack, before the previous step in the script, assigning the ip address to the interface, isn’t done. I am guessing that the script runs too quickly, and some steps are run before the previous step is complete. I simply need to add a sleep command, which inserts an artificial wait in the process.\nWith all said and done, here is the script I have placed on my router:\n\n\n/jffs/wg-stack.sh\n\nmodprobe wireguard\nsleep 2s\nip link add wg-stack type wireguard\nsleep 2s\nwg setconf wg-stack /jffs/wg-stack-setconf\nsleep 2s\nip -4 address add 10.10.11.0/24 dev wg-stack\nsleep 3s\nip link set mtu 1420 up dev wg-stack\nsleep 2s\niptables -A FORWARD -i wg-stack -o br2 -j ACCEPT\n\nAnd following the advice of the freshtomato wiki, I have placed this script in the firewall section:\nuntil [ $(ping -c 1 -A -W 5 -q google.com &&gt;/dev/null && echo 1 || echo 0) -eq 1 ]; do sleep 5; done; /jffs/wg-stack.sh\nOn my remote server, I can simply enable the wg-quick service\nsystemctl enable wg-quick@wg-stack\nAnd now wireguard will be started automatically on both my router and the server.\nBecause my home server, is not accessible to the internet, I must proxy the connection through the contabo vps before I can get in. My ssh config uses the ProxyJump feature to proxy the connection from one vps into another.\n\n\n~/.ssh/config\n\nHost moontron\n                HostName VPS IP\n                port 22\n                user root\n                IdentityFile /home/moonpie/.ssh/moontron\nHost moonstack\n                Hostname 192.168.17.197\n                port 22\n                user root\n                IdentityFile /home/moonpie/.ssh/moonstack\n                ProxyJump moontron\n\nWith this, I can ssh into my moonstack machine from anywhere in the world.\nAnd because ansible can read the ssh config, I can simply plug the host nicknames into my ansible inventory, and my ansible can configure them, from anywhere in the world.\n\n\nVPS Networking\nOf course, this now makes my kolla-ansible installation more complex. Because I have now have two servers, to distribute to, and I also to deal with having to wireguard into my servers in order to be able to ssh into them.\nBeyond that, I have switched from a single node installation, to a multi node installation.\nNow that I have ssh and networking set up, I need to select the features and functions of my nodes.\nKolla-ansible sorts nodes into 4 types.\n\nControl: These run the dashboards, api and the like.\nNetworking: This runs the loadbalancers, virtual networking, and the like\nCompute: VM’s and containers are hosted here\nStorage: Various types of storage are on this node.\n\nFrom the the kolla-ansible globals:\n# This is the raw interface given to neutron as its external network port. Even\n    # though an IP address can exist on this interface, it will be unusable in most\n    # configurations. It is recommended this interface not be configured with any IP\n    # addresses for that reason.\n    #neutron_external_interface: \"eth1\"\nBased on this, it appears that if I attempt to use the singlular ethernet interface on my vps, then I will end up breaking my setup, as that ethernet interface will stop working normally.\nI found something interesting on the openstack docs. Apparently, this setup is possible, but “non trivial” to set up persistently, however, it is automated by Kayobe.\nI went through kayobe’s documentation, and found a youtube tutorial, and the deployment looks to be much more complex than kolla-ansible, which is frustrating to discover.\nDespite Kolla-ansible’s claims of creating a bridge interface and virtual ethernet’s persistently being “non-trivial”, I think they are easier to set up than the alternative options available me.\nBoth openstack-ansible, and kayobe, simply aren’t as well documented. So it’s back to networking.\nI found a useful article from Red Hat, which is a very good overview of what virtual network interfaces I have available to me on a modern linux sysytem.\nThe two that it looks like I need to pick, are either macvlan, or bridge mode. Both of those modes allow you to create virtual ethernet interfaces\n\nMacvtap\nBecause Rocky Linux uses networkmanager, I will use the nmcli tool to do stuff.\nnmcli connection add type macvlan dev enp1s0 mode bridge tap yes ifname macvtap0\n....\n\n\nip a\n....\n4: macvtap0@enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 500\n    link/ether 56:19:3c:7a:b7:e5 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.122.78/24 brd 192.168.122.255 scope global dynamic noprefixroute macvtap0\n       valid_lft 3344sec preferred_lft 3344sec\n    inet6 fe80::2c2:9892:c942:1fec/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\nThis simply creates a device and all, which is able to get ip’s from the external world, and in this case, the parent ethernet interface can still be used normally.\nYou can disable automatic configuration of ipv4/v6 with\nnmcli connection modify macvlan-macvtap0 ipv6.method \"disabled\"\nnmcli connection modify macvlan-macvtap0 ipv4.method \"disabled\"\nWhich I will probably need, as I won’t be paying for another ipv4 address.\nThe nice thing about using NetworkManager is that configuration using NetworkManager is persistent across reboots. Although I don’t want to jump to conclusions, it’s possible that “non-trivial to set up in a persistent manner” is based on older information.\nI further expermimented with macvlan. Since openstack will probably be doing more advanced networking, I attempted to create another macvtap device with the parent being the already existing macvtap device. It got created, but it didn’t get internet access, nor did it automatically get it’s own ip address. I don’t know what this means when it comes to how openstack neutron will interact with it, as neutron may want to do something like that.\nI found a wordpress blogpost archive. In this blog, someone uses the openvswitch to create a bridge, in the necessary setup for neutron to be able to do stuff.\nBased on what that setup looks like, it doesn’t look like neutron is given an interface that is a slave to the bridge, but it looks like an interface which is under the bridge is created for my machine to use normally, while neutron consumes the bridge itself for usage and creation of more virtual interfaces. However, this guide is for deploying openstack neutron on bare metal, and kolla-ansible works by deploying in docker containers.\nI found another relevant doc on the openstack website, about using creating an all in one node using vlan’s, bridge interfaces. If I can figure out how do do this in networkmanager, I think it will actually work\nThe eth0.10 format creates a tagged vlan interface.\nI found a relevant document devstack with a single NIC. Devstack is a way of testing openstack for development purposes and it seems that they’ve anticipated that people deploy openstack from devices like a laptop, which only have one NIC. Their method involves using Open vSwitch to create a bridge, making the main ethernet interface a slave to the bridge, and then configuring the bridge accordingly, giving the new slave an ip address, and using that.\nHowever, I am still confused. The docs make it unclear: Do I give neutron a veth to use for it’s purposes, and use the bridge for network access, or do I give it the bridge, and use the veth created on the bridge to get internet connectivity for myself?\nBased on the earlier linked devstack article, it looks like it’s not a linux bridge that’s created, but a openvswitch bridge that’s created, and then something akin to a veth is given to the host device to use for normal networking. However, kolla-ansible doesn’t seem to make this kind of setup too easy…\nI did some testing, and I was able to add a macvtap interface on a veth interface.\nnmcli connection add type macvlan dev veth1 mode bridge tap yes ifname macvtap0 \nConnection 'macvlan-macvtap0' (d2ea3d24-f831-4508-b2cf-c448e64afe2a) successfully added.\n[moonpie@cachyos-x8664 moonpiedumplings.github.io]$ ip a\n... irrelevant stuff omitted. \n7: macvtap0@veth1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 500\n    link/ether 66:4b:2e:a7:22:f2 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.17.251/24 brd 192.168.17.255 scope global dynamic noprefixroute macvtap0\n       valid_lft 86395sec preferred_lft 86395sec\n    inet6 fe80::5541:1541:5340:ebbc/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\n[moonpie@cachyos-x8664 moonpiedumplings.github.io]$ curl google.com --interface macvtap0\n&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"&gt;\n&lt;TITLE&gt;301 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;\n&lt;H1&gt;301 Moved&lt;/H1&gt;\nThe document has moved\n&lt;A HREF=\"http://www.google.com/\"&gt;here&lt;/A&gt;.\n&lt;/BODY&gt;&lt;/HTML&gt;\n[moonpie@cachyos-x8664 moonpiedumplings.github.io]$ \nPutting a macvtap interface on a veth interface shows that veth interfaces are more versatile than macvtap interfaces, able to do things like allocate more ip address to sub interfaces.\nSo the next test is to see if I can put a bridge interface on a veth interface. If I can put a bridge interface on a veth interface, than openstack neutron can put a bridge interface on a veth interface, meaning I can simply give neutron a veth to consume.\nI attempted to add a bridge with the veth1 interface as a port, but the bridge sits forever on “configuring ip”.\nSo based on this experimentation, it seems as if that linux bridge is used as the main one.\nThere doesn’t appear to be a way to use an existing linux bridge as neutron’s interface, at least not with kolla-ansible. But this does give me a funny idea: what if I install kolla-ansible, lose network connectivity, and then use VNC access to the VPS to finish the configuration manually?\nI don’t think this would work though. I can’t guarantee that network connectivity will be maintained for the entirity of the setup, and if the connection was broken halfway through, then the entire thing could shatter.\nBased on the kolla docs and the git repo, it seems like kolla-ansible does support linuxbridge.\nSo I have two options: nested bridges, or setting up the main bridge and giving it to openstack, or somehow configuring the main openstack bridge in the middle of the install. I could modify the ansible playbooks to add the necessary veth interfaces in the middle of the install, but the issue is that kolla-ansible deploys everything in docker, and from a cursory look through, I can’t immediately figure out how to configure stuff outside the docker containers.\nThe next thing I will try is having a macvtap interface be a port of a bridge, as I haven’t tried that yet. If it works, I can play with the bridge, and see if I can do the veth interface trick, but rather than on a normal bridge, on a bridge that’s on a macvtap interface.\nI did further testing with macvtap. On a debian libvirt virtual machine, using the NAT networking in order to make testing easy, I created a a macvtap interface and then plugged it into a bridge. However, it sat at “configuring ip” forever. But, if I manually assigned it an ip, then I could access it remotely, like from the host machine, from the bridges new ip, but I couldn’t do the reverse. Ping and curl, when asked to use that interface, didn’t work.\nI am wondering if this is a firewall issue. Maybe packet forwarding by sysctl or iptables isn’t enabled?\nI attempted to add a veth interface to the bridge that has macvtap as a port. Same as the bridge, dhcp and auto configuring ip didn’t work, but I could give it what I believed to be a static ip — except I couldn’t access this static ip, or send stuff through it.\nA bridge under macvtap doesn’t appear to work fully.\nWhat about a bridge under a bridge? I created a bridge, plugged a veth into it, and made the veth on the other end a port of a new bridge. Rather than let it sit on “configuring ip”, I gave it a static ip. Same issue… until I ran:\nroot@debian:/home/moonpie# iptables -I DOCKER-USER -j ACCEPT\nroot@debian:/home/moonpie# curl google.com --interface bridge1\n&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"&gt;\n&lt;TITLE&gt;301 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;\n&lt;H1&gt;301 Moved&lt;/H1&gt;\nThe document has moved\n&lt;A HREF=\"http://www.google.com/\"&gt;here&lt;/A&gt;.\n&lt;/BODY&gt;&lt;/HTML&gt;\nroot@debian:/home/moonpie# \nIn this case, bridge0 is my primary bridge, and bridge1 is the “bridge under the bridge”. I added an iptables rule to accept EVERYTHING, and it worked, making me think that the firewall is the issue for macvtap/lan as well.\nI attempted to create a veth interface attached to the bridge interface, and I was suprised to see that it already got it’s own public ip. So apparently, it was iptables getting in the way. However, an iptables policy of “literally accept everything” isn’t very secure, so I need to isolate the necessary rules to have a nested bridge work.\nI am guessing that it’s probably something related to packet forwarding, and I need to configure iptables to happily send the packets from a difference ip through the main bridge without touching them.\nAlright, so after creating a macvtap interface, I realize something: it can send requests out, but I can’t actually access it.\n\n\nBridge + Veth\nI experimented with bridges. Using cockpit’s networkmanager configuration allows you to seamlessly create a bridge which you can still use as a normal ethernet interface for some reason. It even preserves existing ip configuration. But gives me an unanswered question: If bridges are more featureful by default, and can also be used for normal connectivity, why not always use a bridge? More features are better, right?\nAnyway, I was able to create a veth pair, enslave one to the bridge, and then the nonenslaved veth interface could be used as a normal ethernet interface.\nSteps:\nUse cockpit-networkmanager to create a bridge, and add the ethernet interface as a port.\nsudo ip link add veth0 type veth peer name veth1\nsudo ip link set veth0 master br0\nsudo ip link set veth0 up\nsudo ip link set veth1 up\nsudo nmcli connection add type ethernet ifname veth1 con-name veth1\nsudo nmcli connection up veth1\nAnd then that interface can be used normally. It got it’s own ip via dhcp from my router, and now my computer’s ethernet interface has two ip’s.\nBut this isn’t good enough for me. ip is not persistent (although it is scriptable, and I can rerun every boot), unlike NetworkManager configurations.\nHow can I create a veth interface using networkmanager?\nI did some searching around, and it seems like at some point, networkmanager added this option. To create a veth interface, you must set the property, veth.peer when you create an interface.\nSo like so (tested and working)\nnmcli con add type veth ifname veth0 con-name veth0 veth.peer veth1\nAnd to add a veth interface to a bridge using nmcli:\nnmcli connection add type bridge-slave ifname veth1 master bridge0\nNope, this doesn’t seem to work.\nThat creates a new connection, and breaks a lot. Now trying:\nnmcli connection modify veth1 master bridge0\nIt didn’t work, because veth1 seems not to be managed by networkmanager.\nOr alternatively, maybe I can create a veth already enslaved to a bridge.\nI tried this nmcli con add type veth ifname veth0 con-name veth0 peer veth1 master bridge0, but it didn’t work because the wrong veth was added to the bridge. veth0 is added to the bridge, but veth1 is not managed by networkmanager. How can I create a veth interface with the unmanaged veth peer added to the bridge?\nOr maybe I am going about this the wrong way. It appears I can force NetworkManager to manage interfaces by using (from this reddit post):\nnmcli device set $IFNAME managed yes\nOnce I do this, I can simply use cockpit to add the device to the bridge as a bridge member. And a reboot… and my veth1 device gets nuked.\nSo it seems I need to edit NetworkManager configurations to make it that the change is permanent.\nBut there might be an easier method. Looking at the udev rules:\n... (shorted for brevity)\n...\n# Virtual Ethernet device pair. Often used to communicate with a peer interface\n# in another net namespace and managed by libvirt, Docker or the like.\n# Generally we don't want to mess with those. One exception would be the\n# full system containers, like LXC or LXD. LXC containers run via libvirt\n# don't use udev, so this doesn't apply. LXD does, though. To deal with the\n# LXD situation, let's treat the devices called eth* as regular ethernet.\nENV{ID_NET_DRIVER}==\"veth\", ENV{INTERFACE}!=\"eth[0-9]*\", ENV{NM_UNMANAGED}=\"1\"\nThese udev rules are set up in a way that veth devices named eth* will end up being managed by NetworkManager. This definitely seems exploitable.\nSo I do: nmcli con add type veth ifname veth0 con-name veth0 veth.peer eth1\nand sure enough, both interfaces are managed by NetworkManager. Using the web ui, I can add the “veth0” interface to the bridge, and it gets it’s own ip and everything. So what’s happening here?\nBy default, NetworkManager does not manage veth interfaces. But it does manage any interface it does create. When creating a veth interface using NetworkManager, it ends up being a weird scenario where only the main interface is managed, but the peer created is not, and I couldn’t do anything with it. But now I can.\nI created a macvtap interface on top of the eth0 interface and it automatically get’s it’s own ipv4 address. I can curl using the –interface macvtap0 option, and I can access services hosting on this machine via the ip address this interface has.\nIn addition to all this, it persists past a reboot.\nNow for the ultimate test: Nested bridging.\nSteps (assuming predictable network interface names are enabled, because then the names eth0 and eth1 are unused):\n\nCreate eth0 via cockpit and enslave the singular “physical” interface to it using the ui.\nnmcli con add type veth ifname veth1 con-name veth1 veth.peer eth1\nadd veth1 as a port to eth0 via cockpit.\nUse cockpit to disable automatic ip addresses for your bridge, while keeping it enabled.\n\nAnd now you have two network interfaces. Now I will attempt to do this again, creating a bridge on another bridge.\nAnd it works. eth1, gets internet access. I can also create a macvtap interface on the nested bridge, which works as I’d expect there as well.\nThis setup even persists a reboot.\nNow?\nTo do it all again but with a naming scheme that doesn’t suck.\nTo do it all again, but with docker ipables rules getting in the way at the same time.\nOr maybe I can use podman to deploy instead of docker?\nI went through the docs and they say nothing. But the source code does say something. In the container engine part of the ansible configuration, there is an option to set what container engine is being used. This means I can set podman instead of docker.\nHowever, this feels… A bit too far from standard practice. I will first see if I can get docker working.\nHere is the iptables chains with docker installed:\nroot@debian:/home/moonpie# iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy DROP)\ntarget     prot opt source               destination         \nDOCKER-USER  all  --  anywhere             anywhere            \nDOCKER-ISOLATION-STAGE-1  all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere             ctstate RELATED,ESTABLISHED\nDOCKER     all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere            \nACCEPT     all  --  anywhere             anywhere            \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain DOCKER (1 references)\ntarget     prot opt source               destination         \n\nChain DOCKER-ISOLATION-STAGE-1 (1 references)\ntarget     prot opt source               destination         \nDOCKER-ISOLATION-STAGE-2  all  --  anywhere             anywhere            \nRETURN     all  --  anywhere             anywhere            \n\nChain DOCKER-ISOLATION-STAGE-2 (1 references)\ntarget     prot opt source               destination         \nDROP       all  --  anywhere             anywhere            \nRETURN     all  --  anywhere             anywhere            \n\nChain DOCKER-USER (1 references)\ntarget     prot opt source               destination         \nRETURN     all  --  anywhere             anywhere            \nAnd here is the iptables rules without docker installed:\nroot@debian:/home/moonpie# iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nOh. That’s interesting. For whatever reason, docker takes the default forward policy of accept and throws it out the window. Why?\nSince I will be using firewalld as my firewall while it is running, here are the iptables rules with firewall-cmd activated, but no docker:\nroot@debian:/home/moonpie# iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nUh oh. Shouldn’t firewall-cmd change the iptables default input and forward policy to drop? What’s going on here?\nOkay, I’m going to forget the firewall for now. This is a bit too complex of an install.\nI did more research, and kolla-ansible has a bootrstap which happens before deploying openstack itself. This bootstrap phase does stuff like install docker. So my idea is to run the bootstrap phase before I configure the ethernet interfaces, so that everything is set up properly without docker interfering.\nI attempted to do this, by creating a rocky linux virtual machine, and running the kolla all in one installation on it. Interestingly:\nASK [openstack.kolla.baremetal : Set firewall default policy] *************************************************************************************************************************\nskipping: [localhost]\n\nTASK [openstack.kolla.baremetal : Check if firewalld is installed] *********************************************************************************************************************\nok: [localhost]\n\nTASK [openstack.kolla.baremetal : Disable firewalld] ***********************************************************************************************************************************\nchanged: [localhost] =&gt; (item=firewalld)\nKolla disables firewalld. I guess I don’t have to worry about that.\nIt gets most of the way through, but then fails at:\nASK [openstack.kolla.docker : Write docker config] ************************************************************************************************************************************\n Blah blah blah, lots of python errors, they are always messy\n \n original message: Interface 'eth0' not present on host 'localhost'\"}\n``\n\nSo apparently, the bootstrap step of this does more than just install docker and disable firewall. Apparently it tries to actually configure the network interfaces. \n\nSo I am going to switch to set up \"two\" interfaces, make kolla believe that I have two, and then give them to docker. \n\nI created a bridge, called \"eth0\" and a veth attached to it called \"eth1\". \n\nBy default, eth0 is the main interface, and eth1 is the neutron external interface. \n\nI run the bootstrap steps again, and it works without complaining. The prechecks don't complain as well. Now, it's time for the deployment. \n\nIt goes all the way until:\n\n```{.default}\nRUNNING HANDLER [loadbalancer : Wait for haproxy to listen on VIP] *********************************************************************************************************************\nfatal: [localhost]: FAILED! =&gt; {\"changed\": false, \"elapsed\": 300, \"msg\": \"Timeout when waiting for :61313\"}\n\nPLAY RECAP *****************************************************************************************************************************************************************************\nlocalhost                  : ok=61   changed=38   unreachable=0    failed=1    skipped=106  rescued=0    ignored=0   \nWhy did this fail? I’m guessing because I did not set the vip address in globals.yml, for high availability.\nBut I’m not creating a cluster rigth now, and my two node cluster won’t be able to use high availability either.\n\n\n/etc/kolla/globals.yml\n\n# This should be a VIP, an unused IP on your network that will float between\n# the hosts running keepalived for high-availability. If you want to run an\n# All-In-One without haproxy and keepalived, you can set enable_haproxy to no\n# in \"OpenStack options\" section, and set this value to the IP of your\n# 'network_interface' as set in the Networking section below.\n#kolla_internal_vip_address: \"10.10.10.254\"\n\nSo I can disable high availibility, which I will do and try again.\nExcept I’ve encountered an issue when attempt to get this setup. Although eth1 works normally, eth0 (the bridge) will not send any requests. Curl or ping simply time out. However, even more strangely, I can still access cockpit via the ip address that it has setup.\nFor whatever reason, this bridge setup behaves different on debian vs on rocky linux. Even after a reboot, it still doesn’t work.\nChange of plans. Rather than attempting to get away with the bridge also being able to act as a normal interface, I think I will just create two veths attatch them to a singular bridge which does not get an ip address.\nSo I create one bridge, create two veth pairs (eth1-veth1, and eth0-veth0), and attempt to curl using eth0. Nothing. I reboot. Same result.\nWhat?\nip route:\nroot@debian:/home/moonpie# ip route\ndefault via 192.168.122.1 dev eth1 proto dhcp src 192.168.122.97 metric 101 \ndefault via 192.168.122.1 dev eth0 proto static metric 425 \n192.168.122.0/24 dev eth1 proto kernel scope link src 192.168.122.97 metric 101 \n192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.4 metric 425 \n[moonpie@rocky ~]$ ip route\ndefault via 192.168.122.1 dev eth1 proto dhcp src 192.168.122.228 metric 101 \ndefault via 192.168.122.1 dev eth0 proto static metric 425 \n192.168.122.0/24 dev eth1 proto kernel scope link src 192.168.122.228 metric 101 \n192.168.122.0/24 dev eth0 proto kernel scope link src 192.168.122.111 metric 425 \niptables:\n[moonpie@rocky ~]$ sudo iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination       \nroot@debian:/home/moonpie# sudo iptables -L\nChain INPUT (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain FORWARD (policy ACCEPT)\ntarget     prot opt source               destination         \n\nChain OUTPUT (policy ACCEPT)\ntarget     prot opt source               destination         \nBoth have firewalls disabled. The only noticable difference between the two is the version of the linux kernel in use. Debian is using Linux debian 6.1.0-11-amd64, whereas rocky is using Linux rocky 5.14.0-284.25.1.el9_2.x86_64.\nThere are several things to test. The first is to test a different kernel version, because this could be a kernel bug, relating to linux bridges.\nAnother thing to test is if this is a networkmanager bug, so I could attempt to not use networkmanager to create bridges, and instead use ip.\nSo the first thing I try is a newer kernel. The ELrepo is an unofficial repo for Red Hat Enterprise Linux (RHEL, or EL), and clones, like my rocky linux. The steps to add it are on the linked page.\nThen, I nabbed the kernel-ml package, which is the latest stable mainline linux kernel. Now, my rocky linux virtual machine is using Linux rocky 6.5.1-1.el9.elrepo.x86_64.\nAnd same thing.\nSo it’s probably not a kernel bug. Probably. The possibility that the RHEL, or the ELRepo kernel has some difference that causes this bug, is very low.\nWhen looking at the cockpit page for networking, I see that eth0 (my bridge) is receiving data, but the sending data stays at 0. Strange.\nIt probably would have to be a difference between the network configuration between the two distros. Either networkmanager is configured differently by default, or there is a bug in the networking stack.\nFor example, in debian:\n\n\n/etc/NetworkManager/NetworkManager.conf\n\nroot@debian:/etc/NetworkManager# cat NetworkManager.conf \n[main]\nplugins=ifupdown,keyfile\n\n[ifupdown]\n# I changed this from false to true to force NetworkManager to manage every single thing, including the ethernet interface which it didn't do by default. \nmanaged=true \n\nOn the other hand, rocky has everything in this file commented out.\nAnd yup. When I change the relevant options in rocky’s NetworkManager.conf, it works now.\n\n\n/etc/NetworkManager/NetworkManager.conf\n\n.....\n.....\n[main]\n# Default is: \n#plugins=keyfile,ifcfg-rh\n# commented out, of course. I added ifupdown and didn't remove ifcfg-rh, just in case it's needed, but it shouldn't be. \nplugins=keyfile,ifcfg-rh,ifupdown\n\n[ifupdown]\nmanaged=true\n.....\n....\n\nEDIT 6/2/24:\nIt seems that this isn’t enough for this to properly work. On debian, I needed to run apt purge ifupdown in order for things to work properly, otherwise the ethernet interface will get two ip addresses, as it is configured twice by ifupdown and networkmanager. Although I doubt ifupdown will cause interference beyond simply two ip addresses, I want to remove variables.\nAnd this still isn’t enough. I have to edit /etc/network/interfaces and comment out the things related to the default ethernet interface.\n\n\n/etc/network/interfaces\n\n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\n#allow-hotplug enp0s25\n#iface enp0s25 inet dhcp\n\nAnd it works.\nAnd it works now:\n[root@rocky moonpie]# ping google.com -I eth0 -c 1\nPING google.com (142.250.72.174) from 192.168.122.111 eth0: 56(84) bytes of data.\n64 bytes from lax17s50-in-f14.1e100.net (142.250.72.174): icmp_seq=1 ttl=113 time=4.81 ms\n\n--- google.com ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 4.808/4.808/4.808/0.000 ms\n[root@rocky moonpie]# ping google.com -I eth1 -c 1\nPING google.com (142.250.72.174) from 192.168.122.150 eth1: 56(84) bytes of data.\n64 bytes from lax17s50-in-f14.1e100.net (142.250.72.174): icmp_seq=1 ttl=113 time=14.9 ms\n\n--- google.com ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 14.887/14.887/14.887/0.000 ms\nWhy does this work now?\nJust kidding, this setup doesn’t survive a reboot. But it survives a reboot on debian?\nOkay, I played around a little. The correct order is:\n\nEdit NetworkManager.conf\nReboot\nThen do bridge changes.\nReboot just to make sure they persist.\n\nAnd this persists beyond a reboot.\nNow I will test an all in one installation on a virtual machine.\nSo I set it up… and same issue. I think I may need both a newer kernel, and the nmcli changes.\nI install a newer kernel again, and it works. Wow. I think I may actually be encountering a bug. Fascinating.\nBut that’s besides the point. Now I will attempt to do an all in one installation of openstack using kolla ansible.\nThe main difficulty I am anticipating is the fact that installing docker changes the forward networking rules, from the default accept to a policy of drop.\nBut lucky me, that appears to be a debian thing. My network setup still works even after running the bootstrap step.\nI even ran the pre run and the deploy steps, and they worked fine until I ran out of memory.\nAfter adding more memory to the virtual machine, it worked:\nAfter going to the “Using Openstack”, and running the post deploy steps to create an admin password, I have an openstack system:"
  },
  {
    "objectID": "projects/build-server-2/index.html#monitoring",
    "href": "projects/build-server-2/index.html#monitoring",
    "title": "Building my own Server Part 2 — Software",
    "section": "Monitoring",
    "text": "Monitoring\nhttps://github.com/openstack-exporter/openstack-exporter"
  },
  {
    "objectID": "projects/build-server-2/index.html#cinder",
    "href": "projects/build-server-2/index.html#cinder",
    "title": "Building my own Server Part 2 — Software",
    "section": "Cinder",
    "text": "Cinder\nConfiguring cinder seems to require more than simply enabling the service, according to the kolla-ansible docs. I’ve selected the nfs storage backend, so I followed the instructions, but there is no nfs service available.\nsudo dnf install nfs-utils makes the service available to me.\nAnd the docs are wrong, the name of the nfs system service is not nfs or nfsd, but actually:\nsystemctl enable --now nfs-server, at least on rocky linux.\nCinder seems to be the volume mount feature that a minimal openstack install is missing. With it, I can do things like the volume mounts that I expected openstack to have by default."
  },
  {
    "objectID": "projects/build-server-2/index.html#nova",
    "href": "projects/build-server-2/index.html#nova",
    "title": "Building my own Server Part 2 — Software",
    "section": "Nova",
    "text": "Nova\nMain openstack doc for nova\n\nLXC\nNova is one of the default services. By default, it uses libvirt to configure qemu + kvm. This is not satisfactory for me, since I have an nvidia gpu and it is a bit easier to use with containers, such as docker or lxc. Because I don’t have a “server” gpu, I won’t be able to get access to vgpu, a feature which enables you to split a gpu between multiple virtual machines.\nHowever, it is possible to split a gpu between multiple containers even without any special features.\nOne interesting thing is that lxc and docker are just a frontend for a bunch of linux kernel features, so all you need is a supported kernel. According to the arch wiki article for libvirt, you don’t need to install any lxc related tools to actually use libvirt with lxc, you simply point libvirt at the lxc:/// connection. Note Arch linux doesn’t split packages, so everything libvirt needs is probably in one package. On ubuntu, for example, the openstack nova-lxc article mentions how on ubuntu, another package is needed to be installed.\nBut is it even possible to configure a multi hypervisor openstack cloud? One reddit commenter says no. In addition to that, the libvirt_virt_type is of the type “string” rather than the type “list”. This means that I would only be able to put one item here, either kvm or lxc.\nOne thing I could consider doing is running a privileged container on the main compute node, and then make it another compute node which another virtualization type.\nThe main issue with such a setup is how I would handle networking devices. If I run another container on the main compute node, then running it in privileged mode gives it access to all devices, which means all networking devices.\nNo wait, this isn’t actually an issue because I can just configure it so that the container uses different network interfaces. It will still see them, which might cause issues, but it won’t be using them.\nSo how can I run a privileged container with systemd?\nI found a relevant article from Red Hat.\n\n\nGPU passthrough\nHowever, even if I do that, then\nPCI device passthrough\nBased on this doc, it looks like I need to create flavors for virtual machines, which have the allocated gpu’s set up.\nI may also want an external libvirt daemon, because that may be easier to set up pci device passthrough and whatnot.\nI found a reddit post where someone was asking for help about PCI passthrough with kolla-ansible, but no one has replied as of yet.\nA few days later, they replied to their own post.\n\nJust in case anyone searches for this in the future: the file in /etc/kolla/config/nova needs to be named nova-compute.conf (NOT nova.conf) and then it will be added to the configuration."
  },
  {
    "objectID": "projects/build-server-2/index.html#zun-1",
    "href": "projects/build-server-2/index.html#zun-1",
    "title": "Building my own Server Part 2 — Software",
    "section": "Zun",
    "text": "Zun\nZun is the container service. https://docs.openstack.org/kolla-ansible/latest/reference/compute/zun-guide.html.\nConfiguring this is pretty easy, but it should be noted that the user running kolla-ansible must be a member of the docker group, and I don’t know how to set that up before docker is installed, so you have to run bootstrap first, and then set up users and groups.\nAccording to zun’s policy rules it is not possible to create privileged containers by default. Also, Ubuntu’s launchpad has something about this as well.\nOne thing I am thinking about is how to passthrough gpu’s and other things to containers. If I desire to do something like an AI trainig on docker containers (since a lot is done on kubernetes nvidia clusters), then I would end up using Zun for it.\nI found an article in zun’s git saying that it was in the works, but that document is literally 5 years old…\nI also found another 6 year old document about zun. In this doc, it says that pci passthrough has been implemented for zun.\nBecause the docs are unclear, I decided to go through the source code.\nThe source code for the pci config options for zun\nAnd here is the source code for the pci config options for nova.\nBecause they are the exact same thing, I am guessing I can use the configuration options here and here.\nOh, I found something better. The sample zun configuration has a pci passthrough options.\nHowever, the zun step kept failing when I attempted to use kolla-ansible. Apparentlly the master’s release won’t be supporting zun for a while, because they have to update a lot of dependencies or something like that. Here is the git commit with relevant changes.\nBecause of the above, I decided to switch to the 2023.1 release of kolla-ansible, rather than the master release, which it was probably a bad idea to be using."
  },
  {
    "objectID": "projects/build-server-2/index.html#cyborg",
    "href": "projects/build-server-2/index.html#cyborg",
    "title": "Building my own Server Part 2 — Software",
    "section": "Cyborg",
    "text": "Cyborg\nCyborg is management of “hardware acceletors”, like FGPA’s, or in my case, gpu’s.\nThe configuration is here:\nHowever, given the ability to do PCI passthrough with zun and nova already, I don’t think I will need this service."
  },
  {
    "objectID": "projects/build-server-2/index.html#skyline",
    "href": "projects/build-server-2/index.html#skyline",
    "title": "Building my own Server Part 2 — Software",
    "section": "Skyline",
    "text": "Skyline\nSkyline is the next gen web ui for openstack. By default, only horizon, the older ui is installed.\nAccording to this doc skyline serves on 9999 by default. But according to this doc skyline serves on port 8088 by default. But:\nnmap scan report for 192.168.122.111\nHost is up (0.000092s latency).\nNot shown: 990 closed tcp ports (reset)\nPORT     STATE SERVICE         VERSION\n22/tcp   open  ssh             OpenSSH 8.7 (protocol 2.0)\n80/tcp   open  http            Apache httpd\n3306/tcp open  mysql           MySQL 5.5.5-10.11.5-MariaDB-log\n4567/tcp open  tram?\n5000/tcp open  http            Apache httpd\n8000/tcp open  http            Apache httpd\n8022/tcp open  ssh             OpenSSH 8.7 (protocol 2.0)\n9090/tcp open  ssl/zeus-admin?\n9998/tcp open  distinct32?\n9999/tcp open  http            nginx\nSince only 9999 is up, I decided to visit it, but it timed out.\nI think the issue is that I’m running out of ram. It might also be CPU, but the monitoring reports that ram is full, although I have a lot of cpu leftover."
  },
  {
    "objectID": "projects/build-server-2/index.html#magnum-1",
    "href": "projects/build-server-2/index.html#magnum-1",
    "title": "Building my own Server Part 2 — Software",
    "section": "Magnum",
    "text": "Magnum\nhttps://docs.openstack.org/kolla-ansible/latest/reference/containers/magnum-guide.html\nSeems to be pretty simple to deploy. Needs an insecure option for autoscaling though, which doesn’t look too good."
  },
  {
    "objectID": "projects/build-server-2/index.html#neutron",
    "href": "projects/build-server-2/index.html#neutron",
    "title": "Building my own Server Part 2 — Software",
    "section": "Neutron",
    "text": "Neutron\nThe documentation for configuring neutron using kolla-ansible is here.\nI want to enable provider networks for speed for the ipv4 subnet of my router, but I still don’t understand how neutron external subnets, and neutron networking in general works. The main question is this: Can I put virtual machines on an external subnet that is only available to a different networking node?\nIt should be possible.\nTo list the openstack physical networks, you can run:\nopenstack network agent list --agent-type L2 agent\nTo list L3 agents:\nopenstack network agent list --agent-type L3 agent\nI am guessing I can just list all the agents at once.\nopenstack network set --binding:host_id=&lt;hostname&gt; &lt;network-name-or-ID&gt;\nAnd to verify: openstack network show &lt;network-name-or-ID&gt; -c binding:host_id\nJust kidding, those commands were given by chatgpt and they fail. Running them returns:\n(venv) [root@thoth ~]# openstack network agent list --agent-type L2 agent\nusage: openstack network agent list [-h] [-f {csv,json,table,value,yaml}] [-c COLUMN] [--quote {all,minimal,none,nonnumeric}] [--noindent]\n                                    [--max-width &lt;integer&gt;] [--fit-width] [--print-empty] [--sort-column SORT_COLUMN] [--sort-ascending | --sort-descending]\n                                    [--agent-type &lt;agent-type&gt;] [--host &lt;host&gt;] [--network &lt;network&gt; | --router &lt;router&gt;] [--long]\nopenstack network agent list: error: argument --agent-type: invalid choice: 'L2' (choose from 'bgp', 'dhcp', 'open-vswitch', 'linux-bridge', 'ofa', 'l3', 'loadbalancer', 'metering', 'metadata', 'macvtap', 'nic', 'baremetal')\nAnd just listing all agents (in my 2 node install with both with the same subnets):\n(venv) [root@thoth ~]# openstack network agent list\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n| ID                                   | Agent Type         | Host         | Availability Zone | Alive | State | Binary                    |\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n| 2fb24192-27bc-4d4b-8263-cadc1a73c3b0 | DHCP agent         | network-node | nova              | :-)   | UP    | neutron-dhcp-agent        |\n| 429b16e3-bd4f-4507-ae78-6336a12b6b87 | Open vSwitch agent | network-node | None              | :-)   | UP    | neutron-openvswitch-agent |\n| f7e2987b-a6e6-4677-b627-9a32e0508dc9 | Metadata agent     | network-node | None              | :-)   | UP    | neutron-metadata-agent    |\n| fd2a566d-1d47-470d-b0f6-49972b02813a | Open vSwitch agent | main-node    | None              | :-)   | UP    | neutron-openvswitch-agent |\n| ffa989a0-42b4-4184-ba8d-72106ee32c89 | L3 agent           | network-node | nova              | :-)   | UP    | neutron-l3-agent          |\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n\nI did another install, an all in one node (including networking), and a network node. The all-in-one node has both interfaces on the libvirt NAT subnet, “default”, but the network node has it’s neutron external interface on a different libvirt NAT subnet.\nHere is the networking agent list:\n(venv) [root@thoth ~]# openstack network agent list\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n| ID                                   | Agent Type         | Host         | Availability Zone | Alive | State | Binary                    |\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n| 07aa201f-08b6-4dc9-ac1c-98af105ad53e | L3 agent           | main-node    | nova              | :-)   | UP    | neutron-l3-agent          |\n| 1afb9cd1-d1d0-4d59-ba58-f291868222da | Metadata agent     | network-node | None              | :-)   | UP    | neutron-metadata-agent    |\n| 349bbed8-3526-4070-affe-a47c2bbe5985 | DHCP agent         | main-node    | nova              | :-)   | UP    | neutron-dhcp-agent        |\n| 69b660b0-1bbc-4474-8668-71308486f3a8 | DHCP agent         | network-node | nova              | :-)   | UP    | neutron-dhcp-agent        |\n| a62c5d36-8101-4ca3-8479-358657684273 | Metadata agent     | main-node    | None              | :-)   | UP    | neutron-metadata-agent    |\n| b6221a20-3543-43c7-9a99-ce143a699702 | Open vSwitch agent | main-node    | None              | :-)   | UP    | neutron-openvswitch-agent |\n| c0bb80a4-c9e7-48a9-a56a-5fa279bfcef0 | Open vSwitch agent | network-node | None              | :-)   | UP    | neutron-openvswitch-agent |\n| d212772f-4996-413c-b8ef-c1e6b5442b22 | L3 agent           | network-node | nova              | :-)   | UP    | neutron-l3-agent          |\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\nHow can I ensure that the truly external network I create is attatched to a specific L3 agent?\nChatgpt says to use availibility zones.\nHowever, one thing I am looking at that might be simpler is a multi regional deployment: https://docs.openstack.org/kolla-ansible/latest/user/multi-regions.html\nI also tried a multi regional deployment. In addition to not working, giving me any kinds of zones to specify, chatgpt possible\nI did some searching around and I found an interesting option in the ansible variables.\n# Set OVN network availability zones\nneutron_ovn_availability_zones: []\nHowever, I am guessing that I need to enable OVN to use this, so I followed the kolla-ansible docs for this.\nI set the variable of neutron availibility zones to be edge, on my network only node, and main, on my main compute node. However, it seems like my variables were ignored:\n(venv) [root@thoth ~]# openstack network agent list\n+--------------------------------------+------------------------------+--------------+-------------------+-------+-------+----------------------------+\n| ID                                   | Agent Type                   | Host         | Availability Zone | Alive | State | Binary                     |\n+--------------------------------------+------------------------------+--------------+-------------------+-------+-------+----------------------------+\n| 1ca0eec3-22f3-4680-b4c6-2d6e97ab6b8b | DHCP agent                   | main-node    | nova              | :-)   | UP    | neutron-dhcp-agent         |\n| 3f9fffd2-c3f7-4841-910e-bc73ba95ab53 | L3 agent                     | main-node    | nova              | :-)   | UP    | neutron-l3-agent           |\n| 4a11630f-bbaf-4d8e-87d1-deb209e770fe | Open vSwitch agent           | main-node    | None              | :-)   | UP    | neutron-openvswitch-agent  |\n| 820a9bbb-5279-4889-b511-979b1b7d327e | Open vSwitch agent           | network-node | None              | :-)   | UP    | neutron-openvswitch-agent  |\n| bf317078-bf85-488c-87c3-026a23c8793b | L3 agent                     | network-node | nova              | :-)   | UP    | neutron-l3-agent           |\n| e4e3d75b-55b5-4a1e-8bd2-632d8a37e5b9 | Metadata agent               | network-node | None              | :-)   | UP    | neutron-metadata-agent     |\n| ec64e75e-0e61-434e-908f-e4b9d670adbc | DHCP agent                   | network-node | nova              | :-)   | UP    | neutron-dhcp-agent         |\n| f5ea5528-e482-4609-ae33-09ef2d792b28 | Metadata agent               | main-node    | None              | :-)   | UP    | neutron-metadata-agent     |\n| main-node                            | OVN Controller Gateway agent | main-node    | main              | XXX   | UP    | ovn-controller             |\n| c484098d-fd7d-5d9c-81e5-be62657e571e | OVN Metadata agent           | main-node    | main              | XXX   | UP    | neutron-ovn-metadata-agent |\n+--------------------------------------+------------------------------+--------------+-------------------+-------+-------+----------------------------+\n(venv) [root@thoth ~]# openstack availability zone list\n+-----------+-------------+\n| Zone Name | Zone Status |\n+-----------+-------------+\n| internal  | available   |\n| nova      | available   |\n| nova      | available   |\n| nova      | available   |\n| nova      | available   |\n| main      | available   |\n+-----------+-------------+\n(venv) [root@thoth ~]# openstack availability zone list --network\n+-----------+-------------+\n| Zone Name | Zone Status |\n+-----------+-------------+\n| nova      | available   |\n| nova      | available   |\n| main      | available   |\n+-----------+-------------+\nAnd also, when I look in the UI, to create a network, it only lists the “nova” availibility zone hint.\nWhat happened to my edge availability zone?\nAnyway, I decided to do more searching. Apparently, the neutron_ovn_zones are set up here, which is ran as long as the host is a member of the ovn-network-controller group, which is a child of the “network” group. Based on this, I don’t think I need to enable OVN in order to use OVN availability zones. It seems like some portion of OVN is deployed, but only the whole thing is used as a network backend if you enable it.\nSo because of this, I attempted to do another deployment with without setting OVN, but just availability zones.\n(venv) [root@thoth ~]# openstack network agent list\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n| ID                                   | Agent Type         | Host         | Availability Zone | Alive | State | Binary                    |\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n| 750d6eb8-a89b-495e-86ab-65a9235d963c | Open vSwitch agent | network-node | None              | :-)   | UP    | neutron-openvswitch-agent |\n| 7921c839-1935-4d29-888d-8efae2f31aff | L3 agent           | main-node    | nova              | :-)   | UP    | neutron-l3-agent          |\n| 7cb4df27-5d40-429d-816e-8336d7a9867c | Metadata agent     | network-node | None              | :-)   | UP    | neutron-metadata-agent    |\n| ad40304b-b078-434d-bb6d-8e4087f7c7e1 | Open vSwitch agent | main-node    | None              | :-)   | UP    | neutron-openvswitch-agent |\n| b9770fe2-6a6a-4f77-9c5e-c0b058eb6966 | L3 agent           | network-node | nova              | :-)   | UP    | neutron-l3-agent          |\n| d02ba59c-cc43-4a5d-bff0-98bd3713ba53 | Metadata agent     | main-node    | None              | :-)   | UP    | neutron-metadata-agent    |\n| e22251c8-053d-47a7-8980-0e17e2b15341 | DHCP agent         | network-node | nova              | :-)   | UP    | neutron-dhcp-agent        |\n| f46c9a7d-d67b-49bf-9d7a-d9c268fd3229 | DHCP agent         | main-node    | nova              | :-)   | UP    | neutron-dhcp-agent        |\n+--------------------------------------+--------------------+--------------+-------------------+-------+-------+---------------------------+\n(venv) [root@thoth ~]# openstack availability zone list\n+-----------+-------------+\n| Zone Name | Zone Status |\n+-----------+-------------+\n| internal  | available   |\n| nova      | available   |\n| nova      | available   |\n| nova      | available   |\n| nova      | available   |\n+-----------+-------------+\nThis is even worse, now it didn’t even create the main availability zone.\nOn the kolla-ansible docs for neutron, it mentions that if you have multiple physical networks, you need to name them differently.\nThe multinode documention details how to an ansible inventory to declare different variables for each service. Using this, I can replace change the physical interface names from physnet1 to something else. I am thinking physnet4 and physnet6, because of ipv4 and ipv6.\nI tried changing the bridge name, however, nothing changed.\nAccording to the jinja2 template which is used to generate the ml2-ini.conf\n[ml2_type_flat]\n{% if enable_ironic | bool %}\nflat_networks = *\n{% else %}\nflat_networks = {% for interface in neutron_external_interface.split(',') %}physnet{{ loop.index0 + 1 }}{% if not loop.last %},{% endif %}{% endfor %}\n{% endif %}\nThe bridge names variable is completely ignored, and the flat_networks variable is generated purely from neutron_external_interface variable.\nAlthough never directly stated, I think the docs are assuming that every network (and probably openstack, since neutron is also ran on the compute nodes) will have access to the exact same physical networks. However, in my setup, not every node will have access to the same networks. I think I have to use node-specific configurations to, to ensure that openstack understands that the physical network that my VPS has access to, and the one my server has access to, are not the same.\nOne thing I am thinking of trying is simply deploying openstack, editing /etc/kolla/neutron-server/ml2_conf.ini, and then rebooting, and seeing if that works. However, I don’t even now if this will actually work, so I will have to test.\nNow I’ve encountered a problem: this file does not exist on the pure network node. It only exists on the main node. Is it possible to configure koll-ansible to deploy this folder to all nodes?\nI found the exact playbook where the jinja2 template is deployed. I then did some more hunting and found out when this exact playbook is ran, when servers are in the neutron-server group.\nBut after looking throught the multinode inventory, I realized that it’s not network nodes that are in the neutron-server group, but rather only the control nodes.\nBased on this, it seems like that directory is purely for the control nodes, which run neutron-server, the API.\nBecause of this, I decided to add my network node to the neutron-server hosts, which resulted in this directory being created on the server. I then edited the file, but when I tried to create network with the physical net name physnet2 I got an error.\nI found another config file, where the bridges seem to map to physical network names. I edited this file:\n\n\n/etc/kolla/neutron-openvswitch-agent/openvswitch-agent.ini\n\n[ovs]\nbridge_mappings = physnet2:br-ex\n\nBut I still get the same error.\n(venv) [root@thoth kolla]# openstack network create --external --provider-physical-network physnet2 --provider-network-type flat public1\nError while executing command: BadRequestException: 400, Invalid input for operation: physical_network 'physnet2' unknown for flat provider network.\n(venv) [root@thoth kolla]# openstack network create --external --provider-physical-network physnet1 --provider-network-type flat public1\n+---------------------------+--------------------------------------+\n| Field                     | Value                                |\n+---------------------------+--------------------------------------+\n| admin_state_up            | UP                                   |\n| availability_zone_hints   |                                      |\n| availability_zones        |                                      |\n| created_at                | 2023-09-26T03:14:59Z                 |\n| description               |                                      |\n| dns_domain                | None                                 |\n| id                        | 378f8457-c50f-4554-91cf-905d9ec08ac7 |\n| ipv4_address_scope        | None                                 |\n| ipv6_address_scope        | None                                 |\n| is_default                | False                                |\n| is_vlan_transparent       | None                                 |\n| mtu                       | 1500                                 |\n| name                      | public1                              |\n| port_security_enabled     | True                                 |\n| project_id                | 1e28ab734f2d45d986259ee44a54ff42     |\n| provider:network_type     | flat                                 |\n| provider:physical_network | physnet1                             |\n| provider:segmentation_id  | None                                 |\n| qos_policy_id             | None                                 |\n| revision_number           | 1                                    |\n| router:external           | External                             |\n| segments                  | None                                 |\n| shared                    | False                                |\n| status                    | ACTIVE                               |\n| subnets                   |                                      |\n| tags                      |                                      |\n| tenant_id                 | 1e28ab734f2d45d986259ee44a54ff42     |\n| updated_at                | 2023-09-26T03:14:59Z                 |\n+---------------------------+--------------------------------------+\n(venv) [root@thoth kolla]# \nAlthough creating a network with the physical network physnet1 works, physnet2 doesn’t work.\nBut then, I did something that did work. I edited /etc/kolla/neutron-server/ml2_conf.ini on the main node, to contain physnet1,physnet. This worked, and then I could create a network with physnet2.\nBut if I put an instance on it, does it actually work? Well, I tried to create an image, and it failed but the openstack error is very vague. I have gotten this error due to not enough resources in the past, but based on some quick research, it can happen because of networks not being set up properly, either.\nI decided to look at the logs:\n\n\nmain-node:/var/logs/kolla/nova/nova-compute.log\n\n...\n2023-09-26 09:48:40.783 7 ERROR nova.compute.manager [instance: 0ef0f1be-e4aa-4140-89af-4402f992f18b] nova.exception.PortBindingFailed: Binding failed for port ee3ee85b-6edf-4ace-a921-10fcca3637b3, please check neutron logs for more information.\n....\n\nWell, that error is pretty explicit. What I am going to do is redeploy, but without the having two servers with the neutron-server role, and see if that works.\nSo I did the same thing, with only one neutron server, edited openvswitch.ini, and then attempted to create an instance on physnet2. It failed.\nI then created a network on physnet1, and created an instance on that network. It worked, getting an ip which I could access externally and all. I could also do the same with a floating ip on that network, and it would get external internet access, and I could reach it externally. So at least I have this system halfway working.\nRather than attempting to put a virtual machine directly on an external network, I instead sought to allocate it a floating ip address. Same error.\nI decided to search the configuration directories for more related to this:\n[root@main-node /etc/kolla]# find . -type f -exec grep -H 'physnet' {} +\n./neutron-server/ml2_conf.ini:flat_networks = physnet1,physnet2\n./neutron-openvswitch-agent/openvswitch_agent.ini:bridge_mappings = physnet1:br-ex\n\n[root@network-node /etc/kolla]# find . -type f -exec grep -H 'physnet' {} +\n./neutron-openvswitch-agent/openvswitch_agent.ini:bridge_mappings = physnet2:br-ex\nSo I can’t be missing a config file, since find seems not to find anything but those two files that I already edited.\nSo, I decided to do something that I probably should have done from the very beginning: I decided to test with a sole network node, with the main node no longer having that role.\nAnd it didn’t work. I’m not sure yet, but I’m guessing that the default method I was attempting to use for external network interfaction, provider networks, requires that the virtual machine be on the same host as the network is one, since it works by giving virtual machines direct physical access to networks.\nWhat I did, that works (mostly from init-runonce, the sample setup script):\n\nopenstack network create --external --provider-physical-network physnet1 --provider-network-type flat public1\n\n\nor just do it from the UI, create a flat network with the name, physnet1\n\n\nIn the horizon UI, give this subnet the proper options. That’s way easier than trying to edit the command given by init-runonce.\nopenstack network create demo-net\n\n\nCreate a subnet in the horizon UI\n\n\nopenstack router create demo-router\nopenstack router add subnet demo-router demo-subnet\nopenstack router set --external-gateway public1 demo-router\n\n\nNote: according to init-runonce, the command is different for ipv6 external networks.\n\nThen, you can create an instance on that internal subnet. After the instance is created, you should be able to allocate it a floating ip address. When security groups were set up properly, I could ping and nmap it.\nHowever, I encountered a hiccup already: The virtual machine does not see it’s ip address, and cannot bind to it for stuff. But that’s a problem for after I verify that this setup works on my full, two network node setup.\nI attempted to set it up this way, but unlike my single network node setup, my virtual machine has no internet access. In addition to that, the floating ip cannot be accessed externally (no ping or nmap).\nAt this point, I am very confused, so I decided to send an email to the openstack mailing list.\nI searched first, and I found a scenario similar to mine. This person had an issue where, when they tried to add more bridges after they have already been deployed, using kolla-ansible’s reconfigure feature and editing the kolla yaml files. Although not the same to my situation, I will look at the logs and see if I am getting similar errors, or the like.\nA few days later, I changed nothing except for updating the kolla-ansible deployment scripts (I recreated the venv). And it worked.\n\nI can use floating ip’s to put virtual machine’s on the network located on the network-node.\nNow, at first there was an error when I attempted to put the virtual machine directly on the provider network, but then I changed that network to be a “shared” network and it worked. I think this is what I was missing when I attempted to do it originally, but there are too many variables for me to be able to isolate properly.\nAnyway, now I can do both provider and floating ip on the main node. Provider networks are more performant, apparently.\nHowever, one reddit user says that ipv6 does not work when port security is enabled. I need to investigate this more closely.\n\nIPv6\nI began to test to see if I could create an ipv6 floating ip:\n(venv) [root@intmain-node ~]# openstack floating ip create public1\nError while executing command: BadRequestException: 400, Bad floatingip request: Network d5eaee4c-5a21-48f6-8b7b-a73f956bbeb9 does not contain any IPv4 subnet.\nAcording to the docs, floating ip addresses do not support ipv6.\nI asked around, and apparently, you are supposed to use subnet pools for this.\nI first attempted to create one with my libvirt nat subnet:\n(venv) [root@intmain-node ~]# openstack subnet pool create --share --pool-prefix fd00::0/8  --default-prefix-length 8 demo-subnetpool6\nBadRequestException: 400: Client Error for url: http://192.168.124.112:9696/v2.0/subnetpools, Illegal prefix bounds: default_prefixlen=8, min_prefixlen=64.\nApparently, openstack doesn’t support prefix lengths smaller than /64. That’s perfectly ok, as my “production” deployment will have that subnet. But my libvirt nat subnet, I made it a /8, which probably wasn’t a good idea.\nI later changed my libvirt subnet to be smaller, but the subnet pools appeared to behave similarly to normal subnets, with me not being able to put a virtual machine on the remote network.\nChatgpt suggests that I do a more complex setup, where I basically create a physical network, subnet pool, and then allocate a portion of that subnet pool to a non-external network? I recorded the commands it gave so I can replicate them:\nopenstack subnet pool create --default-prefix-length &lt;Prefix_Length&gt; --pool-prefix &lt;CIDR&gt; --name external-pool\nopenstack network create --external --provider-network-type flat --provider-physical-network &lt;Physical_Network&gt; --provider-segment &lt;Segment_ID&gt; external-net\nopenstack subnet create --network external-net --subnet-pool external-pool --no-dhcp external-subnet\nopenstack network create my-project-net\nopenstack subnet create --network my-project-net --subnet-pool external-pool my-project-subnet\nopenstack server create --flavor &lt;Flavor&gt; --image &lt;Image&gt; --nic net-id=my-project-net my-vm\nI was attempting to do this from the UI, where I noticed that there was now an option to create a subnet from an existing pool, rather than specifying a number. But It fails with:\n Error: Failed to create subnet \"None\" for network \"demo-sixnet\". Details\nFailed to allocate subnet: Cannot allocate requested subnet from the available set of prefixes. Neutron server returns request_ids: ['req-dccfaaae-c4e6-4bbc-99cc-3e5a9a49a467']\nI don’t know why this happens yet. I tried with both a /64, which is the same as the subnet above, and several smaller values, like /128, the smallest it would let me work with, and it still failed.\nI found an interesting article from the openinfra blog (archive) about how to disable NAT for the ipv4 subnets, but it appears to be written for the older openstack, before the unified cli.\nAccording to the blog, however, it says that the address scopes are mandatory if you want to have this setup. Here are the mikata (older release), and here are the latest docs from the openstack docs.\nBased on the address scope guide, I tried following that. But when I get to one of the later steps:\n(venv) [root@intmain-node ~]# openstack subnet create --ip-version 6 --ipv6-ra-mode slaac --ipv6-address-mode slaac --subnet-pool subnet-pool-ip6 --prefix-length 128 --network network2 subnet-ip6-2\nHttpException: 500: Server Error for url: http://192.168.124.112:9696/v2.0/subnets, Failed to allocate subnet: Insufficient prefix space to allocate subnet size /128.\nBoth /64 and /128 give me the same error. On one hand, I want to increase the size of my libvirt subnet, however, then that would not be representative of my real scenario, where I will only have a /64 subnet.\nLinked in the address scopes doc, is a video. That video is not very helpful, but in links to it’s presentation, hosted on a site called slideshare. Slideshare recommends similar presentations, and I saw two others, this one, and this one.\nBoth articles mention that with SLAAC (what is slaac?), only a /64 prefix is used.\nAnyway, I still lack understanding of why this is ahppenign. Maybe a /64 is the minimum size I can allocate? But then why would the UI have options for smaller subnet sizes? Anyways, I am going to test it out by chaging my libvirt subnet to be something bigger, like a /56.\nI attempted to upsize my subnet to a /64, but I kept getting an error. The error was somethign about dnsmasq (dhcp server), and it requiring a minimum subnet prefix size of /64. I turned off dhcpv6, and only then was I able to create an ipv6 /56 subnet. Only tangentally related, but maybe the openstack is having a similar issue?\nI tried creating a subnet with dhcp disabled, same issue.\nAnother thing I will attempt to experiment with is working with ports directly. I might be able to create a port on a subnet, and directly attatch it to a virtual machine.\nI attempted to connect a port from the ipv6 subnet to a virtual machine directly:\nUnexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible. &lt;class 'nova.exception_Remote.PortBindingFailed_Remote'&gt; (HTTP 500)\nI will have to investigate the logs futher, but I suspect that it simply isn’t possible to allocate ports to machines that aren’t on the same network.\nAnother person mentioned how they used BGP to get this working, but I don’t know what BGP is. It seems this setup requires address scopes.\nI spent some time talking on discord (did I link the openstack discord here?). I was told it’s not possible, by someone who seems pretty knowledgable. They said that with ipv6, devices are expected, and recommended to have a unique ip address. Link to relevant doc.\nHowever, I still find this hard to believe, since some blog posts, mention the existance of an ipv6 nat.\nI have a new idea: the router has a subnet, and this subnet is exposed via a vpn. Packets are forwarded to this subnet. Because this subnet is located on the same physical node, there is no need to worry about the more compex pieces of openstack networking.\nOne reddit user says that this is possible if ebtables are disabled.\nOkay, later the person I was talking to on discord mentioned that it is indeed possible, but not with NAT, but instead with BGP.\n\nYes as the flow will be: ipv6 address X is on node A but to reach it you need to contact me router A (on network node).\n\nI asked what to search for since docs aren’t too good, and they said:\n\nOpenstack neutron OVN/OVS BGP north/south traffic ipv6 🙂 Should give you plenty documentation.\n\nI asked further about doing this with just OVS, since OVN seems to add more complications and is not enabled by default in kolla-ansible.\n\nOVN is still OpenVSwitch but with a different approach and structure, thought about it as OVS 2.0.\n\n\nAnd yes you can achieve what you want with OVS instead of OVN.\n\nFrom the openstack docs: https://docs.openstack.org/neutron/2023.1/admin/config-bgp-floating-ip-over-l2-segmented-network.html but it seems to mainly apply to ipv4.\nAlso: https://docs.openstack.org/neutron/latest/admin/config-bgp-dynamic-routing.html\nYoutube video, and the slides are in pdf here\nI also desire to enable vpn-as-a-service. But according to the docs, that’s not too difficult to set up."
  },
  {
    "objectID": "projects/build-server-2/index.html#multi-node-install",
    "href": "projects/build-server-2/index.html#multi-node-install",
    "title": "Building my own Server Part 2 — Software",
    "section": "Multi Node Install",
    "text": "Multi Node Install\nA multi node install is documented here: https://docs.openstack.org/kolla-ansible/latest/user/multinode.html\nFor convinience purpose, I will have my inventory git tracked, in the same repo as my blog.\nOne thing I am confused about, is where does the globals.yml file go? In the all in one installation, it goes on the single server, which you both deploy from, and to. But if I am deploying to a different machine, do I put this file on the machines I am deploying to, or the machines I am deploying from?\nThe other thing I am concerned about is how virtual machines will interact to being on a different node than the primary networking node. If I have a compute node, which doesn’t have access to an external subnet, and a networking node that does, then can I have a virtual machine on the external subnet that only the networking node has access to?\nI have created multiple machines and an ansible inventory.\nBut apparently, the deployment host actually is configured a bit, since I got this error:\nfatal: [localhost]: FAILED! =&gt; {\n    \"assertion\": \"ansible_facts.distribution in host_os_distributions\",\n    \"changed\": false,\n    \"evaluated_to\": false,\n    \"msg\": \"Host OS distribution Archlinux is not supported. Supported distributions are: CentOS, Debian, openEuler, Rocky, Ubuntu\"\n}\nI decided to ignore this and power through anyways.\nHowever, for the post deploy step, I needed to create the /etc/openstack directory and chown it to user so that sudo permissions aren’t needed to modify it.\nAlso, based on my testing, the globals.yml file goes on the deployment host.\nSo I deployed a two node openstack install, and I attempt to create a virtual machine, but I get an error. It’s one of those incomprehensible python errros, but after looking at my virtual machine, they appear to be out of ram.\nThis is weird, because I don’t think I have anything enabled but the core openstack services. Currently, I have just two vps’s, a network node with 3 GB of ram and a compute node with 6 Gb of ram.\nBecause of the ram usage, I can’t properly test. In order to get more ram, I decided to install libvirt on my “moonstack”, which is going to be the openstack compute node. I find this highly ironic (and not the openstack component), since I am basically setting up my server to run virtual machines in an easier manner to prototype for doing so in a harder manner.\nUsing a combination of cockpit-machines, ssh proxy jumping, and virt-manager’s ability to manage a remote I managed to deploy an all in one, and a network node. Besides the default services, I also enabled magnum, zun (and kuryr, container networking), and cinder (via nfs).\nHere is the memory usage for both nodes:\n[root@network-node ~]# free -m\n               total        used        free      shared  buff/cache   available\nMem:            7486        2190        2105          15        3538        5295\nSwap:           7883           0        7883\n[root@main-node ~]# free -m\n               total        used        free      shared  buff/cache   available\nMem:            7486        6816         273          61         857         669\nSwap:           7883          77        7806\nGood. The network node seems to have ample free space, meaning all of my fancy networking will fit just fine on my 8 GB of ram VPS.\nThe main node seems to be a little… strangled, but 8 GB is nothgin compared to the 128 GB teh actual install will have.\nNow I need to test openstack network interactions.\nI upgraded the main node to have 16 GB of ram, and I did an all in one installation + a network node. The network node has a second virtio interface, which is attatched to a different NAT subnet. The point of this is to test how openstack networking works when not all network nodes have their neutron external interfaces on the same subnet.\nRam usage:\n[root@network-node ~]# free -m\n               total        used        free      shared  buff/cache   available\nMem:            7486        2198        2102          14        3532        5287\nSwap:           7883           0        7883\n[root@main-node ~]# free -m\n               total        used        free      shared  buff/cache   available\nMem:           15350        8221         558          90        7025        7129\nSwap:           7883           1        7882"
  },
  {
    "objectID": "projects/build-server-2/index.html#ipv6-on-contabo",
    "href": "projects/build-server-2/index.html#ipv6-on-contabo",
    "title": "Building my own Server Part 2 — Software",
    "section": "Ipv6 on Contabo",
    "text": "Ipv6 on Contabo\nMore vps networking, I can’t believe it.\nAs I get closer to a deployment, I need to make sure that I have the ipv6 addresses available.\nhttps://contabo.com/blog/ipv6-now-available-for-all-our-customers/\nAccording to this article, contabo makes an entire /112 range of ipv6 addresses avaiable to their customers by default. That’s 65535 ip addresses.\nExcept this article seems to be outdated.\nAccording to a newer FAQ, the range seems to be /64.\nhttps://contabo.com/blog/adding-ipv6-connectivity-to-your-server/\nAccording to the above article, contabo has already added ipv6 support, and you simply look at the vps control page to see your available ipv6 address. However, there is no such box on my vps control page.\nI think this may have something to do with the fact that campus internet is not ipv6 capable, so maybe there is no need to show it to me since I would not be able to access it.\nOr it could do with the fact that this article is from 2013 and possible wildly out of date.\nHowever, on the reverse DNS page, my vps’s ipv6 address is shown:\n\nSo, inferring from that, I can tell that my vps’s ipv6 address is “2605:a141:2140:3809:0000:0000:0000:0001”. And if I get a /64 range, then I can tell my range is 2605:a141:2140:3809::/64. Of course, that’s not enough, I also need to know the default gate way for this network… The article above details it for a different subnet, but I don’t know for my own subnet.\nThere is a command, enable_ipv6 which comes out of the box on the contabo servers, but it doesn’t do anthing:\n[root@vmi1403809 ~]# which enable_ipv6\nalias enable_ipv6='sed -i \"/net.ipv6.conf.all.disable_ipv6.*/d\" /etc/sysctl.conf && sysctl -q -p && echo 0 &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6'\n        /usr/bin/sed\n        /usr/sbin/sysctl\n        /usr/bin/echo\nAll it does is change the sysctl parameters to enable ipv6, and it doesn’t actually set it up. Without knowing my default gateway, nothing I do with ipv6 will be able to access the internet.\nUsing cockpit and networkmanager, I set my ipv6 address to what was above. The default gateway was fe80::1, which was noted here. I tried to ping it and nmap it but I diddn’t see anything.\nFor DNS, rather than using contabo’s DNS, I decided to go for google’s two public ipv6 dns’s, 2001:4860:4860::8844, and 2001:4860:4860::8888. These are noted on google’s developer docs.\nSure enough, when I proxy into my server (I used ssh with the -D option), and go to https://test-ipv6.com/ I get this result:\n\nMy server has a public ipv6 address now. And if I can give this interface a public ipv6 address, then openstack will definitely be able to give any virtual interfaces it creates public ipv6 addresses.\nLater, I realized that the way openstack allocates floating ip addresses, is by contacting an external dhcp server, so I have to create my own dhcp server.\nIn addition to that, if I want to access the ipv6 services from my VPS, then I will need to have an ipv6 address configured.\nThe arch wiki has some instructions for setting up dnsmasq, a lightweight, easy to configure dhcp server.\nFor testing, I decided to use the subnet fd00::/8 for the private ipv6 subnet I will be testing on."
  },
  {
    "objectID": "projects/build-server-2/index.html#network-interfaces",
    "href": "projects/build-server-2/index.html#network-interfaces",
    "title": "Building my own Server Part 2 — Software",
    "section": "Network interfaces",
    "text": "Network interfaces\nSo I realized something. According to the kolla-ansible architechture guide, ansible cannot recognize interface names with dashes. So because of this, I’ve decided to rename the wireguard interface from wg-stack to wgstack.\nAnother thing that looks a bit confusing is the dns_interface option, which appears to be used by designate for external dns requests. By default, this interface defaults to the same as the “network_interface”, which will not have public internet access on my vps (since I am using the wireguard as that interface). However, since I won’t be installing designate, it shouldn’t be too much of an issue?"
  },
  {
    "objectID": "writeups/nice-challenge-2/index.html",
    "href": "writeups/nice-challenge-2/index.html",
    "title": "Nice Challenge 2",
    "section": "",
    "text": "This is for the NICE challenge. This challenge was quite a bit harder than the previous one, as it required that I figure out an selinux issue.\nWhenever you see ... in my code, it’s shorthand for “contents omitted for brevity”. I use this to clip extra long text files down, fitting them into my blog.\nSame as before, the introduction was a simulated chatroom:\n\nAnd here are the checks:\n\n\nDev-Web\nI just edited the sudo file with sudo visudo\n#\n# This file MUST be edited with the 'visudo' command as root.\n#\n# Please consider adding local content in /etc/sudoers.d/ instead of\n# directly modifying this file.\n#\n# See the man page for details on how to write a sudoers file.\n#\nDefaults    env_reset\nDefaults    mail_badpass\nDefaults    secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\"\n\n# Host alias specification\n\n# User alias specification\n\n# Cmnd alias specification\n\n# User privilege specification\nroot    ALL=(ALL:ALL) ALL\n\n# Members of the admin group may gain root privileges\n%admin ALL=(ALL) ALL\n\n# Allow members of group sudo to execute any command\n%sudo   ALL=(ALL:ALL) ALL\n\n# See sudoers(5) for more information on \"#include\" directives:\n\n#includedir /etc/sudoers.d\nDefaults:nagios !syslog\nDefaults:nagios !requiretty\nnagios  ALL=(ALL:ALL) NOPASSWD:ALL\ngthatcher ALL = NOPASSWD: ALL\nthanh ALL = NOPASSWD: ALL\nI edited the last two lines:\ngthatcher ALL = (ALL:ALL) ALL\nthanh ALL = (ALL:ALL) ALL\nAnd both checks were passed.\n\n\nDatabase\nI load the html server console up… and I see this:\n\nI load up powershell, and the mysql client is there, so that works.\n\nUSE wordpress;\nCREATE USER 'playerone'@'*' IDENTIFIED BY 'password123';\nDELETE FROM mysql.user WHERE User='root' AND Host NOT IN ('localhost', '127.0.0.1', '::1');\nFLUSH PRIVILEGES;\nWith this, root no longer has remote access to the database.\n\n\nProd-Web\nThe first was to disable ssh login of the user’s “root” and “SSHLOGIN”.\nThe former was easy, I just changed the config file:\n\n\n/etc/ssh/sshd_config\n\n....\nPermitRootLogin no\n....\n\nExcept this didn’t pass the check, so I decided to look into the other configuration option, DenyUsers.\n\n\n/etc/ssh/sshd_config\n\n....\nDenyUsers root sshlogin\n....\n\nFrustratingly, the sshlogin user is not all caps, even though the check describes it as so, but it also describes root as ROOT.\nThis still failed, until I restarted the sshd service, at which point both checks succeed.\nTrying to load the database from the “database machine”, which offers a GUI, I see this:\n\nTrying to load /wp-admin, the wp admin console, gives me a little more info.\n\nSo I decide to go look at the file:\n\n\n/var/www/html/config.php\n\n&lt;?php\n/**\n * The base configuration for WordPress\n *\n * The wp-config.php creation script uses this file during the\n * installation. You don't have to use the web site, you can\n * copy this file to \"wp-config.php\" and fill in the values.\n *\n * This file contains the following configurations:\n *\n * * MySQL settings\n */\n\n// ** MySQL settings - You can get this info from your web host ** //\n/** The name of the database for WordPress */\ndefine( 'DB_NAME', 'wordpress' );\n\n/** MySQL database username */\ndefine( 'DB_USER', 'playerone' );\n\n/** MySQL database password */\ndefine( 'DB_PASSWORD', 'password123' );\n\n/** MySQL hostname */\ndefine( 'DB_HOST', 'database.daswebs.com' );\n\n/** Database Charset to use in creating database tables. */\ndefine( 'DB_CHARSET', 'utf8mb4' );\n\n/** The Database Collate type. Don't change this if in doubt. */\ndefine( 'DB_COLLATE', '');\n\n/**#@+\n * Authentication Unique Keys and Salts.\n *\n * Change these to different unique phrases!\n * You can generate these using the {@link https://api.wordpress.org/secret-key/1.1/salt/ WordPress.org secret-key service}\n * You can change these at any point in time to invalidate all existing cookies. This will force all users to have to log in again.\n *\n * @since 2.6.0\n */\ndefine( 'AUTH_KEY',         'ea355b607a03e1dc4fdbf0b6307b05303073e640');\ndefine( 'SECURE_AUTH_KEY',  'f33aec49919d2584c113a0c97e6fe6521c899ceb');\ndefine( 'LOGGED_IN_KEY',    'c366b663c1d82456edb8e88c592013f6c93b54e9');\ndefine( 'NONCE_KEY',        '1d210afe29e77dd29dd6c444314b6b3d5389f47d');\ndefine( 'AUTH_SALT',        '5ed4f5d28661e0b29b4651e051907704e517e81d');\ndefine( 'SECURE_AUTH_SALT', '98c1307b17e104f4b0f9d17771cd3f6fe781b60e');\ndefine( 'LOGGED_IN_SALT',   'bcb58b6319635fa3f6fed8154e8e56f151a1bd18');\ndefine( 'NONCE_SALT',       '796f651b2fbb7175080e390b31a68ea08dca928f');\n\n/**#@-*/\n\n/**\n * WordPress Database Table prefix.\n *\n * You can have multiple installations in one database if you give each\n * a unique prefix. Only numbers, letters, and underscores please!\n */\n$table_prefix = 'wp_';\n\n....\n....\n\nI had to do some commands on the “database machine”, since it seems that something was likely wrong there.\nUSE wordpress;\nCREATE USER 'playerone'@'*' IDENTIFIED BY 'password123';\nGRANT ALL PRIVILEGES ON wordpress.* TO 'playerone'@'*';\nGRANT ALL PRIVILEGES ON wordpress TO 'playerone'@'*';\nFLUSH PRIVILEGES;\nBut this wasn’t enough. Wordpress still could not connect to the database. I checked to make sure the mysql service was running, and that the firewall was open, both were the case. An nmap scan from the wordpress server sees the port open, and the service, as well.\nI was even able to authenticate to the database from the webserver machine manually, using the mysql client included in the mariadb package.\nSo it’s likely an SELinux issue, as mentioned in the meeting notes.\nSure enough, when searching through the logs, I find something:\n\nI used this: https://serverfault.com/a/456875 for steps.\n[root@www~]# getsebool -a | grep httpd\n...\n...\nhttpd_can_connect_mythtv --&gt; off\nhttpd_can_connect_zabbix --&gt; off\nhttpd_can_network_connect --&gt; off\nhttpd_can_network_connect_cobbler --&gt; off\nhttpd_can_network_connect_db --&gt; off\nhttpd_can_network_memcache --&gt; off\nhttpd_can_network_relay --&gt; off\nhttpd_can_sendmail --&gt; off\n...\n...\nIt seems as if the apache httpd server doesn’t have permissions for that port.\nFollowing those steps:\nsetsebool httpd_can_network_connect_db 1\nsetsebool httpd_can_network_connect_db 1 -P\nAnd httpd can connect to the database!\n\n\nFileshare\nThe goal was to disable anonymous ftp logins. I did some hunting through the systemd services, found the ftp service named proftpd, and then I found the config directory in /etc/proftpd\n\n\n/etc/proftpd/proftpd.conf\n\nInclude /etc/proftpd/modules.conf\n....\n....\nTimesGMT off\n&lt;/Directory&gt;\n&lt;Anonymous ~ftp&gt;\n  User ftp\n  Group nogroup\n  UserAlias anonymous ftp\n  DirFakeUser on ftp\n  DirFakeGroup on ftp\n  RequireValidShell off\n  &lt;Directory *&gt;\n    Umask 000 000\n    HideFiles (welcome.msg)\n    &lt;Limit WRITE&gt;\n      DenyAll\n    &lt;/Limit&gt;\n  &lt;/Directory&gt;\n&lt;/Anonymous&gt;\n\n....\n....\n\nAccording to some docs, that &lt;Anonymous&gt; section defines anonymous access, so I can simply comment it out to deny anonymous access.\nAnd after a sudo systemctl restart proftpd, the check passes.\n\n\nPassed Checks"
  },
  {
    "objectID": "talks/index.html",
    "href": "talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 25, 2024\n\n\nYour very own site on GH pages\n\n\n \n\n\n\n\nJan 23, 2024\n\n\nHow to Ansible\n\n\n \n\n\n\n\nOct 23, 2023\n\n\nContainers (that run on linux)\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/ansible/index.html",
    "href": "talks/ansible/index.html",
    "title": "How to Ansible",
    "section": "",
    "text": "Recently, I wrote a lot of ansible roles and playbooks for the CCDC competition.\nHere is a link to the roles: https://github.com/CSUN-CCDC/CCDC-2023/tree/main/linux/ansible/roles\nThese ansible “roles,” cover a variety of tasks that the cybersecurity competition would require from us, including things like managing firewalls (ufw, iptables, firewalld), shuffling passwords of linux users or databases, or doing backups.\nProfessor Doris Chaney asked me to present on how to use ansible, so here is where the presentation, code, and guide will go.\nThe presentation can be found here\nI have the recording, but I need to edit it a little bit before I post it, but I will do so soon™."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 20, 2024\n\n\nMy server part 4 — Kubernetes\n\n\n \n\n\n\n\nJan 17, 2024\n\n\nBuilding my server part 3 — The switch to debian\n\n\n \n\n\n\n\nJan 15, 2024\n\n\nPackaging quarto using nix\n\n\n \n\n\n\n\nAug 2, 2023\n\n\nBuilding my own Server Part 2 — Software\n\n\n \n\n\n\n\nAug 1, 2023\n\n\nBuilding my own Server Part 1 — Hardware\n\n\n \n\n\n\n\nJul 14, 2023\n\n\nAutomating my server config, first nix, then ansible\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nPackaging Openstack on Nixos\n\n\n \n\n\n\n\nMay 2, 2023\n\n\nCompiling KasmVNC on NixOS\n\n\n \n\n\n\n\nJan 26, 2023\n\n\nKasmweb setup\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/build-server-3/index.html",
    "href": "projects/build-server-3/index.html",
    "title": "Building my server part 3 — The switch to debian",
    "section": "",
    "text": "Recently, I have been very busy working on scripts and ansible playbooks for the Collegiate Cyber Defense Competition.\nIn order to test those playbooks, I have been using Vagrantfiles, as an excerpt, something like this:\n# -*- mode: ruby -*-\n# vi: set ft=ruby :\n\nVagrant.configure(\"2\") do |config|\n\n    config.vm.synced_folder \".\", \"/vagrant\", disabled: true\n    config.vm.provision \"shell\", path: \"scripts/packages.sh\"\n\n    config.vm.define \"318\" do |vmconfig|\n        vmconfig.vm.provision \"ansible\" do |ansible|\n            ansible.playbook = \"ansible/inventory.yml\"\n        end\n        vmconfig.vm.box = \"generic/alpine318\"\n        vmconfig.vm.provider \"libvirt\" do |libvirt|\n            libvirt.driver = \"kvm\"\n            libvirt.memory = 1024\n            libvirt.cpus = 2\n            libvirt.video_type = \"virtio\"\n        end\n        vmconfig.vm.provider \"virtualbox\" do |virtualbox,override|\n            virtualbox.memory = 1024\n            virtualbox.cpus = 1\n        end\n    end\nend\nThis is from the ccdc-2023 github repo. With a single command, vagrant up, I can create virtual machines, and then provision them with ansible playbooks.\nEven more impressive, I can use the generated ansible inventory manually, as noted in the ansible documentation. This creates an environment closer to how I would actually use the vagrant ansible playbooks.\nVagrants snapshots let me easily freeze and revert machines to previous features.\nI used vagrant, in combination with windows vagrant machines to easily test things for the ccdc environment guide.\nHowever, when attempting to up vagrant on my Rocky Linux machine, I got an error: Spice is not supported.\nApparently, Red Hat Enterprise Linux deprecated spice, and now any rebuilds of it, no longer have spice as well.\nhttps://forums.rockylinux.org/t/spice-support-was-dropped-in-rhel-9/6753\nhttps://access.redhat.com/articles/442543 (requires subscription, but the important content is in the beginning).\nExcept, my Arch Linux machines can still run spice virtual machines just fine… so when I want to run a Vagrant box(s) on my server, like the Windows boxes which require much more memory, I can either write around this missing feature, using the much less performant VNC… or I can switch.\nThe main reason why I picked Rocky, was the 4 year support of an operating system supported by Kolla-ansible. This is no longer the only case.\nAs of the current supported Kolla-Ansible, it now supports Debian 12, which will be supported for all 4 years of my bachelors. Support Matrix.\nWith this, I decided to switch to debian… but I encountered some challenges."
  },
  {
    "objectID": "projects/build-server-3/index.html#reverse-proxies-and-https",
    "href": "projects/build-server-3/index.html#reverse-proxies-and-https",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Reverse Proxies and HTTPS",
    "text": "Reverse Proxies and HTTPS\nTo ensure maximum security, I need to terminate HTTPS on the home server, which is completely under my control, unlike the VPS I’m renting from Contabo.\nCurrently, I have a simply setup using nginx proxy manager, where it reverse proxies any sites I want to create by terminating TLS/SSL on the VPS.\nI don’t really feel like going back to pure nginx, it was very painful to configure, which is why I switched to the easier frontend, nginx proxy manager (npm).\nI attempted to set up a stream, but I was simply redirected to the “Nginx Proxy Manager” page instead.\nI bought a domain: &lt;moonpiedumpl.ing&gt;. It looks very clean. I will use subdomains for each service, something like &lt;keycloak.moonpiedumpl.ing&gt; will be the keycloak service, and so on.\nI want a declarative reverse proxy, that can reverse proxy existing https servers, without issues. Caddy can probably do this.\nI think the crucial thing is that is that Caddy some of the reverse proxy headers by default. It’s documented here: https://caddyserver.com/docs/caddyfile/directives/reverse_proxy#defaults\nSo the second Caddy proxy, located on the internal server, would have the option “trusted proxies”, set to the ip address that the internal server sees coming from the VPS. This will cause it to not edit the headers, allowing the internal server to see the real ip address of the machines connecting to the services.\nCaddy can also configure https automatically: https://caddyserver.com/docs/caddyfile/options#email\nAs for TLS passthrough on the external Caddy service, I found some relevant content:\nhttps://caddy.community/t/reverse-proxy-chaining-vps-local-server-service-trusted-proxies/18105\nAlright. This isn’t working. Double reverse proxies are a pain to configure, plus they seem to be lacking support for some things.\nSo… alternatives?\nIt seems to forward traffic as it, which is good, because I don’t want it to get in the way of caddy.\nI also need to test iptables as a reverse proxy. I don’t want to test this, as it requires that I tear down some of my declarative configuration to temporarily test this in an imperative manner.\nAlternatively, I am looking at iptables.\nThere are several relevant stackoverflow answers: this one and this one.\nI did a little bit of experimenting with iptables, but it looks to be more effort than it is worth to configure, for something that might not even work.i\nI started looking through this: https://github.com/anderspitman/awesome-tunneling\nProjects like frp appeal to me and look like they do what I want to.\nOut of them, rathole appeals to me the most. The benchmarks advertise high performance, and it has an official docker container. This would make it easy to deploy.\nI don’t like the existing rathole docker images. The official one is older than the code is, and other docker container uses a bash script reading enviornment variables to configure itself.\nI’ve decided to compile rathole myself. I want a static binary I can copy over to an empty docker container, and then docker can handle the service managemnt.\nI want to compile rathole myself. Originally I was going to compile rathole statically on an alpine container, but I kept encountering issues, so I decided to use the debian container (that is one of the solutions I saw on the answer sites)\nFROM rust:latest as stage0 # Debian bookworm\n\nENV RUSTFLAGS=\"-C target-feature=+crt-static\"\n\nRUN apt install musl-tools\n\nRUN git clone https://github.com/rapiz1/rathole\n\nRUN rustup target install x86_64-unknown-linux-musl\n\nRUN cd rathole && cargo build --release --target x86_64-unknown-linux-musl\n\nFROM scratch\n\nCOPY --from=stage0 /rathole/target/x86_64-unknown-linux-musl/release/rathole /bin/rathole\n\nCMD [\"/bin/rathole\"]\nIt uses a multi stage build to copy the statically compiled binary to the final container."
  },
  {
    "objectID": "projects/build-server-3/index.html#podman",
    "href": "projects/build-server-3/index.html#podman",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Podman",
    "text": "Podman\n\nRootless\nThings were going smoothly, until I encountered a roadblock:\nTASK [Create caddy directory] ************************************************************************************************************\nfatal: [moonstack]: FAILED! =&gt; {\"msg\": \"Failed to set permissions on the temporary files Ansible needs to create when becoming an unprivileged user (rc: 1, err: chmod: invalid mode: ‘A+user:podmaner:rx:allow’\\nTry 'chmod --help' for more information.\\n}). For information on working around this, see https://docs.ansible.com/ansible-core/2.16/playbook_guide/playbooks_privilege_escalation.html#risks-of-becoming-an-unprivileged-user\"}\nThe relevant ansible code uses become to run as the podmaner user, and then runs the ansible podman container module.\nIt links to some docs here: https://docs.ansible.com/ansible-core/2.16/playbook_guide/playbooks_privilege_escalation.html#risks-of-becoming-an-unprivileged-user\nThis was fixed after I installed the acl package.\n\n\nPods\nPodman has “pods”. These are somewhat similar to docker networks, which I prefer.\nWith docker networks, the “network” has a dns, and containers can find eachother by the container name. I enjoyed this with nginx proxy manager, because then I could just forward “containername:port” and it would be easy.\nPodman pods work a bit differently. Every pod can contain multiple containers, but they share a network interface. I tested this with two alpine containers, even if I bind a port to localhost, then the other container can still access it.\nI am using ansible to create pods.\n\n\nSystemd Integration\nAnsible can generate systemd units for pods and containers:\n\nPods: https://docs.ansible.com/ansible/latest/collections/containers/podman/podman_pod_module.html#parameter-generate_systemd\nContainers: https://docs.ansible.com/ansible/latest/collections/containers/podman/podman_container_module.html#parameter-generate_systemd\n\nRather than starting the containers using ansible, I can then start the whole pod using the systemd unit for the pod, and ansible’s systemd service module. (From my testing, if you start a pod, it also starts all containers in the pod).\nIn order for this to work, I need to have systemd user services configured and working. However, I get an error:\npodmaner@thoth:~$ systemctl status --user\nFailed to connect to bus: No medium found\nA bit of research says that this is because the systemd daemon is not running… but it is:\nroot@thoth:~# ps aux | grep \"systemd --user\"\npodmaner    1095  0.0  0.0  18984 10760 ?        Ss   13:33   0:00 /lib/systemd/systemd --user\nmoonpie     1513  0.1  0.0  19140 10780 ?        Ss   13:33   0:00 /lib/systemd/systemd --user\nroot        1648  0.0  0.0   6332  2036 pts/6    S+   13:35   0:00 grep systemd --user\nIt works on my moonpie user:\nmoonpie@thoth:~$ systemctl status --user\n● thoth\n    State: running\n    Units: 139 loaded (incl. loaded aliases)\n     Jobs: 0 queued\n   Failed: 0 units\n    Since: Sat 2024-04-20 13:33:22 PDT; 1min 27s ago\n  systemd: 252.22-1~deb12u1\n   CGroup: /user.slice/user-1000.slice/user@1000.service\n           └─init.scope\n             ├─1513 /lib/systemd/systemd --user\n             └─1514 \"(sd-pam)\"\nI found an askubuntu answer which has the solution I need. The variable $XDG_RUNTIME_DIR and $DBUS_SESSION_ADDRESS need to be set — however, in my testing, only the former actually needed to be set.\nAnsible’s documentation on user services actually addresses this.\n- name: Run a user service when XDG_RUNTIME_DIR is not set on remote login\n  ansible.builtin.systemd_service:\n    name: myservice\n    state: started\n    scope: user\n  environment:\n    XDG_RUNTIME_DIR: \"/run/user/{{ myuid }}\"\nThe environment dictionary sets environment variables for a task.\nAnd then of course, when configuring and testing, I like the solution as suggested by the stackoverflow answer:\nsudo machinectl shell user@ to log into a local user with dbus and environment variables properly set up.\nNow, I simply have to ensure that generated systemd unitfiles for containers go in the right place, $HOME/.config/systemd/user/, like so:\n- name: Authentik Redis database\n      containers.podman.podman_container:\n        name: authentik_redis\n        pod: authentik_pod\n        image: docker.io/library/redis:alpine\n        command: --save 60 1 --loglevel warning\n        env:\n          AUTHENTIK_SECRET_KEY: \"{{ authentik_secret_key }}\"\n        volumes:\n          - \"{{ homepath.stdout }}/authentik/redis:/data\"\n        generate_systemd: \n          restart_policy: \"always\"\n          path: \"{{ homepath.stdout }}/.config/systemd/user/\"\nAnd the services shows up in the systemd user session:\npodmaner@thoth:~/.config$ systemctl --user status container-authentik_redis.service\n○ container-authentik_redis.service - Podman container-authentik_redis.service\n     Loaded: loaded (/home/podmaner/.config/systemd/user/container-authentik_redis.service; disabled; preset: enabled)\n     Active: inactive (dead)\n       Docs: man:podman-generate-systemd(1)\nHowever…\n\n\n$HOME/.config/systemd/user/container-authentik_redis.service\n\n[Unit]\nDescription=Podman container-authentik_redis.service\nDocumentation=man:podman-generate-systemd(1)\nWants=network-online.target\nAfter=network-online.target\nRequiresMountsFor=/tmp/podman-run-1001/containers\n\n[Service]\nEnvironment=PODMAN_SYSTEMD_UNIT=%n\nRestart=always\nTimeoutStopSec=70\nExecStart=/usr/bin/podman start authentik_redis\nExecStop=/usr/bin/podman stop  \\\n        -t 10 authentik_redis\nExecStopPost=/usr/bin/podman stop  \\\n        -t 10 authentik_redis\nPIDFile=/tmp/podman-run-1001/containers/vfs-containers/5fed7e5a1116899a7e1915793f0f8d3bd0f09c1884e8d574138a72b55964211f/userdata/conmon.pid\nType=forking\n\n[Install]\nWantedBy=default.target\n\npodman generate systemd doesn’t actually generate quadlets for systemd, but rather a unit user service that calls podman. This is problematic because I was expecting quadlets to be used because quadlets define the container themselves in the unit file.\nWithout quadlets, podman manages it’s own state, which has issues, and was the entire reason I was looking into alternatives to podman for managing state.\nMore research: https://github.com/linux-system-roles/podman: I found an ansible role to generate podman quadlets, but I don’t really want to include ansible roles in my existing ansible roles. Also, it intakes kubernetes yaml, which is very complex for what I am trying to do. At that point, why not just use a single node kubernetes cluster and let kubernetes manage state?\nHowever:\n\nQuadlet, a tool merged into Podman 4.4\n\npodmaner@thoth:~/.config/systemd/user$ podman --version\npodman version 4.3.1\nMy Debian system doesn’t support quadlets. I couuld use nix or something else to get a newer version of podman, since I am doing rootless, but that feels like way to much complexity for what will ultimately not be a permanent solution, as I plan to eventually move to a kubernetes cluster.\nLater, I did discover that ansible supports writing quadlet files, rather than creating a container:\nFrom ansible docs on podman_container:\n\nquadlet - Write a quadlet file with the specified configuration. Requires the quadlet_dir option to be set.\n\nI still don’t think I’m going to use quadlets, but the option is nice.\nRather, I opted for something else: Generating a systemd service using ansible.\nFrom my testing, starting a pod using podman start pod-name starts all containers in a pod, so I can simply generate a container for systemd service for each pod and then when that service is started (or stopped), it also controls all containers in the pod.\n - name: Create relevant podman pod  \n      containers.podman.podman_pod:\n        name: authentik_pod\n        userns: keep-id:uid=1000,gid=1000\n        state: created\n        generate_systemd: \n          restart_policy: \"always\"\n          path: \"{{ homepath.stdout }}/.config/systemd/user/\"\nThen, I can use ansible’s systemd service module to start the pod, and all the containers in it… except not.\n\n\n$HOME/.config/systemd/user/pod-authentik_pod.service\n\n# pod-authentik_pod.service\n# autogenerated by Podman 4.3.1\n# Sat Apr 27 16:07:36 PDT 2024\n\n[Unit]\nDescription=Podman pod-authentik_pod.service\nDocumentation=man:podman-generate-systemd(1)\nWants=network-online.target\nAfter=network-online.target\nRequiresMountsFor=/tmp/podman-run-1001/containers\nWants=\nBefore=\n\n[Service]\nEnvironment=PODMAN_SYSTEMD_UNIT=%n\nRestart=always\nTimeoutStopSec=70\nExecStart=/usr/bin/podman start f596f20d51c2-infra\nExecStop=/usr/bin/podman stop  \\\n        -t 10 f596f20d51c2-infra\nExecStopPost=/usr/bin/podman stop  \\\n        -t 10 f596f20d51c2-infra\nPIDFile=/tmp/podman-run-1001/containers/vfs-containers/f3f084bceb1041e2a9aa18a9b8088de1efa6ff6ea7bb81f0fbfb4bf8fd1558d2/userdata/conmon.pid\nType=forking\n\n[Install]\nWantedBy=default.target\n\nRather than podman start pod-name, it just starts the infrastructure for that pod, meaning the containers in the pod itself aren’t started. Same goes for stopping pods as well. Although I suspect I can manage pods using systemd, similar to containers, podman’s systemd unit file generation doesn’t do that.\nSo, now what?\nI see two options:\n\nLine/word replace the systemd file to reference the pod itself, rather than the infrastructure pod\nGenerate systemd unit files for each container, and then start those\n\nBoth of these suck. The first is hacky, and the second is way to wordy and probably redundant.\nWhy isn’t there an easier way to start an entire pod at once, rootless?\nPodman doesn’t support this. But do you know what does? Kubernetes. I think it’s time too switch. I’ve had too many issues with podman."
  },
  {
    "objectID": "projects/build-server-3/index.html#ansibilizing-the-server",
    "href": "projects/build-server-3/index.html#ansibilizing-the-server",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Ansibilizing the server",
    "text": "Ansibilizing the server\n\nAnsible facts HOME =/= $HOME\nI was having ansible run some things as another user, and using something like this to get the home directory of the user it was running as:\n- name: Do podmaner container\n  become: true\n  become_user: \"{{ rathole_user }}\"\n  block:\n    - name: Create rathole config\n      ansible.builtin.file:\n        path: \"{{ lookup('ansible.builtin.env', 'HOME') }}/rathole/config\"\n        state: directory\n        mode: '0775'\nBut this errors. Despite rathole_user being podmaner, the error is something like:\nTASK [rathole : Create rathole config] *******************************************************************************************\nfatal: [moonstack]: FAILED! =&gt; {\"changed\": false, \"msg\": \"There was an issue creating /home/moonpie/rathole as requested: [Errno 13] Permission denied: b'/home/moonpie/rathole'\", \"path\": \"/home/moonpie/rathole/config\"}\nThis might be because, when executing one off commands, sudo does not keep environment variables\nmoonpie@thoth:~$ sudo -iu podmaner echo $HOME\n/home/moonpie\nInterestingly enough, using the $HOME environment variable works though.\n- name: Do podmaner container\n  become: true\n  become_user: \"{{ rathole_user }}\"\n  block:\n    - name: Get home\n      ansible.builtin.command: \"echo $HOME\"\n      register: homepath\n    - name: Print current home\n      ansible.builtin.debug:\n        msg: \"{{ homepath.stdout }}\"\nTASK [rathole : Print current home] **********************************************************************************************\nok: [moonstack] =&gt; {\n    \"msg\": \"/home/podmaner\"\n}\nOh, I figured it out. After a quick google, this is because this lookup runs on the control node, rather than the server being managed.\nFrom the ansible docs: “query the environment variables available on the controller”\nI also decided to try ansible_facts['env']['HOME'], but that still outpus /home/moonpie. It seems that facts are gathered using hte user ansible initially logs in as.\nansible_env['HOME'] doesn’t work either, probably for the same reason.\nThis was a fun little tangent, I think I am going to return to using $HOME and other environment variables. Since things like podman volumes can’t simply be fed environment variables, I have to echo $HOME to get it first, and then save it to stdout. It feels like there should be a cleaner way to do this, but this does work.\n\n\nAnsible Podman Healthchecks\nI think I found some undocumented behavior. I was attempting to adapt the authentik docker compose healthchecks to be configured via ansible and podman instead.\nThe ansible podman_container docs have a section on healthchecks.\nThe official docs say this is correct:\n- name: Authentik postgres database\n  containers.podman.podman_container:\n    name: authentik_postgres\n    pod: authentik_pod\n    image: \"moonpiedumplings/authentik_postgres:12\"\n    healthcheck: \"CMD-SHELL pg_isready -d $${POSTGRES_DB} -U $${POSTGRES_USER}\"\n    healthcheck_retries: 5\n    healthcheck_start_period: 20s\nAnd it is. But I attempted to copy and paste the official authentik docker-compose for this anyway, to see if it worked:\n- name: Authentik Redis database\n  containers.podman.podman_container:\n    name: authentik_redis\n    pod: authentik_pod\n    image: docker.io/library/redis:alpine\n    command: --save 60 1 --loglevel warning\n    healthcheck:\n      test: \"CMD-SHELL redis-cli ping | grep PONG\"\n      start_period: 20s\n      interval: 30s\n      retries: 5\n      timeout: 3s\nAnd I found that this actually still worked.\npodmaner@thoth:~$ podman inspect authentik_redis\n...\n....\n\"--healthcheck-command\",\n\"{'test': 'CMD-SHELL redis-cli ping | grep PONG', 'start_period': '20s', 'interval': '30s', 'retries': 5, 'timeout': '3s'}\",\n...\n...\nI find this interesting that it works despite not being documented. Should I rely on this behavior? I like the way it looks, as it’s much more readable than the _underscore format for these particular variables.\n\n\nAnsible vault\nIf I am going to use rathole, then I need to deploy a config file with a client and server secrets. Although an extra step of complexity, thankfully, ansible makes handling secrets easy, with ansible vault.\nMain docs here: https://docs.ansible.com/ansible/latest/vault_guide/index.html\nDocs on handling vault passwords\nI only need a few variables encrypted, so maybe:\nDocs on encrypting individual variables\nOr I could encrypt a file with the variables I want and add them? :\nDocs on encrypting files\nI opted for the only encrypting certain variables within my inventory file, using a single password."
  },
  {
    "objectID": "projects/build-server-3/index.html#rathole",
    "href": "projects/build-server-3/index.html#rathole",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Rathole",
    "text": "Rathole\nIn addition to the build above, I used a template to deply the rathole container, in case I want more than just http or https on my server (minecraft?).\nFor example, here is the “client” code:\n\n\nclient.toml.j2\n\n{% for service in rathole_services %}\n\n[client.services.{{ service.name }}]\ntoken = \"{{ rathole_secret }}\"\nlocal_addr = \"{{ service.local_addr }}\"\n{% endfor %}\n\nUsing ansible’s ability to do for loops. The server config has something similar."
  },
  {
    "objectID": "projects/build-server-3/index.html#caddy",
    "href": "projects/build-server-3/index.html#caddy",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Caddy",
    "text": "Caddy\nI have a simple caddyfile, right now, but it doesn’t work:\n\n\nCaddyfile.j2\n\nhttp://test.moonpiedumpl.ing { \n  reverse_proxy * http://localhost:8000\n}\n\nI need to expand this. I dislike how Caddy only takes a config file, and there isn’t something like a premade docker container that takes environment variables, but it is what it is.\nFor a jinja2 template, something like this:\n\n\nCaddyfile.j2\n\n{% for service in caddy_services %}\n{{ service.domain_name }} {\n    {% if service.port is defined %}\n    reverse_proxy http://127.0.0.1:{{ service.port }}\n    {% if service.request_body is defined %}\n    request_body {\n        max_size 2048MB\n    }\n    {% endif %}\n    {% endif %}\n    {% if service.file_root is defined %}\n    root * {{ service.file_root }}\n    file_server\n    {% endif %}\n}\n{% endfor %}\n\nAnd then the caddy role would take these hosts:\n- role: caddy\n  vars:\n    caddy_services:\n      - domain_name: \"test.moonpiedumpl.ing\"\n        port: 8000 # This is where the python http server runs by default, for my tests.\nThis almost worked, but I got an error in the caddy logs\ndial tcp 127.0.0.1:8000: connect: connection refused\nThe issue is that, unlike docker containers, podman containers have their own network interface. Attempting to curl localhost on them would result in the wrong network interface being accessed.\nAlso, I need to add the trusted_proxies option, otherwise caddy will not report the real ip address of the client.\nI adjusted the podman container to not forward any ports, and instead to be in “host” networking mode, where it shares the interfaces with the host.\n\n\nCaddyfile.j2\n\n{\n    http_port {{ caddy_http_port }}\n    https_port {{ caddy_https_port }}\n    {% if caddy_trusted_proxies | length &gt; 0 %}\n    trusted_proxies static {{ caddy_trusted_proxies | join(' ') }}\n    {% endif %}\n}\n\nExcept this errors. Caddy complains: Error: adapting config using caddyfile: /etc/caddy/Caddyfile:4: unrecognized global option: trusted_proxies and the container dies.\nAlso, the generated file looks really weird:\n\n\nCaddyfile\n\n{\n    http_port 8080\n    https_port 8443\n        trusted_proxies staic 154.12.245.181\n    }\ntest.moonpiedumpl.ing {\n        reverse_proxy http://127.0.0.1:8000\n            }\n\nThe spacing is off, and that might be the reason why it errors?\nI corrected the jinja2 spacing according to it’s docs, and also corrected the caddy config. trusted_proxies needs to be placed inside a servers block for it to be a global option, or inside each of the sites, if you want to make it site specific.\nThe corrected generated config:\n\n\nCaddyfile\n\n{\n    http_port 8080\n    https_port 8443\n    servers {\n        trusted_proxies static 154.12.245.181\n    }\n}\ntest.moonpiedumpl.ing {\n    reverse_proxy http://127.0.0.1:8000\n}\n\nHowever, this does not work. I only get:\n[moonpie@lizard moonpiedumplings.github.io]$ curl test.moonpiedumpl.ing\ncurl: (52) Empty reply from server\nThe caddy logs do not show anything relevant. I also did an additional test, by running a podman alpine container with host networking, where it was able to curl localhost just fine.\nIt is interesting, how Caddy returns an “empty reply”, rather than what I am expecting, a “502 Bad Gateway” — that’s the reply that you usually get when the backend server is down, which it was in a lot of my testing.\nSo I throw in a quick “Hello World”\nhello.moonpiedumpl.ing {\n    respond \"Hello World!\"\n}\nAnd still get an empty reply.\nThe caddy logs, however, do say soemthign interesting:\n\n\nShow Logs\n\n{\"level\":\"info\",\"ts\":1707778344.991714,\"logger\":\"http.auto_https\",\"msg\":\"server is listening only on the HTTPS port but has no TLS connection policies; adding one to enable TLS\",\"server_name\":\"srv0\",\"https_port\":8443}\n.....\n....\n.....\n{\"level\":\"info\",\"ts\":1707778344.991727,\"logger\":\"http.auto_https\",\"msg\":\"enabling automatic HTTP-&gt;HTTPS redirects\",\"server_name\":\"srv0\"}\n{\"level\":\"error\",\"ts\":1708452118.3423748,\"logger\":\"http.acme_client\",\"msg\":\"validating authorization\",\"identifier\":\"hello.moonpiedumpl.ing\",\"problem\":{\"type\":\"urn:ietf:params:acme:error:connection\",\"title\":\"\",\"detail\":\"154.12.245.181: Fetching http://hello.moonpiedumpl.ing/.well-known/acme-challenge/1qnsZcobKUGtdEG2KON-FeFwio97TFxWCu7hqXSY89s: Error getting validation data\",\"instance\":\"\",\"subproblems\":[]},\"order\":\"https://acme-v02.api.letsencrypt.org/acme/order/1560639027/246082250997\",\"attempt\":2,\"max_attempts\":3}\n{\"level\":\"error\",\"ts\":1708452118.3424225,\"logger\":\"tls.obtain\",\"msg\":\"could not get certificate from issuer\",\"identifier\":\"hello.moonpiedumpl.ing\",\"issuer\":\"acme-v02.api.letsencrypt.org-directory\",\"error\":\"HTTP 400 urn:ietf:params:acme:error:connection - 154.12.245.181: Fetching http://hello.moonpiedumpl.ing/.well-known/acme-challenge/1qnsZcobKUGtdEG2KON-FeFwio97TFxWCu7hqXSY89s: Error getting validation data\"}\n{\"level\":\"warn\",\"ts\":1708452118.3426523,\"logger\":\"http\",\"msg\":\"missing email address for ZeroSSL; it is strongly recommended to set one for next time\"}\n{\"level\":\"info\",\"ts\":1708452119.2906954,\"logger\":\"http\",\"msg\":\"generated EAB credentials\",\"key_id\":\"N8YqjedBRmclRYYFjYqZGw\"}\n{\"level\":\"info\",\"ts\":1708452136.0253348,\"logger\":\"http\",\"msg\":\"waiting on internal rate limiter\",\"identifiers\":[\"hello.moonpiedumpl.ing\"],\"ca\":\"https://acme.zerossl.com/v2/DV90\",\"account\":\"\"}\n{\"level\":\"info\",\"ts\":1708452136.0253737,\"logger\":\"http\",\"msg\":\"done waiting on internal rate limiter\",\"identifiers\":[\"hello.moonpiedumpl.ing\"],\"ca\":\"https://acme.zerossl.com/v2/DV90\",\"account\":\"\"}\n{\"level\":\"info\",\"ts\":1708452143.7124622,\"logger\":\"http.acme_client\",\"msg\":\"trying to solve challenge\",\"identifier\":\"hello.moonpiedumpl.ing\",\"challenge_type\":\"http-01\",\"ca\":\"https://acme.zerossl.com/v2/DV90\"}\n{\"level\":\"error\",\"ts\":1708452153.466225,\"logger\":\"http.acme_client\",\"msg\":\"challenge failed\",\"identifier\":\"hello.moonpiedumpl.ing\",\"challenge_type\":\"http-01\",\"problem\":{\"type\":\"\",\"title\":\"\",\"detail\":\"\",\"instance\":\"\",\"subproblems\":[]}}\n{\"level\":\"error\",\"ts\":1708452153.4662635,\"logger\":\"http.acme_client\",\"msg\":\"validating authorization\",\"identifier\":\"hello.moonpiedumpl.ing\",\"problem\":{\"type\":\"\",\"title\":\"\",\"detail\":\"\",\"instance\":\"\",\"subproblems\":[]},\"order\":\"https://acme.zerossl.com/v2/DV90/order/mlTQtXTaA_eyy7PFDbdZSg\",\"attempt\":1,\"max_attempts\":3}\n{\"level\":\"error\",\"ts\":1708452153.4663181,\"logger\":\"tls.obtain\",\"msg\":\"could not get certificate from issuer\",\"identifier\":\"hello.moonpiedumpl.ing\",\"issuer\":\"acme.zerossl.com-v2-DV90\",\"error\":\"HTTP 0  - \"}\n{\"level\":\"error\",\"ts\":1708452153.466387,\"logger\":\"tls.obtain\",\"msg\":\"will retry\",\"error\":\"[hello.moonpiedumpl.ing] Obtain: [hello.moonpiedumpl.ing] solving challenge: hello.moonpiedumpl.ing: [hello.moonpiedumpl.ing] authorization failed: HTTP 0  -  (ca=https://acme.zerossl.com/v2/DV90)\",\"attempt\":1,\"retrying_in\":60,\"elapsed\":41.717184567,\"max_duration\":2592000}\n\nIt seems like the certificate challenge fails. If caddy is doing auto https redirects, then it makes sense that the reply is empty.\nAnd curling https?\n[moonpie@lizard server-configs]$ curl https://hello.moonpiedumpl.ing\ncurl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to hello.moonpiedumpl.ing:443\nIf caddy isn’t serving on https properly, then it makes sense that this fails.\nLater, I think I discover the issue:\npodmaner@thoth:~/rathole$ podman ps\nCONTAINER ID  IMAGE                                      COMMAND               CREATED      STATUS            PORTS                                        NAMES\n868115b4e818  docker.io/moonpiedumplings/rathole:latest  rathole --client ...  2 weeks ago  Up 7 minutes ago                                               rathole\n860ec72e5c78  docker.io/library/caddy:latest             caddy run --confi...  2 weeks ago  Up 7 minutes ago  0.0.0.0:8080-&gt;80/tcp, 0.0.0.0:8443-&gt;443/tcp  caddy\nWhy is Caddy forwarding ports? My ansible playbooks set the podman networking to be host networking. It seems that, when rerunning a podman container using ansible, it doesn’t properly destroy it, leaving some lingering configs.\nHowever, I get an error:\npodmaner@thoth:~$ podman logs caddy\n...\n...\n{\"level\":\"info\",\"ts\":1709764202.9553177,\"logger\":\"tls.issuance.acme\",\"msg\":\"waiting on internal rate limiter\",\"identifiers\":[\"hello.moonpiedumpl.ing\"],\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\",\"account\":\"moonpiedumplings2@gmail.com\"}\n{\"level\":\"info\",\"ts\":1709764202.955384,\"logger\":\"tls.issuance.acme\",\"msg\":\"done waiting on internal rate limiter\",\"identifiers\":[\"hello.moonpiedumpl.ing\"],\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\",\"account\":\"moonpiedumplings2@gmail.com\"}\n{\"level\":\"info\",\"ts\":1709764202.9641545,\"logger\":\"tls.issuance.acme\",\"msg\":\"waiting on internal rate limiter\",\"identifiers\":[\"test.moonpiedumpl.ing\"],\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\",\"account\":\"moonpiedumplings2@gmail.com\"}\n{\"level\":\"info\",\"ts\":1709764202.964186,\"logger\":\"tls.issuance.acme\",\"msg\":\"done waiting on internal rate limiter\",\"identifiers\":[\"test.moonpiedumpl.ing\"],\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\",\"account\":\"moonpiedumplings2@gmail.com\"}\n{\"level\":\"info\",\"ts\":1709764203.0985403,\"logger\":\"tls.issuance.acme\",\"msg\":\"waiting on internal rate limiter\",\"identifiers\":[\"sso.moonpiedumpl.ing\"],\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\",\"account\":\"moonpiedumplings2@gmail.com\"}\n{\"level\":\"info\",\"ts\":1709764203.0985787,\"logger\":\"tls.issuance.acme\",\"msg\":\"done waiting on internal rate limiter\",\"identifiers\":[\"sso.moonpiedumpl.ing\"],\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\",\"account\":\"moonpiedumplings2@gmail.com\"}\n{\"level\":\"info\",\"ts\":1709764203.2928238,\"logger\":\"tls.issuance.acme.acme_client\",\"msg\":\"trying to solve challenge\",\"identifier\":\"test.moonpiedumpl.ing\",\"challenge_type\":\"tls-alpn-01\",\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\"}\n{\"level\":\"info\",\"ts\":1709764203.434616,\"logger\":\"tls.issuance.acme.acme_client\",\"msg\":\"trying to solve challenge\",\"identifier\":\"sso.moonpiedumpl.ing\",\"challenge_type\":\"tls-alpn-01\",\"ca\":\"https://acme-v02.api.letsencrypt.org/directory\"}\n{\"level\":\"error\",\"ts\":1709764203.478285,\"logger\":\"tls.obtain\",\"msg\":\"could not get certificate from issuer\",\"identifier\":\"test.moonpiedumpl.ing\",\"issuer\":\"acme-v02.api.letsencrypt.org-directory\",\"error\":\"[test.moonpiedumpl.ing] solving challenges: presenting for challenge: presenting with embedded solver: could not start listener for challenge server at :443: listen tcp :443: bind: permission denied (order=https://acme-v02.api.letsencrypt.org/acme/order/1605766427/250173951227) (ca=https://acme-v02.api.letsencrypt.org/directory)\"}\nAnd more of the same.\nSo apparently, Caddy doesn’t automatically do the acme challenge on the http_port or https_port, and you have to explicitly specify an alternate port:\nsso.moonpiedumpl.ing {\n    reverse_proxy http://127.0.0.1:9000\n\n    tls {\n        issuer acme {\n            alt_http_port 8080\n            alt_tlsalpn_port 8443\n        }\n    }\n}\nAnd of course, this is templated as part of my Caddyfile:\n{% for service in caddy_services %}\n{{ service.domain_name }} {\n    {% if service.port is defined %}\n    reverse_proxy http://127.0.0.1:{{ service.port }}\n    {% if service.request_body is defined %}\n    request_body {\n        max_size 2048MB\n    }\n    {% endif %}\n    {% endif %}\n    {% if service.file_root is defined %}\n    root * {{ service.file_root }}\n    file_server\n    {% endif %}\n    \n    tls {\n        issuer acme {\n            alt_http_port {{ caddy_http_port }}\n            alt_tlsalpn_port {{ caddy_https_port }}\n        }\n    }\n}\n{% endfor %}\nBut with this, Caddy finally works properly.\nI saw another interesting thing: caddy security module (github), which lets you integrate caddy with ldap or other forms of authentication. I want to inegrate this, because it would be better to have auth built into the single sign on.\n#jinja2:lstrip_blocks: True\n\nFROM docker.io/library/caddy:{{ caddy_version }}-builder AS builder\n\nRUN xcaddy build {% for plugin in caddy_plugins %}--with {{ plugin }}{% endfor %} \n\nFROM docker.io/library/caddy:{{ caddy_version }}\n\nCOPY --from=builder /usr/bin/caddy /usr/bin/caddy\nWith this, I have the caddy_security module installed.\nConfiguring it, is much, much harder.\nI started with copying the existing example Caddyfile they give:"
  },
  {
    "objectID": "projects/build-server-3/index.html#authentik",
    "href": "projects/build-server-3/index.html#authentik",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Authentik",
    "text": "Authentik\nThe listen settings are here: https://goauthentik.io/docs/installation/configuration/#listen-settings\nSo the podman pod which runs all of authentik will have those ports forwarded.\n- name: Create relevant podman pod\n  containers.podman.podman_pod:\n    name: authentik_pod\n    state: present\n    ports:\n      - \"9000:9000\"\n      - \"9443:9443\"\n      - \"3389:3389\"\n      - \"6636:3389\"\nI designed a role, and deployment was going pretty smoothly. Until it errored:\nfailed: [moonstack] (item=postgres) =&gt; {\n    \"ansible_loop_var\": \"item\",\n    \"changed\": false,\n    \"item\": \"postgres\",\n    \"module_stderr\": \"Shared connection to 192.168.17.197 closed.\",\n    \"module_stdout\": \"\nTraceback (most recent call last):\n  File \\\"/var/tmp/ansible-tmp-1708731694.1592944-80284-196261000245469/AnsiballZ_file.py\\\", line 107, in &lt;module&gt;\n    _ansiballz_main()\n  File \\\"/var/tmp/ansible-tmp-1708731694.1592944-80284-196261000245469/AnsiballZ_file.py\\\", line 99, in _ansiballz_main\n    invoke_module(zipped_mod, temp_path, ANSIBALLZ_PARAMS)\n  File \\\"/var/tmp/ansible-tmp-1708731694.1592944-80284-196261000245469/AnsiballZ_file.py\\\", line 47, in invoke_module\n    runpy.run_module(mod_name='ansible.modules.file', init_globals=dict(_module_fqn='ansible.modules.file', _modlib_path=modlib_path),\n  File \\\"&lt;frozen runpy&gt;\\\", line 226, in run_module\n  File \\\"&lt;frozen runpy&gt;\\\", line 98, in _run_module_code\n  File \\\"&lt;frozen runpy&gt;\\\", line 88, in _run_code\n  File \\\"/tmp/ansible_ansible.builtin.file_payload_37zf8n_k/ansible_ansible.builtin.file_payload.zip/ansible/modules/file.py\\\", line 987, in &lt;module&gt;\n  File \\\"/tmp/ansible_ansible.builtin.file_payload_37zf8n_k/ansible_ansible.builtin.file_payload.zip/ansible/modules/file.py\\\", line 973, in main\n  File \\\"/tmp/ansible_ansible.builtin.file_payload_37zf8n_k/ansible_ansible.builtin.file_payload.zip/ansible/modules/file.py\\\", line 680, in ensure_directory\n  File \\\"/tmp/ansible_ansible.builtin.file_payload_37zf8n_k/ansible_ansible.builtin.file_payload.zip/ansible/module_utils/basic.py\\\", line 1159, in set_fs_attributes_if_different\n  File \\\"/tmp/ansible_ansible.builtin.file_payload_37zf8n_k/ansible_ansible.builtin.file_payload.zip/ansible/module_utils/basic.py\\\", line 919, in set_mode_if_different\nPermissionError: [Errno 1] Operation not permitted: b'/home/podmaner/authentik/postgres'\",\n    \"msg\": \"MODULE FAILURE\\nSee stdout/stderr for the exact error\",\n    \"rc\": 1\n}\nWhich is… weird. Looking at the ansible code that generates this:\n- name: Create relevant configuration files\n  ansible.builtin.file:\n    path: \"{{ homepath.stdout }}/authentik/{{ item }}\"\n    state: directory\n    mode: '0755'\n  loop: [\"media\", \"custom-templates\", \"certs\", \"postgres\", \"redis\"]\nIt’s the same exact task, looped through… but it only errors for /home/podmaner/authentik/postgres ‽\npodmaner@thoth:~/authentik$ ls -la\ntotal 28\ndrwxr-xr-x 7 podmaner podmaner 4096 Feb 23 15:37 .\ndrwx------ 9 podmaner podmaner 4096 Feb 23 15:37 ..\ndrwxr-xr-x 2 podmaner podmaner 4096 Feb 23 15:37 certs\ndrwxr-xr-x 2 podmaner podmaner 4096 Feb 23 15:37 custom-templates\ndrwxr-xr-x 2 podmaner podmaner 4096 Feb 23 15:37 media\ndrwx------ 2   165605 podmaner 4096 Feb 23 15:37 postgres\ndrwxr-xr-x 2   166534 podmaner 4096 Feb 23 15:39 redis\nFor some reason, only the postgres directory has incorrect permissions?\nAnd the weirdest part: This only happens upon a rerun of the role. If I delete the /home/podmaner/authentik/ directory, and run the role, then everything runs properly, and the postgres directory gets the proper permissions.\nWhy does this happen? My intuition is telling me this is a bug, since the behavior is so blatantly inconsistent.\nIt’s not really that though. After deleting the postgres directory, it was recreated.\npodmaner@thoth:~/authentik$ podman logs authentik_postgres\nError: Database is uninitialized and superuser password is not specified.\n       You must specify POSTGRES_PASSWORD to a non-empty value for the\n       superuser. For example, \"-e POSTGRES_PASSWORD=password\" on \"docker run\".\n\n       You may also use \"POSTGRES_HOST_AUTH_METHOD=trust\" to allow all\n       connections without a password. This is *not* recommended.\n\n       See PostgreSQL documentation about \"trust\":\n       https://www.postgresql.org/docs/current/auth-trust.html\nWhoops. Lol. I was attempting to convet authentik’s docker-compose.yml to podman+ansible instructions, but my copy was incomplete, and I forgot to configure the postgres container properly.\nThat explains why the container won’t start… but it still doesn’t explain the file permissions error.\nAfter taking a closer look at the output of ls -la, it seems that the podman containers change ownership of the files. Why doesn’t it do this for any of my other containers?\nI’m guessing this is because the postgres and redis container, unlike every other container, have the main process inside as a different user other than root.\nI looked into podman user namespaces. I tried setting user namespaces for the relevant containers, first, but I got an error:\nTASK [authentik : Authentik postgres database] ***********************************************************************************************************************\nfatal: [moonstack]: FAILED! =&gt; {\"changed\": false, \"msg\": \"Can't run container authentik_postgres\", \"stderr\": \"Error: --userns and --pod cannot be set together\\n\", \"stderr_lines\": [\"Error: --userns and --pod cannot be set together\"], \"stdout\": \"\", \"stdout_lines\": []}\nSo instead, I set the usernamespace for the whole pod, and it didn’t erorr:\n- name: Create relevant podman pod  \n      containers.podman.podman_pod:\n        name: authentik_pod\n        recreate: true\nIt seems like the error is that, the podmaner user would no longer have write permissions, preventing it from doing anything to those specific files.\nHowever, the podman container still died.\n\n\nShow error\n\npodmaner@thoth:~$ podman logs authentik_worker\n{\"event\": \"Loaded config\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708731540.8134813, \"file\": \"/authentik/lib/default.yml\"}\n{\"event\": \"Loaded environment variables\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708731540.813802, \"count\": 5}\n{\"event\": \"Starting authentik bootstrap\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708731540.8138793}\n{\"event\": \"----------------------------------------------------------------------\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708731540.8139021}\n{\"event\": \"Secret key missing, check https://goauthentik.io/docs/installation/.\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708731540.8139138}\n{\"event\": \"----------------------------------------------------------------------\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708731540.8139226}\n{\"event\": \"Loaded config\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708807909.1802225, \"file\": \"/authentik/lib/default.yml\"}\n{\"event\": \"Loaded environment variables\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708807909.1805325, \"count\": 5}\n{\"event\": \"Starting authentik bootstrap\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708807909.1806104}\n{\"event\": \"----------------------------------------------------------------------\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708807909.180633}\n{\"event\": \"Secret key missing, check https://goauthentik.io/docs/installation/.\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708807909.1806445}\n{\"event\": \"----------------------------------------------------------------------\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708807909.1806536}\n{\"event\": \"Loaded config\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817283.6261683, \"file\": \"/authentik/lib/default.yml\"}\n{\"event\": \"Loaded environment variables\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817283.6264079, \"count\": 5}\n{\"event\": \"Starting authentik bootstrap\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817283.6264803}\n{\"event\": \"----------------------------------------------------------------------\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817283.6265018}\n{\"event\": \"Secret key missing, check https://goauthentik.io/docs/installation/.\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817283.626513}\n{\"event\": \"----------------------------------------------------------------------\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817283.6265216}\npodmaner@thoth:~$ podman logs authentik_server\n{\"event\": \"Loaded config\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817279.5668721, \"file\": \"/authentik/lib/default.yml\"}\n{\"event\": \"Loaded environment variables\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817279.567172, \"count\": 5}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6441257, \"path\": \"authentik.admin.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.644982, \"path\": \"authentik.crypto.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6455739, \"path\": \"authentik.events.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6467671, \"path\": \"authentik.outposts.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6484737, \"path\": \"authentik.policies.reputation.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.651618, \"path\": \"authentik.providers.scim.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6528664, \"path\": \"authentik.sources.ldap.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6535037, \"path\": \"authentik.sources.oauth.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6540685, \"path\": \"authentik.sources.plex.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6560044, \"path\": \"authentik.stages.authenticator_totp.settings\"}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6598718, \"path\": \"authentik.blueprints.settings\"}\n{\"event\": \"Booting authentik\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.6600318, \"version\": \"2023.10.7\"}\n{\"event\": \"Enabled authentik enterprise\", \"level\": \"info\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.661141}\n{\"event\": \"Loaded app settings\", \"level\": \"debug\", \"logger\": \"authentik.lib.config\", \"timestamp\": 1708817281.661508, \"path\": \"authentik.enterprise.settings\"}\n{\"event\": \"Loaded GeoIP database\", \"last_write\": 1706549214.0, \"level\": \"info\", \"logger\": \"authentik.events.geo\", \"pid\": 2, \"timestamp\": \"2024-02-24T23:28:02.805111\"}\n{\"app_name\": \"authentik.crypto\", \"event\": \"Failed to run reconcile\", \"exc\": \"ProgrammingError('relation \\\"authentik_crypto_certificatekeypair\\\" does not exist\\\\nLINE 1: ...hentik_crypto_certificatekeypair\\\".\\\"key_data\\\" FROM \\\"authentik...\\\\n                                                             ^')\", \"level\": \"warning\", \"logger\": \"authentik.blueprints.apps\", \"name\": \"managed_jwt_cert\", \"pid\": 2, \"timestamp\": \"2024-02-24T23:28:03.610865\"}\n{\"app_name\": \"authentik.crypto\", \"event\": \"Failed to run reconcile\", \"exc\": \"ProgrammingError('relation \\\"authentik_crypto_certificatekeypair\\\" does not exist\\\\nLINE 1: SELECT 1 AS \\\"a\\\" FROM \\\"authentik_crypto_certificatekeypair\\\" W...\\\\n                             ^')\", \"level\": \"warning\", \"logger\": \"authentik.blueprints.apps\", \"name\": \"self_signed\", \"pid\": 2, \"timestamp\": \"2024-02-24T23:28:03.612779\"}\n{\"app_name\": \"authentik.outposts\", \"event\": \"Failed to run reconcile\", \"exc\": \"ProgrammingError('relation \\\"authentik_outposts_outpost\\\" does not exist\\\\nLINE 1: ..._id\\\", \\\"authentik_outposts_outpost\\\".\\\"_config\\\" FROM \\\"authentik...\\\\n                                                             ^')\", \"level\": \"warning\", \"logger\": \"authentik.blueprints.apps\", \"name\": \"embedded_outpost\", \"pid\": 2, \"timestamp\": \"2024-02-24T23:28:03.647866\"}\n{\"event\": \"Task published\", \"level\": \"info\", \"logger\": \"authentik.root.celery\", \"pid\": 2, \"task_id\": \"d28b9ddc12eb44eaa99a25dfe3477f37\", \"task_name\": \"authentik.blueprints.v1.tasks.blueprints_discovery\", \"timestamp\": \"2024-02-24T23:28:03.876853\"}\n{\"event\": \"Task published\", \"level\": \"info\", \"logger\": \"authentik.root.celery\", \"pid\": 2, \"task_id\": \"763c497b4e4c43e2bb745de6a613e104\", \"task_name\": \"authentik.blueprints.v1.tasks.clear_failed_blueprints\", \"timestamp\": \"2024-02-24T23:28:03.878293\"}\n{\"app_name\": \"authentik.core\", \"event\": \"Failed to run reconcile\", \"exc\": \"ProgrammingError('relation \\\"authentik_core_source\\\" does not exist\\\\nLINE 1: ...\\\"authentik_core_source\\\".\\\"user_matching_mode\\\" FROM \\\"authentik...\\\\n                                                             ^')\", \"level\": \"warning\", \"logger\": \"authentik.blueprints.apps\", \"name\": \"source_inbuilt\", \"pid\": 2, \"timestamp\": \"2024-02-24T23:28:03.881205\"}\\\n\nIt seems I forgot another environment variable, which has been documented on authentik’s site.\nI did that, and it still dies.\nInvestigating the error, it seems to be something about relevant file permissions, it is unable to create the necessary file.\nTraceback (most recent call last):\n  File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n  File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n  File \"/lifecycle/migrate.py\", line 98, in &lt;module&gt;\n    migration.run()\n  File \"/lifecycle/system_migrations/tenant_files.py\", line 15, in run\n    TENANT_MEDIA_ROOT.mkdir(parents=True)\n  File \"/usr/local/lib/python3.12/pathlib.py\", line 1311, in mkdir\n    os.mkdir(self, mode)\nPermissionError: [Errno 13] Permission denied: '/media/public'\nI need to do more research into how rootless podman, and the different userns modes, interact with volumes.\nI found one article, but it’s focus isn’t quite what I want, and it lacks overview of the different userns modes, and how they interact with volume mounts.\nI spent some time experimenting, and did some more research. Something crucial was a comment I noticed in the docker-compose, under the section related to the “worker” node of authentik:\n# `user: root` and the docker socket volume are optional.\n    # See more for the docker socket integration here:\n    # https://goauthentik.io/docs/outposts/integrations/docker\n    # Removing `user: root` also prevents the worker from fixing the permissions\n    # on the mounted folders, so when removing this make sure the folders have the correct UID/GID\n    # (1000:1000 by default)\nSo apparently, authentik expects to exist as user 1000 and group 1000, and not anything else. Thankfully, podman has a special userns mode, to map a UID/GID inside the container to a different host user:\n- name: Create relevant podman pod  \n  containers.podman.podman_pod:\n    name: authentik_pod\n    recreate: true\n    # userns: host\n    userns: keep-id:uid=1000,gid=1000\n    state: started\n    ......\nAfter this, it works. But I’m still not satisfied. Postgres keeps on changing the permissions of it’s own bind mount.\nAccording to the docker library page for postgres (from Arbitrary –user Notes):\n\nThe main caveat to note is that postgres doesn’t care what UID it runs as (as long as the owner of /var/lib/postgresql/data matches), but initdb does care (and needs the user to exist in /etc/passwd):\n\nIt then offers three ways to get around this, and one way is to use the --user flag, in docker. Red Hat has an article mentioning that this exists in podman as well. It didn’t work.\nI did some more research into why it is not possible for there to be a different usernamespace mapping for each container in a pod, and the short version is that containers within a pod need to be in the same user namespace in order for them to interact with eachother properly, like networking.\nI made a Dockerfile for postgres, adding the authentik user to the container:\nFROM docker.io/library/postgres:12-alpine\n\nRUN echo \"authentik:x:1000:1000::/authentik:/usr/sbin/nologin\" &gt; /etc/passwd\nIt still chowns/chmods the whole directory to be 700, but that’s okay. Apparently, this is intended behavior, hardcoded, and postgres refuses to start if file permissions are not 700. It’s required that the user running postgres, exclusively owns the postgres data folders.\nBut now file permissions are correct at least, and the user in the container is mapped to the user outside the container, enabling the directory to be manipulated properly (backed up, restored, etc) outside of the container.\nHowever, I still get another error. Even with a fresh authentik deployment, the authentik_worker container gives an error:\nTraceback (most recent call last):\n  File \"/lifecycle/migrate.py\", line 98, in &lt;module&gt;\n    migration.run()\n  File \"/lifecycle/system_migrations/tenant_to_brand.py\", line 25, in run\n    self.cur.execute(SQL_STATEMENT)\n  File \"/ak-root/venv/lib/python3.12/site-packages/psycopg/cursor.py\", line 732, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.DuplicateTable: relation \"authentik_brands_brand\" already exists\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"&lt;frozen runpy&gt;\", line 198, in _run_module_as_main\n  File \"&lt;frozen runpy&gt;\", line 88, in _run_code\n  File \"/lifecycle/migrate.py\", line 118, in &lt;module&gt;\n    release_lock(curr)\n  File \"/lifecycle/migrate.py\", line 67, in release_lock\n    cursor.execute(\"SELECT pg_advisory_unlock(%s)\", (ADV_LOCK_UID,))\n  File \"/ak-root/venv/lib/python3.12/site-packages/psycopg/cursor.py\", line 732, in execute\n    raise ex.with_traceback(None)\npsycopg.errors.InFailedSqlTransaction: current transaction is aborted, commands ignored until end of transaction block\nI think this is because there is no container dependecies or the like set up, so the authentik_worker starts before postgres is finished doing it’s thing. To test, I deployed the databases first, and then started the worker and server containers. Sure enough, authentik then deploys properly, without erroring.\nTo get around this, I need to add dependencies to my containers. To do that, I need to use podman’s systemd integration. Since it’s not appropriate for this section (since it will apply to many other services), I am continuing in podman systemd integration\n\nConfiguration\nYeah, there’s a lot of stuff to configure. Single-Sign-On is very, very complex.\nI started out by adding a single user, my main user under Directory &gt; Users, called “moonpiedumplings”.\nThis user is usable via\n\nApplications\n\n\nInvitations\nAuthentik has a feature called “invitations”, where it can generate a one or multiple time usable link that people can use to self register.\nThe goal here is that all my users are stored within authentik, and no services have any local users. I can use invites to\nHere are the docs on invitations: https://docs.goauthentik.io/docs/user-group-role/user/invitations\n\n\nTeraform\nAuthentik has a terraform provider: https://registry.terraform.io/providers/goauthentik/authentik/latest/docs\nBut I don’t think I’m going to use it, as that’s yet another layer of complexity."
  },
  {
    "objectID": "projects/build-server-3/index.html#forgejo",
    "href": "projects/build-server-3/index.html#forgejo",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Forgejo",
    "text": "Forgejo\nI started by finding forgejo’s docker installation guides: https://forgejo.org/docs/next/admin/installation-docker/\nHowever, there is another problem:\nAlthough forgejo supports rootless, it seems to expect a userid and groupid of 1000:1000, which is not the user I am running it as. Similar to Authentik, I will have to create the pod with userns mapping.\nI basically copied the Authentik deployment, and got it deployed.\n\nSo it seems that, when forgejo is first created, the initial user that registrates will be the “admin” user, able to administrate the site. I can’t add the oauth2 from the openid.\nI created an admin account, and am given an interface to add an oauth2 provider, but I’m wondering if there is some way to do this via configuration files, instead of from the GUI."
  },
  {
    "objectID": "projects/build-server-3/index.html#bitwardenvaultwarden",
    "href": "projects/build-server-3/index.html#bitwardenvaultwarden",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Bitwarden/Vaultwarden",
    "text": "Bitwarden/Vaultwarden\nI want to self host a password manager, and I have selected vaultwarden, the open source version of bitwarden.\nLike other services, I don’t want vaultwarden to have any of it’s own users, and instead to connect to Authentik via LDAP or Oauth2.\nHowever, I quickly realized a problem: Password managers (when properly architectured), encrypt the password database via a master password… if Vaultwarden is authneticating users via LDAP, then how will it get the master password?\nI found a few solutions\n\nhttps://github.com/dani-garcia/vaultwarden/wiki/Syncing-users-from-LDAP\n\nThis seems more doable, but it doesn’t sync passwords, merely automates invites to vaultwarden.\n\nhttps://github.com/bitwarden/directory-connector\n\nI don’t understand how this software works fully yet, but it seems ot sync vaultwarden users with LDAP.\nHere are some docs: https://bitwarden.com/help/ldap-directory/\nThis seems to a be better, but more complex to use. I think it syncs passwords as well, but I don’t know yet."
  },
  {
    "objectID": "projects/build-server-3/index.html#kubernetes",
    "href": "projects/build-server-3/index.html#kubernetes",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Kubernetes",
    "text": "Kubernetes\n()\npodmaner@thoth:~$ podman kube generate authentik_pod\nspec:\n  automountServiceAccountToken: false\n  containers:\n  ....\n  ....\n  hostUsers: false\n  hostname: authentik_pod\n \nThis isn’t quite right. hostUsers: false means to not use user namespaces, which I am using right now in podman. See the [kubernetes docs]"
  },
  {
    "objectID": "projects/build-server-3/index.html#openstack-notes",
    "href": "projects/build-server-3/index.html#openstack-notes",
    "title": "Building my server part 3 — The switch to debian",
    "section": "Openstack Notes",
    "text": "Openstack Notes\nI will probably get to this later on.\nRather than trying to do an openstack native implemenation of a public ipv6 addresses for virtual machines on a private ip address, I can simply have my router set up a “private” ipv6 subnet, and then VPN (or an L2TP, which does not come with encryption). Then, I can do a 1 to 1 NAT, or something of the sort, but without any address translation. By putting VM’s on this subnet, I can give them public ipv6 addresses. This is simpler, and compatible with more than just openstack.\nSomething like this is definitely possible.\nhttps://superuser.com/questions/887745/routing-a-particular-subnet-into-a-vpn-tunnel\nhttps://serverfault.com/questions/608517/how-to-route-traffic-from-private-network-to-openvpn-subnet-and-back\nhttps://superuser.com/questions/1162231/route-public-subnet-over-vpn"
  },
  {
    "objectID": "playground/podman-docker/index.html",
    "href": "playground/podman-docker/index.html",
    "title": "Podman vs Docker",
    "section": "",
    "text": "So I’ve been having some trouble, not with docker itself, but the way it interacts with the rest of the system. The big thing is that docker overwrites iptables rules, meaning any other bridges won’t work properly unless you set up iptables rules with the -I DOCKER-USER, which specifically sits in front of docker.\nI encountered this issue with linux containers, where the lxc containers did not have networking when docker was installed.\nI also encoutered a similar issue when trying to setup NAT, the instructions on the arch wiki stated that the steps for machines that had docker installed were different but I did not realize this in my previous research.\nThe hope is that podman will have no such issue. But does podman have every feature of docker?\nThe first thing to consider is that podman has a docker-compat setup, where it gives you a compatibility command:\n[moonpie@cachyos-x8664 containers]$ docker ps\nEmulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.\nCONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES\nIt’s pretty good. Except:\n[moonpie@cachyos-x8664 registries.conf.d]$ docker pull jc21/nginx-proxy-manager:latest\nEmulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.\nError: short-name \"jc21/nginx-proxy-manager:latest\" did not resolve to an alias and no unqualified-search registries are defined in \"/etc/containers/registries.conf\"\nUnlike docker, podman does not have the &lt;docker.io&gt; as a default registry. You can still pull with docker.io/jc21/nginx-proxy-manager, but it’s inconvinient to change every script and whatnot to be like that.\nThe Red Hat Enable Sysadmin has a blog on this. In the “pulling by shortnames” section they changed some podman settings:\n\n\n/etc/containers/registries.conf\n\n...\n...\nunqualified-search-registries = ['registry.fedoraproject.org', 'registry.access.redhat.com', 'registry.centos.org', 'docker.io']\n...\n...\n\nThis makes it so that podman will search for these images when not specificially told a registry.\n[moonpie@cachyos-x8664 containers]$ docker pull jc21/nginx-proxy-manager\nEmulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg.\n? Please select an image: \n  ▸ registry.fedoraproject.org/jc21/nginx-proxy-manager:latest\n    registry.access.redhat.com/jc21/nginx-proxy-manager:latest\n    registry.centos.org/jc21/nginx-proxy-manager:latest\n    docker.io/jc21/nginx-proxy-manager:latest\nWow, this is pretty cool. It gives you an option to select when you have multiple default registries. If you only have one, then it just immediately defaults to that.\nPodman also has support for docker-compose\nOnce you have the docker compatibility package installed, simply start the podman service/socket, and docker-compose will work.\nExcept, I am wondering, will docker networks work? Podman seems to do something different, with podman pods, which don’t behave the same way as docker networks.\nTo test, I have created a simple docker-compose.yml, which has two contianers on the same network:\nversion: '3'\nservices:\n  alpine_container_1:\n    image: alpine:latest\n    command: [\"tail\", \"-f\", \"/dev/null\"]  # Keeps the container running\n    networks:\n      - my_network\n\n  alpine_container_2:\n    image: alpine:latest\n    command: [\"tail\", \"-f\", \"/dev/null\"]  # Keeps the container running\n    networks:\n      - my_network\n\nnetworks:\n  my_network:\nIt seems to work, but….\n[moonpie@cachyos-x8664 podman-docker]$ sudo podman network inspect podman-docker_my_network\n[\n     {\n          \"name\": \"podman-docker_my_network\",\n          \"id\": \"0b11ee2d93ccd3427374b667b55dd7c42f12d68d296f6dca2ccad12a491b7e7d\",\n          \"driver\": \"bridge\",\n          \"network_interface\": \"podman1\",\n          \"created\": \"2023-09-03T06:21:14.350164238-07:00\",\n          \"subnets\": [\n               {\n                    \"subnet\": \"10.89.0.0/24\",\n                    \"gateway\": \"10.89.0.1\"\n               }\n          ],\n          \"ipv6_enabled\": false,\n          \"internal\": false,\n          \"dns_enabled\": true,\n          \"labels\": {\n               \"com.docker.compose.network\": \"my_network\",\n               \"com.docker.compose.project\": \"podman-docker\",\n               \"com.docker.compose.version\": \"2.20.3\"\n          },\n          \"options\": {\n               \"isolate\": \"true\"\n          },\n          \"ipam_options\": {\n               \"driver\": \"host-local\"\n          }\n     }\n]\n[moonpie@cachyos-x8664 podman-docker]$ \nWhereas, when using docker network inspect:\nmoonpie@office:~$ docker network inspect mine\n[\n    {\n        \"Name\": \"mine\",\n        \"Id\": \"73125f7a201062e8561f6d34110c2fdca2a3e21c423998e58da9ded56222a954\",\n        \"Created\": \"2023-02-23T03:57:12.249530356Z\",\n        \"Scope\": \"local\",\n        \"Driver\": \"bridge\",\n        \"EnableIPv6\": false,\n        \"IPAM\": {\n            \"Driver\": \"default\",\n            \"Options\": {},\n            \"Config\": [\n                {\n                    \"Subnet\": \"172.18.0.0/16\",\n                    \"Gateway\": \"172.18.0.1\"\n                }\n            ]\n        },\n        \"Internal\": false,\n        \"Attachable\": false,\n        \"Ingress\": false,\n        \"ConfigFrom\": {\n            \"Network\": \"\"\n        },\n        \"ConfigOnly\": false,\n        \"Containers\": {\n            \"01af5e2990ba9c4cee41de65500f72bd2a3c8e373bce373be251cf5a7a4549dd\": {\n                \"Name\": \"meshcentral\",\n                \"EndpointID\": \"c8b101d499a88d7c9ed0d476fa09915018e58fea0e83b24093b2d8768c12c464\",\n                \"MacAddress\": \"02:42:ac:12:00:07\",\n                \"IPv4Address\": \"172.18.0.7/16\",\n                \"IPv6Address\": \"\"\n            },\n            \"329b7fd59807573522a92c0e006f9cf92d44133a1f4e197d25f666a66bc5241e\": {\n                \"Name\": \"v2ray_proxy_1\",\n                \"EndpointID\": \"01ff85e55d6f1c9dee40ec09dbc258a1a5d5ad2728d35e3020d52211c076ebb1\",\n                \"MacAddress\": \"02:42:ac:12:00:03\",\n                \"IPv4Address\": \"172.18.0.3/16\",\n                \"IPv6Address\": \"\"\n            },\n        },\n        \"Options\": {},\n        \"Labels\": {}\n    }\n]\nWhen using docker to to create networks, it tells you the ip addressed fo the containers in a network. Podman doesn’t do this.\nBut can the containers interact with eachother?\nYes, they can. While in one container, I could ping the other container. However, I had to get the container ip addresses by creating a shell in the containers, and then running ip a to view the ip addresses first.\nIs there any way to view the container ip addresses from outside the container? Not all containers come with a shell builtin, some don’t have it for size purposes, and solely contain what is needed to run whatever application the container is packaging.\n\n\nPodman inspect\n\n[root@cachyos-x8664 podman-docker]# podman inspect podman-docker-alpine_container_1-1 \n[\n     {\n          \"Id\": \"eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7\",\n          \"Created\": \"2023-09-03T06:21:14.357752403-07:00\",\n          \"Path\": \"tail\",\n          \"Args\": [\n               \"-f\",\n               \"/dev/null\"\n          ],\n          \"State\": {\n               \"OciVersion\": \"1.1.0-rc.3\",\n               \"Status\": \"running\",\n               \"Running\": true,\n               \"Paused\": false,\n               \"Restarting\": false,\n               \"OOMKilled\": false,\n               \"Dead\": false,\n               \"Pid\": 37035,\n               \"ConmonPid\": 37033,\n               \"ExitCode\": 0,\n               \"Error\": \"\",\n               \"StartedAt\": \"2023-09-03T06:21:14.742250101-07:00\",\n               \"FinishedAt\": \"0001-01-01T00:00:00Z\",\n               \"Health\": {\n                    \"Status\": \"\",\n                    \"FailingStreak\": 0,\n                    \"Log\": null\n               },\n               \"CgroupPath\": \"/machine.slice/libpod-eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7.scope\",\n               \"CheckpointedAt\": \"0001-01-01T00:00:00Z\",\n               \"RestoredAt\": \"0001-01-01T00:00:00Z\"\n          },\n          \"Image\": \"7e01a0d0a1dcd9e539f8e9bbd80106d59efbdf97293b3d38f5d7a34501526cdb\",\n          \"ImageDigest\": \"sha256:7144f7bab3d4c2648d7e59409f15ec52a18006a128c733fcff20d3a4a54ba44a\",\n          \"ImageName\": \"docker.io/library/alpine:latest\",\n          \"Rootfs\": \"\",\n          \"Pod\": \"\",\n          \"ResolvConfPath\": \"/run/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata/resolv.conf\",\n          \"HostnamePath\": \"/run/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata/hostname\",\n          \"HostsPath\": \"/run/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata/hosts\",\n          \"StaticDir\": \"/var/lib/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata\",\n          \"OCIConfigPath\": \"/var/lib/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata/config.json\",\n          \"OCIRuntime\": \"crun\",\n          \"ConmonPidFile\": \"/run/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata/conmon.pid\",\n          \"PidFile\": \"/run/containers/storage/overlay-containers/eae4cc793f5f86b69e81a785cc32df36718250436d67e1b62037414b1d8e53e7/userdata/pidfile\",\n          \"Name\": \"podman-docker-alpine_container_1-1\",\n          \"RestartCount\": 0,\n          \"Driver\": \"overlay\",\n          \"MountLabel\": \"\",\n          \"ProcessLabel\": \"\",\n          \"AppArmorProfile\": \"\",\n          \"EffectiveCaps\": [\n               \"CAP_CHOWN\",\n               \"CAP_DAC_OVERRIDE\",\n               \"CAP_FOWNER\",\n               \"CAP_FSETID\",\n               \"CAP_KILL\",\n               \"CAP_NET_BIND_SERVICE\",\n               \"CAP_SETFCAP\",\n               \"CAP_SETGID\",\n               \"CAP_SETPCAP\",\n               \"CAP_SETUID\",\n               \"CAP_SYS_CHROOT\"\n          ],\n          \"BoundingCaps\": [\n               \"CAP_CHOWN\",\n               \"CAP_DAC_OVERRIDE\",\n               \"CAP_FOWNER\",\n               \"CAP_FSETID\",\n               \"CAP_KILL\",\n               \"CAP_NET_BIND_SERVICE\",\n               \"CAP_SETFCAP\",\n               \"CAP_SETGID\",\n               \"CAP_SETPCAP\",\n               \"CAP_SETUID\",\n               \"CAP_SYS_CHROOT\"\n          ],\n          \"ExecIDs\": [\n               \"8354949b4e0bf728dafa7b3e7b857ccce18e953b0f20473dbb06fe6442bf1d12\"\n          ],\n          \"GraphDriver\": {\n               \"Name\": \"overlay\",\n               \"Data\": {\n                    \"LowerDir\": \"/var/lib/containers/storage/overlay/4693057ce2364720d39e57e85a5b8e0bd9ac3573716237736d6470ec5b7b7230/diff\",\n                    \"MergedDir\": \"/var/lib/containers/storage/overlay/bbadcba900462960315aa406866164bd58a12573e76140093563525f595b1732/merged\",\n                    \"UpperDir\": \"/var/lib/containers/storage/overlay/bbadcba900462960315aa406866164bd58a12573e76140093563525f595b1732/diff\",\n                    \"WorkDir\": \"/var/lib/containers/storage/overlay/bbadcba900462960315aa406866164bd58a12573e76140093563525f595b1732/work\"\n               }\n          },\n          \"Mounts\": [],\n          \"Dependencies\": [],\n          \"NetworkSettings\": {\n               \"EndpointID\": \"\",\n               \"Gateway\": \"\",\n               \"IPAddress\": \"\",\n               \"IPPrefixLen\": 0,\n               \"IPv6Gateway\": \"\",\n               \"GlobalIPv6Address\": \"\",\n               \"GlobalIPv6PrefixLen\": 0,\n               \"MacAddress\": \"\",\n               \"Bridge\": \"\",\n               \"SandboxID\": \"\",\n               \"HairpinMode\": false,\n               \"LinkLocalIPv6Address\": \"\",\n               \"LinkLocalIPv6PrefixLen\": 0,\n               \"Ports\": {},\n               \"SandboxKey\": \"/run/netns/netns-4a204211-d01a-6d0f-bfd3-bfc34e73885e\",\n               \"Networks\": {\n                    \"podman-docker_my_network\": {\n                         \"EndpointID\": \"\",\n                         \"Gateway\": \"10.89.0.1\",\n                         \"IPAddress\": \"10.89.0.2\",\n                         \"IPPrefixLen\": 24,\n                         \"IPv6Gateway\": \"\",\n                         \"GlobalIPv6Address\": \"\",\n                         \"GlobalIPv6PrefixLen\": 0,\n                         \"MacAddress\": \"b2:af:b4:78:28:1b\",\n                         \"NetworkID\": \"podman-docker_my_network\",\n                         \"DriverOpts\": null,\n                         \"IPAMConfig\": null,\n                         \"Links\": null,\n                         \"Aliases\": [\n                              \"podman-docker-alpine_container_1-1\",\n                              \"alpine_container_1\",\n                              \"eae4cc793f5f\"\n                         ]\n                    }\n               }\n          },\n          \"Namespace\": \"\",\n          \"IsInfra\": false,\n          \"IsService\": false,\n          \"KubeExitCodePropagation\": \"invalid\",\n          \"lockNumber\": 0,\n          \"Config\": {\n               \"Hostname\": \"eae4cc793f5f\",\n               \"Domainname\": \"\",\n               \"User\": \"\",\n               \"AttachStdin\": false,\n               \"AttachStdout\": false,\n               \"AttachStderr\": false,\n               \"Tty\": false,\n               \"OpenStdin\": false,\n               \"StdinOnce\": false,\n               \"Env\": [\n                    \"TERM=xterm\",\n                    \"container=podman\",\n                    \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                    \"HOME=/root\",\n                    \"HOSTNAME=eae4cc793f5f\"\n               ],\n               \"Cmd\": [\n                    \"tail\",\n                    \"-f\",\n                    \"/dev/null\"\n               ],\n               \"Image\": \"docker.io/library/alpine:latest\",\n               \"Volumes\": null,\n               \"WorkingDir\": \"/\",\n               \"Entrypoint\": \"\",\n               \"OnBuild\": null,\n               \"Labels\": {\n                    \"com.docker.compose.config-hash\": \"6418caee2d42d6859badadf595f62af5fc514baebd949edbc3b925c9dd53ecc3\",\n                    \"com.docker.compose.container-number\": \"1\",\n                    \"com.docker.compose.depends_on\": \"\",\n                    \"com.docker.compose.image\": \"sha256:7e01a0d0a1dcd9e539f8e9bbd80106d59efbdf97293b3d38f5d7a34501526cdb\",\n                    \"com.docker.compose.oneoff\": \"False\",\n                    \"com.docker.compose.project\": \"podman-docker\",\n                    \"com.docker.compose.project.config_files\": \"/home/moonpie/vscode/moonpiedumplings.github.io/playground/podman-docker/docker-compose.yml\",\n                    \"com.docker.compose.project.working_dir\": \"/home/moonpie/vscode/moonpiedumplings.github.io/playground/podman-docker\",\n                    \"com.docker.compose.service\": \"alpine_container_1\",\n                    \"com.docker.compose.version\": \"2.20.3\"\n               },\n               \"Annotations\": {\n                    \"io.container.manager\": \"libpod\",\n                    \"org.opencontainers.image.stopSignal\": \"15\"\n               },\n               \"StopSignal\": 15,\n               \"HealthcheckOnFailureAction\": \"none\",\n               \"CreateCommand\": [\n                    \"/usr/bin/podman\",\n                    \"--log-level=info\",\n                    \"system\",\n                    \"service\"\n               ],\n               \"Umask\": \"0000\",\n               \"Timeout\": 0,\n               \"StopTimeout\": 0,\n               \"Passwd\": true\n          },\n          \"HostConfig\": {\n               \"Binds\": [],\n               \"CgroupManager\": \"systemd\",\n               \"CgroupMode\": \"private\",\n               \"ContainerIDFile\": \"\",\n               \"LogConfig\": {\n                    \"Type\": \"journald\",\n                    \"Config\": null,\n                    \"Path\": \"\",\n                    \"Tag\": \"\",\n                    \"Size\": \"0B\"\n               },\n               \"NetworkMode\": \"bridge\",\n               \"PortBindings\": {},\n               \"RestartPolicy\": {\n                    \"Name\": \"\",\n                    \"MaximumRetryCount\": 0\n               },\n               \"AutoRemove\": false,\n               \"VolumeDriver\": \"\",\n               \"VolumesFrom\": null,\n               \"CapAdd\": [],\n               \"CapDrop\": [],\n               \"Dns\": [],\n               \"DnsOptions\": [],\n               \"DnsSearch\": [],\n               \"ExtraHosts\": [],\n               \"GroupAdd\": [],\n               \"IpcMode\": \"shareable\",\n               \"Cgroup\": \"\",\n               \"Cgroups\": \"default\",\n               \"Links\": null,\n               \"OomScoreAdj\": 0,\n               \"PidMode\": \"private\",\n               \"Privileged\": false,\n               \"PublishAllPorts\": false,\n               \"ReadonlyRootfs\": false,\n               \"SecurityOpt\": [],\n               \"Tmpfs\": {},\n               \"UTSMode\": \"private\",\n               \"UsernsMode\": \"\",\n               \"ShmSize\": 65536000,\n               \"Runtime\": \"oci\",\n               \"ConsoleSize\": [\n                    0,\n                    0\n               ],\n               \"Isolation\": \"\",\n               \"CpuShares\": 0,\n               \"Memory\": 0,\n               \"NanoCpus\": 0,\n               \"CgroupParent\": \"\",\n               \"BlkioWeight\": 0,\n               \"BlkioWeightDevice\": null,\n               \"BlkioDeviceReadBps\": null,\n               \"BlkioDeviceWriteBps\": null,\n               \"BlkioDeviceReadIOps\": null,\n               \"BlkioDeviceWriteIOps\": null,\n               \"CpuPeriod\": 0,\n               \"CpuQuota\": 0,\n               \"CpuRealtimePeriod\": 0,\n               \"CpuRealtimeRuntime\": 0,\n               \"CpusetCpus\": \"\",\n               \"CpusetMems\": \"\",\n               \"Devices\": [],\n               \"DiskQuota\": 0,\n               \"KernelMemory\": 0,\n               \"MemoryReservation\": 0,\n               \"MemorySwap\": 0,\n               \"MemorySwappiness\": 0,\n               \"OomKillDisable\": false,\n               \"PidsLimit\": 2048,\n               \"Ulimits\": [\n                    {\n                         \"Name\": \"RLIMIT_NPROC\",\n                         \"Soft\": 4194304,\n                         \"Hard\": 4194304\n                    }\n               ],\n               \"CpuCount\": 0,\n               \"CpuPercent\": 0,\n               \"IOMaximumIOps\": 0,\n               \"IOMaximumBandwidth\": 0,\n               \"CgroupConf\": null\n          }\n     }\n]\n\nI found something, but the output is massive and not easy to read.\nThe relevant part:\n\"Networks\": {\n                    \"podman-docker_my_network\": {\n                         \"EndpointID\": \"\",\n                         \"Gateway\": \"10.89.0.1\",\n                         \"IPAddress\": \"10.89.0.2\",\nWhat about rootless? Using the podman-compose I can launch rootless containers. However, according to the arch wiki some extra dependencies are required for dns to work.\nAfter installaling:\nRootless test (podman-compose):\n/ # ping podman-docker_alpine_container_2_1\nPING podman-docker_alpine_container_2_1 (10.89.1.5): 56 data bytes\n64 bytes from 10.89.1.5: seq=0 ttl=42 time=0.039 ms\nRootfull test (docker-compose with podman backend):\n/ # ping podman-docker-alpine_container_1-1\nPING podman-docker-alpine_container_1-1 (10.89.0.2): 56 data bytes\n64 bytes from 10.89.0.2: seq=0 ttl=42 time=0.044 ms\nSo I can’t get the ip, but because dns in the container works, they can find eachother that way. Nice.\nSince rootful podman containers using docker-compose work nicely, podman seems more and more appealing to me.\nAnother feature of podman that appeals to me is the auto updates\nAnd another feature that makes podman appeal to me is the integration podman has with the cockpit web ui for managing linux systems. Because cockpit is made by Red Hat, who also develops podman, they deprecated docker support and now it only supports podman."
  },
  {
    "objectID": "playground/index.html",
    "href": "playground/index.html",
    "title": "Playground",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 4, 2024\n\n\nCreating a nix flake, the “proper” way\n\n\n \n\n\n\n\nMay 2, 2024\n\n\nContainer images and setuid binaries\n\n\n \n\n\n\n\nMar 11, 2024\n\n\nExperiments with Running python in the browser\n\n\n \n\n\n\n\nFeb 20, 2024\n\n\nAutomatically provisioning VMs from OVAs\n\n\n \n\n\n\n\nJan 6, 2024\n\n\nAnsible: Defined is not Truthy\n\n\n \n\n\n\n\nOct 4, 2023\n\n\nA notes format that I can automatically convert to flashcards?\n\n\n \n\n\n\n\nSep 3, 2023\n\n\nPodman vs Docker\n\n\n \n\n\n\n\nAug 29, 2023\n\n\nAttempting to get secure boot and bootable BTRFS snapshots\n\n\n \n\n\n\n\nAug 28, 2023\n\n\nThe CSUN game room has pretty locked down computers…\n\n\n \n\n\n\n\nJun 21, 2023\n\n\nThe cruise ship I am on has very locked down computers\n\n\n \n\n\n\n\nJun 12, 2023\n\n\nDorking around\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nA very clever crypto scam\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nCan I include text from other files in quarto?\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nCan I write my resume in python?\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-1",
    "href": "playground/flashcards/test2.html#section-1",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "How reflective an object or substance is"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-2",
    "href": "playground/flashcards/test2.html#section-2",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Measured in percentage of electromagnetic radiation reflected"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-4",
    "href": "playground/flashcards/test2.html#section-4",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Dry: 10 (actually 9.8) C° per 1000 meters (1 km).\nWet/Saturated: 6 (actually 5.5) C⁰ per 1000 meters."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-6",
    "href": "playground/flashcards/test2.html#section-6",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "When a gas is compressed without exchaning heat with it’s surroundings. This results in the temperature of the gas increasing (or vice versa)."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-8",
    "href": "playground/flashcards/test2.html#section-8",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Cold air, in the prescense of a warmer water surface. Water evaporates into the colder air, saturated it, and becoming fog."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-10",
    "href": "playground/flashcards/test2.html#section-10",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Warm air from ocean is cooled to dew point"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-12",
    "href": "playground/flashcards/test2.html#section-12",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Moist air rises and cools, reaching the dew point."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-14",
    "href": "playground/flashcards/test2.html#section-14",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Warmer air in the presense of a cold surface. Heat transfers from air to cold surface, and as this air cools, the moisture in it condenses into the"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-16",
    "href": "playground/flashcards/test2.html#section-16",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "The former is dust, salt, or other particles in the atmosphere. The latter is essentially a cloud on the ground."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-18",
    "href": "playground/flashcards/test2.html#section-18",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Small (microscopic) particles in the air. Hygroscopic nuclei attract water, hydrophic avoid it."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-23",
    "href": "playground/flashcards/test2.html#section-23",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Long, thin, and wispy cloud type"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-25",
    "href": "playground/flashcards/test2.html#section-25",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Temperature at which air must be cooled for water vapor to condense."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-27",
    "href": "playground/flashcards/test2.html#section-27",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "When air pressure is affected by the convergence or divergence of air masses, which can be caused by heat, or"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-29",
    "href": "playground/flashcards/test2.html#section-29",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "States that pressure of water vapor and other debris in air + air itself is equal to total pressure."
  },
  {
    "objectID": "playground/flashcards/test2.html#section-31",
    "href": "playground/flashcards/test2.html#section-31",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Boundary between air masses"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-32",
    "href": "playground/flashcards/test2.html#section-32",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Types of Fronts.\n\nWarm\nCold\nOccluded\nStationary"
  },
  {
    "objectID": "playground/flashcards/test2.html#section-33",
    "href": "playground/flashcards/test2.html#section-33",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Types of Fronts:"
  },
  {
    "objectID": "playground/csun-computers/index.html",
    "href": "playground/csun-computers/index.html",
    "title": "The CSUN game room has pretty locked down computers…",
    "section": "",
    "text": "My first class at Cal State Northridge (CSUN) happened today.\nIt was pretty exciting, but after class, I wandered around to stave off boredom. I ended up finding the CSUN game room, which was a pretty sick setup. Although one side was a game room with a few pool tables and other table games, and beyond that, couches and game consoles, the other side was setup similar to the computer cafe’s which are still popular in South Korea.\nObviously I needed to asses their security. Because of my experience with the cruise ship computers, I somewhat knew what I needed to do. The windows UI on the machines in the CSUN game room, is locked down.\nRather than the normal window’s interface, it was a proprietary interface specially dedicated to games. I could not even open a browser.\nIn addition to that, none of the keyboard shortcuts worked. Windows + R (run dialog). Or Windows + E (explorer).\nSo similar to the cruise ships, my initial goal was to restart the windows UI. For some strange reason, the windows UI is located under the program explorer.exe, which is the same program as the filebrowser. On the cruise ship, finding and launching this program is how I restarted the windows UI.\nHowever, I quickly discovered something interesting. When launching discord through the locked down UI, the edge browser would launch for half a second, and then disappear, the tab closing itself. I launched a new tab before it closed, and edge stayed open…\nOnce edge was open, the first thing I tried was chromium’s Control + O open file dialog.\nHowever, I couldn’t open things, and more interestingly, the right click, and any other methods of accessing the context menu I tried, were blocked.\n\nSo I tried to download another file browser. Blocked:\n\nBut interesting, I could click the “open downloads folder” and get the proper windows file browser open. In this file browser, I could simply single click on applications to open them.\nI located explorer.exe, the program which launched the windows UI on the cruise ship computers, and…\n\nFor whatever reason, launching the explorer.exe program simply results in another instance of the filebrowser launching, rather than the windows UI launching. So I decided to try some other stuff.\n\n\nI tried a few things. Powershell, command prompt, registry editor, no dice.\nI went down the list of apps in C:\\Windows, checking things out, when I found something interesting unblocked:\n\nThe strangest thing about being able to launch this application, window’s “Computer Management” app, located at compmgmt.msc is that it requies administrative privileges to simply launch.\nWhen first bootup, the computer autologins to a default user called “User”. Apparently, the default user was an administrator.\nFrom here, I can get to “task scheduler”. Because I am already an administrator for some reason, I am able to schedule tasks, which can be any abitrary thing.\nHowever, this isn’t really a big deal when it comes to access of this machine itself. All these computers at this pc cafe use image based deployments, and I am presuming these images are immutable so any system changes I make would be reset the moment I log out.\nHowever, with administrative access within the network, I could do things like a Denial-of-Service attack, by eating up bandwidth, or I could seek lateral movement through the network, if the machines aren’t isolated properly.\nThere were multiple ways this kind of access could have been prevented. Why do the users logged in need administrative access? All they are doing is using discord and playing games, they don’t need administrative access.\nAnother thing that could have been done is simply preventing me from launching compmgmt.msc, the same way powershell or cmd are prevented.\nAnd finally, they could have used a browser other than edge. The browser on that system is not intended for normal browsing, but rather it’s intended for logging into your online accounts, like Discord or the video game providers. Since that’s the case, why use a fully featured browser that lets you open a file browser? There exist browsers that are far more locked down, and cannot do things like browse files, only open a single webpage — perfect for this usecase."
  },
  {
    "objectID": "playground/cruise-ship-computers/index.html",
    "href": "playground/cruise-ship-computers/index.html",
    "title": "The cruise ship I am on has very locked down computers",
    "section": "",
    "text": "So I finally found the internet cafe on the cruise ship. I didn’t think they had one.\nThe computers look something like this:\n\nThey have a keyboard, mouse and screen, but the rest of the machine was locked away, physically, behind the desk. I couldn’t get to it, and the cabinets were locked.\nThe only reason I know it’s windows is because all devices on this ship, including point of sale devices appear to be using windows. However, there is no taskbar, or windows UI visible, just a chrome browser in an incognito tab session, and a big blue “End Internet Session” button in the top right. When presses, this button completely resets the session.\nSo obviously, I decided to do some snooping around.\nCan I get open task manager from the cntrl alt delete interface? :\n\nNo, I can’t. But I can do other things.\n\nControl + O brings up the file open dialog, which lets me see the immediate user filesystem. Using this, I figured out the currently logged in user. Then, using chrome’s file:// dialog, I decided to do some browsing.\n\nI went through all the directories of the currently logged in user, but I didn’t find anything of note. It seems like the reset of the seession deletes any downloaded files, or something like that.\nHowever, I quickly discovered something more interesting:\n\nI could get to the C:/Windows directory, where the explorer.exe file was. Explorer.exe is the program that launches the windows UI.\nBut when I tried to click on it, chrome just downloaded it instead. But this was promising.\nSo I tried accessing the C drive through the “open file” interface that pops up when you hit Control + O.\n\nHowever, when I tried to access the C drive, I got an access denied. But if I put in the absolute path like so:\n\nThen it worked:\n\nI right clicked on explorer.exe, and selected “open”. And I got the windows UI to launch.\n\nFrom here, I had broken out of the small box that I was locked in. But I decided to snoop around more.\n\nAww, command prompt was disabled. Too bad.\n\nBut powershell wasn’t.\n\nWindows defender was active, but it was complaining about virus definitons being outdated.\nThe settings app wouldn’t launch, it would close instantly. And attempting to launch control panel made erred as well, complaining about being disabled by the administrator.\nSo, I decided to use powershell to do further snooping around:\n\nWow, that is an old version of windows. It looks like they haven’t updated this system from when they first installed it in 2020 (It’s currently 6/24/2023 as I write this).\n\nThe whoami command tells a little bit about users and permissions. Looking at permissions, I notice something. The user I am logged in as is denied all permissions except one, bypass traversal checking. Bypass traversal checking allows users to visit lower level directories they haven’t been explicitly granted permission to go to, even if they are denied to higher level directories.\nIn this case, even though I have been denied access to the C drive, this singular permission enabled me to get into C:/Windows, and launch the explorer.exe program. This singular persmission created a security hole for me to crawl to.\nNow, there are probably ways I can exploit this further. Since these machines are connected to the network, even if they aren’t connected to the internet, I could use an http server on another network connected device to send an payload to be executed on these devices, targeting a vulnerability not patched in this older version of windows. Or, since it is an older version of windows, there may be a easy exploit, something I could simply type into powershell.\nI ended up not going any further. Ironically, I was pretty busy this vacation, between my other personal projects, and all the traveling."
  },
  {
    "objectID": "playground/ansible-truthy/index.html",
    "href": "playground/ansible-truthy/index.html",
    "title": "Ansible: Defined is not Truthy",
    "section": "",
    "text": "I was creating some ansible playbooks recently, and I came across an interesting issue.\nIn my variable definition file, this:\n\n\nvars/main.yml\n\ntest_var: # This is a test var\n\nWhen evaluating whether or not it was defined:\n- name: Print test var\n  ansible.builtin.debug:\n    var: test_var\n- name: Is null defined\n  ansible.builtin.debug:\n    msg: \"test_var is defined\"\n  when: test_var is defined\nThis would actually evaluate to true:\nTASK [Print test var] ************************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"test_var\": null\n}\n\nTASK [Is null defined] ***********************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": \"test_var is defined\"\n}\nSo apparently, null is defined. This is a somewhat weird behavior.\nApparently, it’s only when variables are completly unset, unmentioned in any files, that a variable is not considered defined.\nHowever, even if variables are not defined, then the value of null is still considerd to be falsey\n- name: What about truthyism?\n      ansible.builtin.debug:\n        msg: \"test_var is truthy\"\n      when: test_var\n- name: What about the bool filter\n  ansible.builtin.debug:\n    msg: \"test var passes the bool\"\n  when: test_var | bool\nTASK [What about truthyism?] *****************************************************************************************************************************\nskipping: [localhost]\n\nTASK [What about the bool filter] ************************************************************************************************************************\nskipping: [localhost]\nAnd these tasks are skipped, because null is falsey, and doesn’t satisfy the when statements.\nWhat about an empty string?\nempty_string: \"\"\n- name: What about a length filter?\n  ansible.builtin.debug:\n    msg: \"empty_string passes the length filter\"\n  when: empty_string | length &gt; 0\n- name: Print empty_string\n  ansible.builtin.debug:\n    var: empty_string\n- name: Is empty_string defined\n  ansible.builtin.debug:\n    msg: \"empty_string is defined\"\n  when: empty_string is defined\n- name: What about truthyism?\n  ansible.builtin.debug:\n    msg: \"empty_string is truthy\"\n  when: empty_string\n- name: What about the bool filter\n  ansible.builtin.debug:\n    msg: \"empty_string passes the bool\"\n  when: empty_string | bool\n- name: Empty string truthy in assert?\n  ansible.builtin.assert:\n    that: empty_string\nTASK [What about a length filter?] ***********************************************************************************************************************\nskipping: [localhost]\n\nTASK [Print empty_string] ********************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"empty_string\": \"\"\n}\n\nTASK [Is empty_string defined] ***************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": \"empty_string is defined\"\n}\n\nTASK [What about truthyism?] *****************************************************************************************************************************\nskipping: [localhost]\n\nTASK [What about the bool filter] ************************************************************************************************************************\nskipping: [localhost]\n\nTASK [Empty string truthy in assert?] ********************************************************************************************************************\nfatal: [localhost]: FAILED! =&gt; {\n    \"assertion\": \"empty_string\",\n    \"changed\": false,\n    \"evaluated_to\": false,\n    \"msg\": \"Assertion failed\"\n}\n...ignoring\nSo an empty_string is considered defined, falsey, and unlike a null value, it can also be passed through the length filter, to get falsey.\nWhat about completely unset? Not bothering to mention a variable in any files?\n- name: Is variable truthy\n  ansible.builtin.debug:\n    msg: \"unset is truthy?\"\n  when: unset\n  ignore_errors: true\n- name: Uset variable defined?\n  ansible.builtin.debug:\n    msg: \"unset is defined\"\n  when: unset is defined\nTASK [Empty string truthy in assert?] ********************************************************************************************************************\nfatal: [localhost]: FAILED! =&gt; {\n    \"assertion\": \"empty_string\",\n    \"changed\": false,\n    \"evaluated_to\": false,\n    \"msg\": \"Assertion failed\"\n}\n...ignoring\n\nTASK [Is variable truthy] ********************************************************************************************************************************\nfatal: [localhost]: FAILED! =&gt; {\"msg\": \"The conditional check 'unset' failed. The error was: error while evaluating conditional (unset): 'unset' is undefined. 'unset' is undefined\\n\\nThe error appears to be in '/stuff/playbook.yml': line 53, column 7, but may\\nbe elsewhere in the file depending on the exact syntax problem.\\n\\nThe offending line appears to be:\\n\\n      ignore_errors: true\\n    - name: Is variable truthy\\n      ^ here\\n\"}\n...ignoring\n\nTASK [Uset variable defined?] ****************************************************************************************************************************\nskipping: [localhost]\nSo this is an interesting phenomenon. Trying to check a completely undefined variable for truthyness doesn’t work.\nWhat about an is defined, and a check for truthyness? For my specific usecase, I have an ansible role that generates a variable, that some roles may rely on. If the first role isn’t run or run out of order, things could break.\nI want an ansible.builtin.assert, which essentially checks some conditions, and fails, stopping the playbook if they are not met. How can I check if a variable is defined first, and then truthy? Now, a check for truthyism will still fail, but without the assert catching the error, the error message won’t be as explicit.\nI created a variable called notunset, and set it to “stuff”, and ran some similar tests.\n- name: Notunset variable defined?\n  ansible.builtin.debug:\n    msg: \"unset is defined\"\n  when: notunset is defined\n- name: Notunset variable defined and truthy?\n  ansible.builtin.assert:\n    that: notunset is defined and notunset | bool\n  ignore_errors: true\nTASK [Is notunset variable truthy] ***********************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": \"notunset is truthy?\"\n}\n\nTASK [Notunset variable defined?] ************************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": \"unset is defined\"\n}\n\nTASK [Notunset variable defined and truthy?] *************************************************************************************************************\nfatal: [localhost]: FAILED! =&gt; {\n    \"assertion\": \"notunset is defined and notunset | bool\",\n    \"changed\": false,\n    \"evaluated_to\": false,\n    \"msg\": \"Assertion failed\"\n}\n...ignoring\nHuh. Why does this fail?\nA little modification though, and it succeeds:\n- name: Notunset variable defined and truthy?\n  ansible.builtin.assert:\n    that: notunset is defined and notunset\n  ignore_errors: true\nTASK [Notunset variable defined and truthy?] *************************************************************************************************************\nok: [localhost] =&gt; {\n    \"changed\": false,\n    \"msg\": \"All assertions passed\"\n}\nWhen doing a little more testing, this interacts properly with assert with undefined, null, and empty strings, properly turning them into falsey and truthy values.\nAnother thing to note is the default filter. When a variable is undefined, it will assign it a default value.\nInterestingly, this also considers null to be undefined. When working with the test_var, which defined but never assigned a value (null):\n- name: How about the default filter?\n  ansible.builtin.debug:\n    msg: \"{{ test_var | default('test_var is not defined') }}\"\nTASK [How about the default filter?] *********************************************************************************************************************\nok: [localhost] =&gt; {\n    \"msg\": \"\"\n}\nNull is considered defined again, as shown here.\nwhen: variable | default(False)\nCreates an elegant way to check if a variable exists, and set it to a False otherwise. This is the final solution I’ve settled on, for when I can’t guarantee variables are defined (including null or an empty list/string)"
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html",
    "href": "guides/unrestricted-tethering/index.html",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "",
    "text": "So I was on a plane ride recently, and the plane gave free, unlimited wifi, to one device, as long as you were using a T-Mobile data plan.\nI used my phone for a bit, but then I wanted to use my computer.\nOkay, so I’m writing this later, after I had written the guide. Apparently, the plan for the boat was 4 devices per person, not total. And even though I had wifi, when I tested USB tethering, it just worked. Either my phone does the proxying automatically or something, because it shouldn’t have worked according to reports of people I’ve talked to online.\nBut after looking at some configs on my computer, It does look like my phone creates a virtual network which my computer connects to, and proxies all traffic."
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html#why",
    "href": "guides/unrestricted-tethering/index.html#why",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "",
    "text": "So I was on a plane ride recently, and the plane gave free, unlimited wifi, to one device, as long as you were using a T-Mobile data plan.\nI used my phone for a bit, but then I wanted to use my computer.\nOkay, so I’m writing this later, after I had written the guide. Apparently, the plan for the boat was 4 devices per person, not total. And even though I had wifi, when I tested USB tethering, it just worked. Either my phone does the proxying automatically or something, because it shouldn’t have worked according to reports of people I’ve talked to online.\nBut after looking at some configs on my computer, It does look like my phone creates a virtual network which my computer connects to, and proxies all traffic."
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html#how",
    "href": "guides/unrestricted-tethering/index.html#how",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "How",
    "text": "How\nThere is probably an equivalent set of steps you can take to get this setup using iSH, the iPhone terminal emulator, and iPhone USB tethering, as this process is universal, but this guide is written for Android specifically, as that is what I have.\nFirstly, tether your phone using USB tethering. Simply plug your android device into your computer, and then in the settings, you can usually find the usb tethering option in the tethering and mobile hotspot options, wireless and networks, or usb preferences sections of settings. Test if this works. I should have tested, instead of trying to do everything in this guide first, because the security of United airlines Wifi seems to be less than that of the cell carrier of that one reddit user who said that simply USB tethering didn’t allow them to get around their carrier hotspot data cap for other devices.\nInstall termux. Termux is a terminal emulator for android, which gives access to many linux utilities. Yyou can get it from F-droid, or the Github Releases. The version in the Google Play store is outdated, and not recommended.\nIn termux, first update the system:\npkg update\nThen, install the necessary package. We are going to be using a Socks5 Proxy for this.\npkg install microsocks\nNow run the command ip a You should get an output similar to this\nmoonpie@localhost:~/vscode/moonpiedumplings.github.io&gt; ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: wlan0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether f6:41:0a:25:44:c5 brd ff:ff:ff:ff:ff:ff permaddr 0c:dd:24:ca:bb:f1\n    altname wlo1\n    altname wlp0s20f3\n3: usb0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000\n    link/ether 2a:56:99:ae:d0:6f brd ff:ff:ff:ff:ff:ff\n    altname enp0s20f0u1\n    inet 192.168.67.6/24 brd 192.168.67.255 scope global dynamic noprefixroute usb0\n       valid_lft 3573sec preferred_lft 3573sec\n    inet6 fe80::96de:a3e6:3915:6283/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\nAlthough this is ran from my computer which is currently tethered using this method, so your output won’t be the same. You will lack a usb0 part, and you wlan0 will have an inet value. Note this value, as this is the IP address of your phone on the wifi you are connected to.\nNow, for security purposes, it’s important to ensure that the proxy requires authentication, otherwise anyone can connect to it.\nmicrosocks -u username -p password -1 # This command requires a username and password to authenticate once, then whitelists, the connecting ip, so that it doesn’t have to authenticate.\nTo authenticate the proxy to put your device on the whitelist, you can use any method to connect to a socks5, authenticated proxy, such as curl --socks5 user:password@phoneip:1080 google.com, on your computer or connecting device.\nThen, you can connect to the proxy using other means. For example, firefox has a built in option to connect to socks5 proxies, although it does not support authentication.\n\nWith this, the traffic from your browser will go through your phone, enabling you to get around the one-device restriction.\nAlternatively, on linux, there is usually a proxy options with this setting, to enable you to set a socks5 proxy system wide.\nHere is this option in my KDE system settings.\n\nIf you have no way to authenticate a socks5 proxy, then you can run an unauthenticated proxy. Simply run microsocks, and connect the same way.\n\n\n\n\n\n\nWarning\n\n\n\nThis method works by creating an unauthenticated socks5 proxy server. Although not apparent to the average user, anyone with scanning utilities on the same network can see this server, and connect to it.\nIf they connect, then no private data of yours will be put in jepoardy.\nHowever, if they do connect they will be able to eat up your bandwidth, potentially slowing your connection down, or they could do illegal things and you could be held liable for them, as it is your connection.\nI’m going to see if there is some way to bind this only to the tethered device.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen authenticating to a socks5 server, the passwords are sent over the network in plaintext. Although if you connect over USB tethering, no network should be exposed to the wireless network, if you are connecting directly over the wireless network, then people would be able to sniff your password and connect to your socks5 server.\nI might research if there is a way to limit the connections in advance, but it might not be possible with such a small and lightweight server application.\n\n\nAirplane wifi, and later on when I get to it, boat wifi, are both really slow. Don’t expect much speed from this setup when you are already limited. But I suspect this works great for those who live in areas without internet, but are on cellular data plans that let them created limited hotspots or do limited tethering."
  },
  {
    "objectID": "guides/npm/index.html",
    "href": "guides/npm/index.html",
    "title": "Nginx proxy manager",
    "section": "",
    "text": "What is NPM and why do I want to use it?\nNginx proxy manager (npm, but not the node one) is a web based frontend for nginx that automatically also configures letsencrypt, similar to how certbot does it. It makes nginx much easier to use. Rather than writing config files, you can just click around, which is much easier. For a high school computer science class, I think NPM is better, because it doesn’t have the complexity of npm (less potential for accidental failurs), but still teaches people about ports mapping, encryption, and other necessary skills.\nIn addition to that, with npm, even if someone does create a bad config, only their server goes down. With npm, a bad nginx config leads to the whole server going down. That is… not optimal.\n\n\nInstallation and Setup\nInstallation is simpleish.\nFirst, create a docker network for usage with our docker containers (step from here). Because these are our school projects, I will call that network nighthawks\ndocker network create nighthawks\nCreate a folder called npm, and put a docker-compose.yml in it (basic compose file from here):\nversion: '3'\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      - '80:80'\n      - '81:81'\n      - '443:443'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\ndocker-compose up -d and you’re good to go. It should be noted that you need to have ports 443 and 80 unused by anything else, like Nginx proper. So if you are running nginx, stop it first before you up NPM.\nNPM does need to have port 81 accessible.\nYou can either use a reverse proxy, or open up the port to be accessible from the internet.\nIf you want the port to be accessible from the internet, you might have a firewall of some kind, so just open that. And if you are using one of the big cloud providers (aws, azure, oracle), then you also might have to configure security control groups, as that acts as an extra firewall for those server types. See my cockpit guide on how to do this with AWS.\nIf you want to do a reverse proxy, just use npm to do it: Use proxy post to connect http://npm_app_1:81 to a domain name.\nNow, to configure npm, just access the web interface at https://[domainname/ip]:81\n\n\nUsage\nYou may notice above, in the section about using a reverse to expose npm, I use the docker container name, rather than a port. That’s the amazing part of npm. As long as your docker containers are on the same network, all you need is a hostname and the used port. You don’t even need to expose ports in your docker-compose.yml\nSo rather than the docker-compose.yml we use in our deployment guide\nWe can use:\nversion: '3'\nservices:\n      web:\n              image: flask_port_v1\n              build: .\n              #ports: # ports section not needed\n                     # - \"8086:8080\"\n              volumes:\n                      - persistent_volume:/app/volumes\nvolumes:\npersistent_volume:\n  driver: local\n  driver_opts:\n    o: bind\n    type: none\n    device: /home/ubuntu/flask_portfolio/volumes\n    # replace just flask_portfolio\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\nAnd then you can simply expose http://flask_port_v1_web_1:8080 to the world!"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 6, 2024\n\n\nFree resources to learn various software things\n\n\n \n\n\n\n\nNov 15, 2023\n\n\nCCDC Environment setup\n\n\n \n\n\n\n\nOct 28, 2023\n\n\nCCDC Learning Resources\n\n\n \n\n\n\n\nAug 7, 2023\n\n\nTmux\n\n\n \n\n\n\n\nJun 28, 2023\n\n\nGit\n\n\n \n\n\n\n\nJun 15, 2023\n\n\nUrestricted Wifi/Hotspot as long as you have one unlimited device\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nInstalling the nix package manager and how it’s useful\n\n\n \n\n\n\n\nFeb 28, 2023\n\n\nNginx proxy manager\n\n\n \n\n\n\n\nFeb 14, 2023\n\n\nHow to get a subdomain from duckdns\n\n\n \n\n\n\n\nSep 30, 2022\n\n\nSetting up cockpit\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/duckdns/index.html",
    "href": "guides/duckdns/index.html",
    "title": "How to get a subdomain from duckdns",
    "section": "",
    "text": "Why?\nPeople can’t register for freenom consistently, and it can take time to get a domain from the one’s we have as a class. Duckdns allows people to create their own free domain, extremely easily, and nearly instantly.\n\n\nRegistration and Setup\n\nThis is the registration page. I really like duckdns because you only need a github account to login, which we already have.\n\nAfter you login, you will see this. You can get your own subdomain, and then set your ip address manually.\nTo find the ip address of your server, run this command on your AWS server:\ncurl ifconfig.me\nIf the command curl is not found, install it using apt.\nThen, you can manually input your server’s ip address into the duckdns website.\nIn your nginx confiuration file, make sure you set your nginx configuration file to be your domain name.\nserver {\n    listen 80;\n    listen [::]:80;\n    server_name [yoursubdomain].duckdns.org;\n\n    location / {\n        proxy_pass http://localhost:8087; # Make sure this matches the port your docker-compose.yml is set to\n        # Simple requests\n        if ($request_method ~* \"(GET|POST)\") {\n                add_header \"Access-Control-Allow-Origin\"  *;\n        }\n\n        # Preflight requests\n        if ($request_method = OPTIONS ) {\n                add_header \"Access-Control-Allow-Origin\"  *;\n                add_header \"Access-Control-Allow-Methods\" \"GET, POST, OPTIONS, HEAD\";\n                add_header \"Access-Control-Allow-Headers\" \"Authorization, Origin, X-Requested-With, Content-Type, Accept\";\n                return 200;\n        }\n    }\n}"
  },
  {
    "objectID": "guides/ccdc-learning/index.html",
    "href": "guides/ccdc-learning/index.html",
    "title": "CCDC Learning Resources",
    "section": "",
    "text": "This document was designed for our CSUN CCDC team, but since it’s useful, I am putting it on my personal blog as well.\nPrerequisites:"
  },
  {
    "objectID": "guides/ccdc-learning/index.html#guided",
    "href": "guides/ccdc-learning/index.html#guided",
    "title": "CCDC Learning Resources",
    "section": "Guided",
    "text": "Guided\nSome resources to get started on your linux journey.\nhttps://linuxjourney.com/\nLinuxjourney is a very good guide, but it has some stuff not relevant to what we are doing.\nYou don’t need to networking nomad, but you do need to do grasshopper and journeyman.\nOut of the stuff on those two programs, skip:\n\nText-fu\nText-fu advanced\n\nWe will probably either be using nano or automation/scripting to edit files.\n\nBoot processes\nFrom init, skip anything not relevant to systemd. No one uses upstart or systemv anymore."
  },
  {
    "objectID": "guides/ccdc-learning/index.html#active-directory",
    "href": "guides/ccdc-learning/index.html#active-directory",
    "title": "CCDC Learning Resources",
    "section": "Active Directory",
    "text": "Active Directory\nA cat explains active directory\nHigh level overview, covering some of the history and many of the concepts behind active directory.\nA cat explains kerberos\nI have not watched yet, but based on the above video, which mentions how active directory includes kerberos, it is probably relevant.\nMicrosoft’s Intro to AD\nGOAD - Game of Active Direcotyr\nThis is a highly insecure active directory which can be auto deployed to hyperv, proxmox, virtualbox, or azure. It uses a mix of ansible, terraform, and packer to provision and deploy."
  },
  {
    "objectID": "blog/twitch/index.html",
    "href": "blog/twitch/index.html",
    "title": "Linux and Programming Related Twitch Streamers",
    "section": "",
    "text": "I like to watch people program on twitch. Seeing people suffer with code reminds me that I am not alone.\nI may be missing a few, so I will add them onto this list as I find them.\nIn addition to that, the notes on their content and OS are as of when I last watched them, so they may be inaccurate. If people stop being programmers, then I will take them offline.\nIn no particular order, here the ones I’ve seen around:\n\n\n\nStreamer\nContent\nLinux User?\nMisc Notes\n\n\n\n\nhet_tanis\nTeaches linux, k8s devops, and ansible related things on stream.\nWindows as desktop, but uses xenserver (kinda unorthodox choice IMO) as a VM host for linux machines of every type\nKillercoda courses can be found here\n\n\ncidermcdread\nJust Chatting, Baldur’s Gate, other various games.\nNixos\nAlso discusses philosophical and political topics in a very engaging manner\n\n\nchrissiecodes\nPython development, often for stream related features\nUbuntu in WSL\nVery active and knowledgable chat which is fun to be in.\n\n\ntrshpuppy\npython, javascript, and CTF’s/hacking\nWindows, but Kali in a VM\n\n\n\nhedge_in\nLow level operating system and machine learning\nNo\nOften is studying these topics on stream for class\n\n\npwncollege\nVarious college clacsses in the cybersecurity curriculum of Arizona State University\nVarious teachers some of whom probably use linux, but the format the classes are in does not let me see or ask\n\n\n\ncyberkaida\nLow level operating programming, python. Reverse engineering\nMacos, but does work for linux like on the rasperry pi\n\n\n\nlcolonq\nRust programming, or whatever language emacs uses.\nNixos\nUses emacs window manager as his main wm.\nWrote his own vtuber rendering engine, which renders to ascii art in the terminal.\n\n\nbadcop_\nGamejams, showing off content and programming for. Stream related features.\nWSL\n\n\n\ntheyagich\nGamedev in Godot\nManjaro\n\n\n\ncybersteffie\nSeems to be doing IRL more often recently, but has done programming work on stream\nWSL\n\n\n\nambivalentbunnie\nOften studies for high level machine learning and data science classes on stream\nNo\n\n\n\nExpiredPopsicle\nGamdev/vtuber dev in godot. Also plays modded doom.\nDebian Stable\n\n\n\nrw_grim\nWork on open source source multi chat client pidgin\nDebian Unstable, sometimes experimental\n\n\n\nhexadigital\nVarious games\nManjaro on Desktop, Debian on server\nDoes not talk, but does chat\n\n\nquantumapprentice\nvarious programming projects\nAlternates between Linux Mint and Windows\n\n\n\nTheAltF4Stream\nCloud, Devops, Kubernetes\nMac on desktop\n\n\n\nMelkey\nWeb/Fullstack. ML/AI\nUbuntu I think, but I will have to confirm\n\n\n\nenlynn_\nVarious programming\nEndeavor OS\n\n\n\npurplelf\nWorks at Gumroad, where she does Ruby and React, on stream\nNobara\n\n\n\nanthonywritescode\nPython developer\nCurrent Ubuntu LTS in a virtual machine\n\n\n\nelvensourcerer\nDevelops QT in python\nNixos\n\n\n\nlastmiles\nVarious things in Linux and FreeBSD. Setting up Cuda in a Bhyve VM running linux when I was last there.\nFreeBSD 15 (as of 1/18/2024).\n\n\n\nshrutivtuber\nLearning rust on stream\nArch Linux\n\n\n\nkodder\nVarious devops and system admin things\nRegolith on Pop_OS! on desktop, Proxmox on the one server I saw\nSaid he was gonna stream once a week on sunday\n\n\n0xfaraday\nGo and python programming for cybersecurity related tools\nKali Linux in a VM, I’m assuming windows on host\nHas participated as a blue teamer in CCDC\n\n\nZabbly (YT)\nDevelops Incus on stream\nEither Ubuntu or Debian, they’re never live when I have time to ask\n\n\n\nAsahi Lina (YT)\nDevelops various things on stream, including Asahi Linux\nAsahi Linux\n\n\n\nategondev\nGames jams and gamedev\nWindows\nModerates and develops for the programming.dev lemmy instance."
  },
  {
    "objectID": "blog/more-security/index.html",
    "href": "blog/more-security/index.html",
    "title": "Increasing my security",
    "section": "",
    "text": "Previously, I’ve been very lax with security. But as I go into the world, and will be leaving my devices unattended, and as what is on my computers becomes more valuable than game site logins, I now need to put actual effort into securing my devices."
  },
  {
    "objectID": "blog/more-security/index.html#cleaning",
    "href": "blog/more-security/index.html#cleaning",
    "title": "Increasing my security",
    "section": "Cleaning",
    "text": "Cleaning\nI needed to clean the data off of the USB’s. However due to USB’s unique method of data storage, simply deleting the data isn’t enough. I decided to overwrite the data once for a more secure erase. However, it wasn’t practical to overwrite whole files, so I have to find the sensitive data, so that I can delete it. This is much faster than overwriting everything.\nfind . -type f -regex '.*mozilla.*\\.\\(json\\|db\\|sqlite\\)$'\nThis uses a regex to find anything ending in .json, .db, or .sqlite, that is also has a parent or grandfather directory with the name mozilla in it.\nfind . -type f -regex ''.*mozilla.*\\.\\(json\\|db\\|sqlite\\)$' -exec &lt;securedeletecommand&gt; {} \\;\nWith this, I can run my secure delete command on my USB’s. I think I am going to use srm a command line tool that is compatible with the rm file remover standard util.\nsrm -rfsv is the command I will use.\nfind . -type f -regex '.*mozilla.*\\.\\(json\\|db\\|sqlite\\)$' -exec srm -rfsv {} \\;\nAnd this will clean my external storage quite nicely, while not touching other databases or json files that don’t have sensitive data."
  },
  {
    "objectID": "blog/more-security/index.html#virtual-fido-device",
    "href": "blog/more-security/index.html#virtual-fido-device",
    "title": "Increasing my security",
    "section": "Virtual FIDO Device",
    "text": "Virtual FIDO Device\nI may also consider a virtual Fido2 device, to emulate a yubikey, if I don’t really want to keep them plugged in. Here are a few projects I found:\nhttps://github.com/bulwarkid/virtual-fido\nhttps://github.com/psanford/tpm-fido\nhttps://github.com/danstiner/rust-u2f"
  },
  {
    "objectID": "blog/helping/index.html",
    "href": "blog/helping/index.html",
    "title": "Being an internet helper",
    "section": "",
    "text": "The below is my “literacy narrative essay”, for an english class\nI was so close to hacking this system. Using a USB thumb drive as a boot device, I booted a different operating system on the computer, which originally ran windows. I then used some digital tools to enable the local administrator account, in a passwordless mode. When I removed the USB thumb drive, and restarted the machine, it returned to the innocuous login prompt, asking me to sign in with my school credentials. I ignored that, and signed into the admin account instead. I’m in.\nIt was a different kind of puzzle. But just like any other puzzle, I solved it for the gratification, rather than there being some end goal. Eventually, the computer’s were reset, and all my hard work was lost.\nLater, when my computer was becoming too slow, I felt an itch in the back of my mind again. A new puzzle.\nHow do I make my computer faster?\nI was tinkering already, and I had some ideas of how to make it faster, but I needed reassurance. Unfortunately, not all puzzles can be solved alone. I ended up asking for help on the internet.\nAlthough 4 years ago, and the post has since been deleted, although not by me, I still remember what I asked for. I wanted to know how to replace the lowest level of software on my computer, the operating system, going from the slower Windows that is available on most devices by default, to the more difficult to use, but faster, Linux. However, I did not want to break the existing Windows install, and I also wanted to have “save points” of my linux installation, so that I could tinker and then restore my system to a previous state, giving my system some resilience.\nThe users were patient, and kind, and although they didn’t hold my hand for the entire process, they pointed me in the right directions, teaching me terminology and pointing me to resources that I could use to help myself. Eventually, I managed to solve that puzzle, and some variations of the changes I made, I do on all computers I own today.\nAfter I ran out of puzzles to solve, I began solving other people’s puzzles. I would watch the forums religiously, viewing the newest posts first, so I could contribute before someone else. Sometimes, things were easy, and it was simply a case of a newer user not knowing what to search for, and I could point them in the right direction. For example, people would often ask if their hardware is compatible with Linux, so I would point them to a site that extensively documents hardware compatibility. With every reply thanking me, I felt immense gratification.\nOf course, things wouldn’t always be as simple as that. Often, users weren’t simply seeking to do something, but to understand something as well. In this case where existing resources weren’t enough, I had to explain step by step.\nOne user was curious about how Linux software distribution works. What makes it different from Windows, or Mac, they asked? As painful as it was to type from my phone, I presented the unbiased reality of how Linux software management is objectively superior. Because some things, you have to explain personally, and you can’t point people to external resources.\nVery often, new users don’t know how to ask for help, and you have to request for more information from them. For some reason, people often have trouble running games on Linux, but they won’t mention what game they are trying to play? I don’t understand how people understand how to get help if they don’t give enough information, but asking for help isn’t really something that comes intuitively to people.\nOf course, once I know what game they play, I can give more detailed help. One time, there was a user who had nothing but a chromebook, but desired to get more performance out of it so that they could play one of my favorite games. Over the course of several days via purely discord text chats, I helped this person with installing Linux and installing the game on an unsupported device and operating system. But despite all that, their efforts didn’t bear much fruit, and they ended up giving up.\nAlthough frustrating to see someone give up, it happens sometimes. On the other end, I’ve had to give up on cases as well. In one online community I followed, there was a constant influx of users asking similar or even the same questions. Although many people, including me, first started out by answering them, eventually the community got tired and sometimes even hostile to such users.\nOn the internet, “Help Vampires” are one type of person that appears in forums. This term refers to people who are unwilling to do the bare minimum of research, or put any effort into any problem. An infrequent occurrence on the internet, It’s difficult to help someone if they don’t help themselves. In many cases, it’s impossible.\nA similar thing happened when a user was attempting to play a video game on Linux, and got an error about the kernel (the lowest lowest level of software, part of, but also under the operating system) not supporting a feature. Because of this, they were attempting to install a new kernel. However, what I found strange was the error message was requesting a feature that normally is not needed. The actual issue this user was encountering was that they enabled a non-necessary feature in their game launcher.\nThis is a common occurrence, common enough that there is actually a term for it: the XY problem where a user attempts to get help with their attempted solution, rather than their actual problem. Rather than explaining the whole XY problem, I would usually refer people to this website: https://xyproblem.info/.\nIn another post, a user asked if anyone has experience with the integration of two pieces of technology, one being a VPN, and the other being a management platform. They gave no other information. What this user was actually asking was: “Is there anybody around with experience with these techs that is willing to look into my problem, including extracting what my actual problem is me because I can’t be bothered to explain it?”\nThis is another occurrence on the internet that happens frequently enough that there is a term to refer to it, and a website which I refer people to: https://dontasktoask.com. Don’t ask your question, simply ask it to save everybody’s time.\nAs long as you have technical literacy, giving help to people troubleshooting on the internet is easy. Doing it nearly daily, for 4 years, without being burned out, is not. Many user’s helped out for a short period of time, to give back to the community, before becoming burned out and stopping. There were very few constants, people you could recognize just by their username, because they were that consistent.\nI was one of them. Linux gave me a lot. Rather than gaming, I was tinkering. Working with Linux is likely what my job will be. But I didn’t, and still don’t, have the particular skill set to contribute to the Linux community through programming. I do intend to learn, but at the same time, I’ve come to love being a “helper” on the internet. The pleasure I get is threefold, one part for solving a problem, another for helping a stranger, and yet another for giving to this community that was willing to invest in me, back when I was barely a teen and didn’t know much."
  },
  {
    "objectID": "blog/dorm/index.html",
    "href": "blog/dorm/index.html",
    "title": "Getting situated in my dorms",
    "section": "",
    "text": "For my college, I am going to California State University, Northridge (CSUN). In addition to bringing along normal supplies, I also brought my homelab along.\n\nOn the right, is a desktop comuputer, which would be the server I am using for my homelab things.\nOn top of that, is my router, which I have configured to wireguard vpn into a VPS I am renting. In addition to that, the router will connect via ethernet to campus internet, and then give me internet, which I have found to be suprisingly faster than accessing campus wifi via internet.\nTo the left, is my laptop, which I have “docked” there. It connects up to my monitor and keyboard:\n\nHowever, the ethernet isn’t currently activated. Apparently, you have to submit a maintenence request for them to turn it on. And since the maintenence workers don’t work on weekends, I have to wait until monday.\nOne thing that would really suck is if it wasn’t just the wifi that used eduroam to authenticate, but also the ethernet.\nEduroam wifi requires special tools to authenticate, and it’s possible that my router may not have these tools installed.\nI visited https://test-ipv6.com in order to test ipv6 networking:\n\nI did some futher testing. Both my phone, my laptop, and my other laptop had different public ip addresses. Except they weren’t truly public:\n[moonpie@cachyos-x8664 moonpiedumplings.github.io]$ nmap -sV 130.166.192.250\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-08-27 11:10 PDT\nNote: Host seems down. If it is really up, but blocking our ping probes, try -Pn\nNmap done: 1 IP address (0 hosts up) scanned in 3.07 seconds\n[moonpie@cachyos-x8664 moonpiedumplings.github.io]$ nmap -Pn 130.166.192.250\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-08-27 11:10 PDT\nsits here forever\nIt seems that, despite the machines having public ip addresses, there is a a firewall in place to prevent any access from the external world. So it isn’t truly a “public” ip address.\nThe output of ip a agrees with this.\n[moonpie@cachyos-x8664 ~]$ ip a\n... extraneous stuff omitted for brevity\n...\n3: wlan0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000\n    link/ether 4c:d5:77:2d:b2:6d brd ff:ff:ff:ff:ff:ff\n    inet 10.40.66.133/20 brd 10.40.79.255 scope global dynamic noprefixroute wlan0\n       valid_lft 26229sec preferred_lft 26229sec\n    inet6 fe80::e25c:14be:fd65:8944/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\nSo despite having private ip addresses, each and every ip address maps to a public ip address.\nApparently the whole of 130.166.0.0/16 is owned by CSUN.\nThis is honestly a tragedy. Entire countries don’t have ipv4, only v6 because the US hoards v4 addresses, like in this case, what could be one or a few NAT’s is instead an ip address for every device.\nAs for speed:\n\nPretty good, but it’s 11 AM when I did this. That’s more than double the speeds I was seeing last night, something around 110 down and 20 up. I may rerun the test again at a more congested time just to make sure.\nOverall, the setup is niceish, but not having ethernet working immediately is frustrating.\nAlright, a week later I got ethernet working. My router is set up and when I test the speeds on my router internet:\n\nVs the campus wifi:\n\nI’m seeing around double the speeds, as opposed to normal campus wifi."
  },
  {
    "objectID": "blog/blocking/index.html",
    "href": "blog/blocking/index.html",
    "title": "Why schools should be less aggressive with content blocking",
    "section": "",
    "text": "I love experimenting with, and managing internet servers. Over the years, I have learned real world skills from my personal projects, as shown by the role I play my the computer science classes here at del norte. I do things, like create guides or fix broken one’s.\nBut when I try to access my server on school wifi, I get met with the below image.\n\nNot managed. My server is not blocked because it contains dangerous or harmful content, it’s blocked because my school’s (and so many others) uses a whitelist system. A whitelist means that everything is blocked by default, and things must explicitly be allowed through the firewall.\nNow I have a way around this. I can use a VPN, or similar software to get around the content filtering restrictions In addition to that, I can simply work on my home wifi, where there are no such restrictions. For me, getting access to my project based learning environments are easy.\nBut for students, who only use chromebooks, they cannot get around these restrictions. Chromebooks, the managed devices distributed by our school and so many others, are completely locked down. The chromebook itself implements the content blocking software, rather than the WiFi.\nThis means that, if a student were working at home on a school chromebook that was their only device, they would be denied resources that I have access to, purely because I have my own laptop. Information, research material, learning material, troubleshooting guides, and things like my digital playgrounds are locked away from those students.\nIt gets worse. Because the restrictions on the chromebooks prevent them from installing software, they cannot install the necessary tools needed for our computer science classes here at Del Norte. And unlike so many other computer based courses at Poway USD, the CompSci class does not provide it’s own computers for use. I personally know someone who wanted to take computer science, but was forced to drop out because they could not obtain their own device.\nObviously, this is unfair. The point of chromebooks is to offer students who are unable to obtain their own device for whatever reason the ability to have a computer to work on so they can be on equal footing with those who can afford their own devices. But what’s the purpose if they don’t actually offer students the ability to participate in the same activities and classes that students with more resources — wealth — have?\nChromebooks should be, at the very least, usable for all classes. But the only just thing is that students with only school provided chromebooks are given access to the exact same resources as those who can afford to buy their own device.\nThe reason why schools can and do provide chromebooks and other internet services is that they are cheap — Cheaper than they are normally. In America, there exists a program called e-rate, which is designed to make technology more accessible to public institutions, by providing discounts or potentially free products and services. The government subsidizes the cost.\nSadly, this program does not seem to cover computers (see section about internet access), which is probably why schools opt for chromeboks, the cheapest, lowest end device. Although it should be noted, that Google and Apple do have their own deals for offering discounts for bulk purchases for schools and other public institutions.\nAs great as e-rate is (it’s why the WiFi on school and college campus is so fast, if you know how to utilize it), it comes with caveats and conditions.\nThe same way the federal government holds funding for certain services over the head of state and local governments, the federal government holds the eligibility for e-rate over the head of schools and libraries. They must meet a certain requirements. One of those conditions is following CIPA, the Children’s Internet Protection Act.\nIn the beginning of this article, I linked an image of what our school’s block screen looks like. Quoting below:\n\nThe site you have requested has been blocked because it does not comply with the filtering requirements as described by the Children’s Internet Protection Act (CIPA).\n\nSo, when you click on CIPA, it takes you to the law itself. The law is very short, very vague, and the really important parts can fit comfortably into this article:\n\nSchools must implement a policy addressing these things. It does not say exactly what policies they must implement, or does it define materials harmful to minors, or even define “hacking”.\nBecause of the way the law is vaguely worded, schools are obligated to implement as aggressively as a content blocking policy as they can, including the complete prevention of installing software on chromebooks. Because the truth of the matter is, if someone can install VSCode (Programming application we use at Del Norte), then they can potentially install censorship circumvention software.\nIt’s very clear what is happening here. The school cannot meet both obligations, one to prevent students from accessing resources, and another to make resources accessible to students. Becuase not attempting to block everything they can leads to losses on the e-rate discounts, Del Norte breaks it’s promise to it’s students to provide resources to them. Because to this institution, and many more, money is more important than the education of their students.\nNow, it’s not like I am personally not doing anything about this. In this blog, I have another post, about creating a system that will let students access. In addition to simply setting this system up, I am optimizing it, to make it cheaper and more accessible to students, especially those with less resources.\nOf course, as amazing as what I am deploying is, allowing students to get access to a fully featured linux desktop from their browser, it is also flawed — it also acts as a censorship circumvention software. Inside the system they have access to, users are given unfiltered content. Because of this, there is a possibility that schools would be obligated to block this as well.\nUltimately, my software is cool, equipping at least our computer science class the ability to be done on nothing but chromebooks, it is but a band-aid for the real problem — the laws that force schools further limit access to digital resources for students who already are lacking resources.\nIt does them little good. Off the top of my head, I know of several different ways to get around these content blocker, even on chromebooks. These blocking measures certainly do have an effect for general purpose use, but against a dedicated student, they are ultimately ineffective.\nFor some context, CIPA went into effect in 2001. The first iPhone came out in 2005. The law’s intentions are nice, but it pretty clearly wasn’t created with the foreknowledge that every student (who could afford one) would have a device in their pocket capable of circumventing pretty much all of the content blocking restrictions.\nBecause of this, despite the law being written to “protect” everybody, it only actually affects one group of people. Those who can only rely on school provided devices.\nThis is a form of institutional oppression of those who are in a lower socioeconomic class. Even though chromebooks are easily capable of installing the necessary software for computer science courses, laws in place force administrators to prevent students from doing so.\nThis is unjust. The restrictions on these laws should be lessened to enable students who already have a lesser access to such resources an equal access. If parents can be trusted to monitor their kid’s internet usage on a personal device, like a phone or macbook, why can’t they be trusted to do the same with a school chromebook?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "writeups/nice-challenge-3/index.html",
    "href": "writeups/nice-challenge-3/index.html",
    "title": "Nice Challenge 3",
    "section": "",
    "text": "This is for the NICE challenge. This challenge was significantly harder than the previous challenges, involving being attacked in realtime, and having to write up incident response reports — and if you were too slow, you didn’t get the points.\nThis challenge involves doing an incident response, after an active cyberattack.\nAfter some investigation, I find out that they have snort set up on their pfsense firewall:\nThis is my first time using snort, so everything below was figured out by web searches and browsing around the snort interface.\nHowever, it did not have any rules enabled by default. I checked a few boxes, and then clicked save to download the rules. After, that, I had to go to Snort &gt; Updates, and explicitly tell it to update the ruleset.\nIn addition to that, snort did not have any interfaces added. I added all interfaces I thought would be relevant, WAN, PROD, SUBSCRN, and USRCRN, and started them:\n\nOn the machines Joomia (Debian 9), which hosts the production server, and Fileshare (Ubuntu 16.04.3), I installed apache2 modsecurity, which is a simple web application firewall.\nsudo apt install libapache2-modsecurity\nsudo cp /etc/modsecurity/modsecurity.conf-recommended /etc/modsecurity.conf\nsudo systemctl restart apache2\n\nThe Attacks\n\nI made a mistake, and did not look at the options for packet capturing. I realized after the fact that pfsense/snort does not capture the relevant packets to inspect them by default.\nThat traffic probably isn’t malicious, given how I did not receive any checks or failures for the 2 reports I submitted, but rather, only for the later traffic.\n\n\nNmap scan\nOn the other hand, there was a nmap scan, from 172.31.2.12 at the Prod-Joomia (172.16.10.100)\n\n\nShow Log\n\n[Wed Feb 14 02:30:07.709171 2024] [:error] [pid 1507] [client 172.31.2.12:34403] [client 172.31.2.12] ModSecurity: Warning. Matched phrase \"nmap scripting engine\" at REQUEST_HEADERS:User-Agent. [file \"/usr/share/modsecurity-crs/rules/REQUEST-913-SCANNER-DETECTION.conf\"] [line \"59\"] [id \"913100\"] [rev \"2\"] [msg \"Found User-Agent associated with security scanner\"] [data \"Matched Data: nmap scripting engine found within REQUEST_HEADERS:User-Agent: Mozilla/5.0 (compatible; Nmap Scripting Engine; http://nmap.org/book/nse.html)\"] [severity \"CRITICAL\"] [ver \"OWASP_CRS/3.0.0\"] [maturity \"9\"] [accuracy \"9\"] [tag \"application-multi\"] [tag \"language-multi\"] [tag \"platform-multi\"] [tag \"attack-reputation-scanner\"] [tag \"OWASP_CRS/AUTOMATION/SECURITY_SCANNER\"] [tag \"WASCTC/WASC-21\"] [tag \"OWASP_TOP_10/A7\"] [tag \"PCI/6.5.10\"] [hostname \"172.31.2.2\"] [uri \"/\"] [unique_id \"Zcwlr38AAQEAAAXjvToAAAAA\"]\n\nHowever, I was too late in reporting this one, and this check was reported as late.\n\n\nSSHD brute force\n● ssh.service - OpenBSD Secure Shell server\n   Loaded: loaded (/lib/systemd/system/ssh.service; enabled; vendor preset: enabled)\n   Active: active (running) since Wed 2024-02-14 01:39:16 UTC; 1h 28min ago\n Main PID: 551 (sshd)\n    Tasks: 1 (limit: 4915)\n   CGroup: /system.slice/ssh.service\n           └─551 /usr/sbin/sshd -D\n\nFeb 14 02:49:43 Prod-Joomla sshd[1792]: Failed password for jcortes from 172.31.2.12 port 38181 ssh2\nFeb 14 02:49:43 Prod-Joomla sshd[1792]: Connection closed by 172.31.2.12 port 38181 [preauth]\nFeb 14 02:49:43 Prod-Joomla sshd[1794]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=172.31.2.12  user=jcortes\nFeb 14 02:49:44 Prod-Joomla sshd[1794]: Failed password for jcortes from 172.31.2.12 port 39276 ssh2\nFeb 14 02:49:44 Prod-Joomla sshd[1794]: Connection closed by 172.31.2.12 port 39276 [preauth]\nFeb 14 02:49:45 Prod-Joomla sshd[1796]: pam_unix(sshd:auth): authentication failure; logname= uid=0 euid=0 tty=ssh ruser= rhost=172.31.2.12  user=jcortes\nFeb 14 02:49:47 Prod-Joomla sshd[1796]: Failed password for jcortes from 172.31.2.12 port 40027 ssh2\nFeb 14 02:49:47 Prod-Joomla sshd[1796]: Connection closed by 172.31.2.12 port 40027 [preauth]\nFeb 14 02:49:47 Prod-Joomla sshd[1798]: Accepted password for jcortes from 172.31.2.12 port 46478 ssh2\nFeb 14 02:49:47 Prod-Joomla sshd[1798]: pam_unix(sshd:session): session opened for user jcortes by (uid=0)\nAnd these were only the recent logs. Going through journalctl -xeu ssh, there were a LOT more attempts.\nI got lucky though, as the attacker didn’t stay in for long:\n\nIt seems that snort detected and blocked 172.31.2.12 (the default config also kills existing connections), detecting the brute force. In the future, I need to spend less time trying to figure out snort, and more time setting up fail2ban to avoid ssh brute forces.\n\n\nAttack 3 — Out of time\nI ran out of time:\n\nI couldn’t figure out what it was, either."
  },
  {
    "objectID": "talks/self-site/index.html",
    "href": "talks/self-site/index.html",
    "title": "Your very own site on GH pages",
    "section": "",
    "text": "For the Layer8 Cybersecurity club here at Cal State Northridge, and potentially more clubs, I will do a presentation on how I set up my site with Github pages, and how to set up your own site the same way.\nThis guide uses Github Actions to autodeploy, so all users need to do is commit from the web interface of Github, their site will automatically render changes.\nSlides are linked here\nVideo is recorded, and I will have it clipped and uploaded soon™."
  },
  {
    "objectID": "playground/setuid-containers/index.html",
    "href": "playground/setuid-containers/index.html",
    "title": "Container images and setuid binaries",
    "section": "",
    "text": "A friend sends me a message in Discord:\n$ find / -perm /4000\n\nThat will show all suid executables on your system\n\nThey then post the output of this command on openbsd:\n/usr/bin/chfn\n/usr/bin/chpass\n/usr/bin/chsh\n/usr/bin/doas\n/usr/bin/lpr\n/usr/bin/lprm\n/usr/bin/passwd\n/usr/bin/su\n/usr/libexec/lockspool\n/usr/libexec/ssh-keysign\n/usr/sbin/authpf\n/usr/sbin/authpf-noip\n/usr/sbin/pppd\n/usr/sbin/traceroute\n/usr/sbin/traceroute6\n/sbin/ping\n/sbin/ping6\n/sbin/shutdown\nAnother friend tries it out on Gentoo Linux\n/usr/bin/chfn\n/usr/bin/chpass\n/usr/bin/chsh\n/usr/bin/doas\n/usr/bin/lpr\n/usr/bin/lprm\n/usr/bin/passwd\n/usr/bin/su\n/usr/libexec/lockspool\n/usr/libexec/ssh-keysign\n/usr/sbin/authpf\n/usr/sbin/authpf-noip\n/usr/sbin/pppd\n/usr/sbin/traceroute\n/usr/sbin/traceroute6\n/sbin/ping\n/sbin/ping6\n/sbin/shutdown\nI suppose I should explain what setuid binaries are. Setuid binaries run with the permissions of the user who owns the binary. Usually, it’s used for things like sudo, which use setuid to elevate the binary to root first, and then the rootful binary then does the things, like grant permissions.\nSetuid binaries are typically considered a security risk, because an exploit like a buffer overflow or some other exploit can be disasterous in a process running as root. In fact, that’s how many privilege escalation vulnerabilities on Linux have happened — exploiting a setuid binary owned by root.\nI then run the command on my own system (Arch Linux)\n[moonpie@lizard ~]$ find / -perm /4000 2&gt;/dev/null\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/chfn\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/chsh\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/gpasswd\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/mount\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/newgrp\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/passwd\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/su\n/home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/umount\n/home/moonpie/.local/share/containers/storage/overlay/b2a22dd93936f487715bbc38b3a93f3f8e7d927fbf473871581c0a333f94d23a/diff/usr/lib/dbus-1.0/dbus-daemon-launch-helper\n/home/moonpie/.local/share/containers/storage/overlay/af21757bc7d5f497f4ce0552dbad07cf0725413c3a305e1ff2c8a7b5097eeb49/diff/usr/lib/dbus-1.0/dbus-daemon-launch-helper\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/chfn\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/chsh\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/gpasswd\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/mount\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/newgrp\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/passwd\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/su\n/home/moonpie/.local/share/containers/storage/overlay/ae134c61b154341a1dd932bd88cb44e805837508284e5d60ead8e94519eb339f/diff/usr/bin/umount\n/home/moonpie/.local/share/containers/storage/overlay/fd77695eba64b4eb5db10dd9ef0181d0053dbc23e6c465f3001d664f19e621d7/diff/usr/lib/openssh/ssh-keysign\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/chfn\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/chsh\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/gpasswd\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/mount\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/newgrp\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/passwd\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/su\n/home/moonpie/.local/share/containers/storage/overlay/ede391454aeab91f6777dd38e55e085975ffcfd298987b8ec685196f2a6c811a/diff/usr/bin/umount\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/chage\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/gpasswd\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/mount\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/newgrp\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/passwd\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/su\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/bin/umount\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/sbin/pam_timestamp_check\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/sbin/unix_chkpwd\n/home/moonpie/.local/share/containers/storage/overlay/c4bc4a1387e82c199a05c950a61d31aba8e1481a94c63196b82e25ac8367e5d1/diff/usr/sbin/userhelper\n/home/moonpie/.local/share/containers/storage/overlay/9b1016e74f7eb4282d4aa84ecefedda2bc0f6625203e5085e070bd649945a965/diff/usr/bin/fusermount3\n/usr/bin/fusermount\n/usr/bin/mount.cifs\n/usr/bin/fusermount3\n/usr/bin/mount.nfs\n/usr/bin/ksu\n/usr/bin/sudo\n/usr/bin/pkexec\n/usr/bin/chage\n/usr/bin/expiry\n/usr/bin/gpasswd\n/usr/bin/passwd\n/usr/bin/sg\n/usr/bin/unix_chkpwd\n/usr/bin/crontab\n/usr/bin/vmware-authd\n/usr/bin/chfn\n/usr/bin/chsh\n/usr/bin/mount\n/usr/bin/newgrp\n/usr/bin/su\n/usr/bin/umount\n/usr/lib/dbus-1.0/dbus-daemon-launch-helper\n/usr/lib/polkit-1/polkit-agent-helper-1\n/usr/lib/mail-dotlock\n/usr/lib/ssh/ssh-keysign\n/usr/lib/kf5/fileshareset\n/usr/lib/qemu/qemu-bridge-helper\n/usr/lib/cockpit/cockpit-session\n/usr/lib/vmware/bin/vmware-vmx\n/usr/lib/vmware/bin/vmware-vmx-debug\n/usr/lib/vmware/bin/vmware-vmx-stats\n/usr/lib/virtualbox/VBoxHeadless\n/usr/lib/virtualbox/VBoxNetAdpCtl\n/usr/lib/virtualbox/VBoxNetDHCP\n/usr/lib/virtualbox/VBoxNetNAT\n/usr/lib/virtualbox/VBoxSDL\n/usr/lib/virtualbox/VirtualBoxVM\n/usr/lib/electron27/chrome-sandbox\n/usr/lib/chromium/chrome-sandbox\n/usr/lib/electron28/chrome-sandbox\n/usr/lib/Xorg.wrap\n/opt/microsoft/msedge/msedge-sandbox\nBut okay. This isn’t too bad. Maybe I can’t execute binaries from the contianer images, outside the container images.\n[moonpie@lizard ~]$ cd /home/moonpie/.local/share/containers/storage/overlay/2fa37f2ee66efbd308b9b91bce81c262f5e6ab6c3bf8056632afc60cc602785c/diff/usr/bin/\n[moonpie@lizard bin]$ ./gpasswd\nconfiguration error - unknown item 'FAIL_DELAY' (notify administrator)\nUsage: gpasswd [option] GROUP\n\nOptions:\n  -a, --add USER                add USER to GROUP\n  -d, --delete USER             remove USER from GROUP\n  -h, --help                    display this help message and exit\n  -Q, --root CHROOT_DIR         directory to chroot into\n  -r, --remove-password         remove the GROUP's password\n  -R, --restrict                restrict access to GROUP to its members\n  -M, --members USER,...        set the list of members of GROUP\n  -A, --administrators ADMIN,...\n                                set the list of administrators for GROUP\nExcept for the -A and -M options, the options cannot be combined.\nThat error is likely due to the fact that my Arch system has a newer version of the config file then whatever this gpasswd binary is expecting.\nTheoretically, if there was a vulnerable setuid binary in one of these containers, then someone could execute it to become able to do things as my user.\nHowever, this isn’t actually that big of an issue, because only I can access my home direcotry, where the container images are stored.\nI also experimented with docker images, with the rootful docker daemon. The same thing happened, where only root has permission to read and execuite the directories those docker images are stored in.\nI panicked at first, seeing all those setuid binaries, but it doesn’t seem to be that much of a deal, and doesn’t give people access to things they don’t already have access to.. by default at least."
  },
  {
    "objectID": "guides/learn-linux/index.html",
    "href": "guides/learn-linux/index.html",
    "title": "Free resources to learn various software things",
    "section": "",
    "text": "My browser bookmarks bar is massive. I basically bookworm anything interesting I come across, and now I have a lot of resources accumulated, and I think it’s worth it to write them down, although formatting is still a work in progress.\nWhen not otherwise stated, the resource is related to Linux administration."
  },
  {
    "objectID": "guides/learn-linux/index.html#troubleshooting-servers",
    "href": "guides/learn-linux/index.html#troubleshooting-servers",
    "title": "Free resources to learn various software things",
    "section": "Troubleshooting servers",
    "text": "Troubleshooting servers\nSadServers\n\nInteractive troubleshooting of broken Linux servers."
  },
  {
    "objectID": "blog/kde-6/index.html",
    "href": "blog/kde-6/index.html",
    "title": "KDE 6 — New features",
    "section": "",
    "text": "KDE 6 is released, and it comes with quite a few new features.\n\nSmoothing out the Wayland migration\nIn addition to the other things mentioned, KDE 6 also comes with some new security and privacy features.\n\nKDE 6 now adds a new settings panel where users can configure if X11 apps can read global inputs.\nFor context, the Linux desktop used to be built on top of this software called X11. X11 worked, but it had some pretty severe flaws, the two main one’s being code rot, and security.\nCode rot is self explanatory. X11 was first built in 1984. It’s been a while since it was made, and it has a lot of old code. The project is so big and complex, that it’s nearly impossible to add significant features to it.\nThe other problem is security. By default, X11 lets running apps have basically absolute control over the other graphical interface. They can “view” the whole screen, read all keypresses, and insert, or modify keypresses.\nWayland, is the newer replacement, although it is technically just a protocol, a definition, as opposed to an actual piece of software. Different pieces of software choose to implement this protocol, with KDE being one of them.\nOne of the issues people have with Wayland, is that, by design, it doesn’t allow all apps to read everything on the screen. This breaks things like screen sharing, with native Discord (although sharing your screen to Discord running in Firefox works).\nIt also breaks push-to-talk on Discord, since that uses a global hotkey. The above setting allows users to temporarily restore that functionality, at the cost of desktop security. , since Discord hasn’t developed it for native wayland yet.\nAnother issue people have with Wayland is problems with Nvidia. Due to Nvidia’s insistence on proprietary drivers and their own standards, it’s been very difficult ot get Wayland support on Nvidia. However, KDE comes with some big changes as, I can finally use Nvidia + Wayland on my larger laptop!\n\n\nOBS\nOne features I’ve been waiting on, which I think will only end up coming in Wayland, is the ability to share some windows, but not all. I experimented with some of the new features of OBS, but they don’t do quite what I want.\nFor example, with OBS and pipewire, I noticed I can select multiple windows from the “Window Share” source:\n\nExcept it doesn’t actually work. Instead, it just puts one of the windows on top, and none of the others.\nI tried an alternative to this:\n\nBut it doesn’t do quite what I want. It creates a literal virtual monitor, including the ability to change settings to things like “unify outputs”, or “extend outputs”.\nWhat I really want is the ability to share only a single kde workspace.\nOBS had an option to share a KDE plasma workspace, but it doesn’t work like what I want. Instead of just sharing a single workspace, it turns all monitors into a single input source. If I switch workspaces, then the screen video switches as well.\n\n\nDrawing Tablets\nKDE 6 finally makes support for drawing tablets first class. You can now configure drawing tablets directly from the settings menu.\n\nSo all the buttons, but also another, more important feature I need: mapping the drawing tablet to a smaller portion of the screen.\nI typically take notes with xournal++, but I struggle because my handwriting is large and sloppy, especially when I am trying to take notes more quickly. From my testing, this feature is amazing, since it makes it easier to write, and read what I’ve wrote afterwards.\n\n\nBugs\nSo far, I’ve encountered two crashes with KDE, after like a week of usage, which is pretty good from what I hear, compared to other people.\nTo recover from a crash, I just switch to a TTY, and/or use a terminal, and then run:\nsystemctl --user restart plasma-plasmallshell\nThis only works on systemd, however. Since Plasma-5.25, KDE consisting of systemd user services has been the default method\nOn non systemd distros, what I’ve heard works is:\nkillall plasmashell && kstart5 plasmashell"
  },
  {
    "objectID": "playground/flake-shell/index.html",
    "href": "playground/flake-shell/index.html",
    "title": "Creating a nix flake, the “proper” way",
    "section": "",
    "text": "So, I think it’s time for me to move from the older channels way of doing things in nix, to the newer flakes. However, the nix docs are very poor, so I am going to document my process of converting my development environment here.\nThe first thing, is that despite the fact that there is a lot of the existing flakes use a utility called flake-utils. However, this tool was ultimately started as an experiment, and has issues. A blog post (archive) goes over some of the issues it has, and recommends against it… except I can’t figure out at all how to apply it.\nThe other recommendation is another pattern, recommended by Reddit user Tomberek:\n{\n  inputs.nixpkgs.url = \"github:NixOS/nixpkgs/22.05\";\n  outputs = {self,nixpkgs}: {\n    devShells = builtins.mapAttrs (system: pkgs: {\n      default = pkgs.mkShell {\n        shellHook = ''\n          echo 'WARNING: gotcha during nix develop shellHooks'\n          function _direnv_hook(){\n              echo 'WARNING, \"_direnv_hook\" has been overwritten\"'\n          }\n        '';\n        packages = [\n          (pkgs.writeShellApplication {\n            name = \"ls\";\n            text = ''\n              echo 'WARNING, \"ls\" has been overwritten\"'\n            '';\n          }\n            )\n        ];\n      };\n    }) nixpkgs.legacyPackages;\n  };\n}\nThere are multiple different ways of creating an output for multiple systems at once, it seems.\nSo far:\n\nForEachSystem\nFlake-parts\nFlake-utils\nTombereks’ method\n\nI suspect there are other methods, potentially better one’s, that aren’t documented.\nMy ultimate goal is to convert the shell.nix I am using, into a flake."
  },
  {
    "objectID": "projects/build-server-4/index.html",
    "href": "projects/build-server-4/index.html",
    "title": "My server part 4 — Kubernetes",
    "section": "",
    "text": "I was working on configuring rootless podman via ansible, but I had trouble because the tooling was incomplete. Ansible is a suboptiomal way to manage containers, and rootless podman can’t manage its own services.\nFor the whole journey, see the previous post\nSo yeah. I’ve decided to switch to Kubernetes, because Kubernetes can manage it’s own services, and be rootless. The configuration-as-code landscape for Kubernetes, is much, much better than podman, and I will get many more options. For example, I can use helm, which is somewhat like a package manager for kubernetes, to install apps. Both Caddy and Authentik offer helm packages. Using the offered packages is probably less work than converting a docker compose to a podman container."
  },
  {
    "objectID": "projects/build-server-4/index.html#user-namespaces",
    "href": "projects/build-server-4/index.html#user-namespaces",
    "title": "My server part 4 — Kubernetes",
    "section": "User Namespaces",
    "text": "User Namespaces\nJust like how Linux has “distributions”, or bundles of software that build on top of the Linux kernels, Kubernetes has distros. I want rootless Kubernetes,\nI started by looking at k3s, the version of kubernetes I have experience with. However, there appear to be some caveats with rootless mode… including not being able to run a multi node cluster.\nThere is an article related to Kubernetes rootless containers: https://rootlesscontaine.rs/getting-started/kubernetes/\nAnd what I see is… not promising. I don’t really desire to do run kind or minikube, because they are stripped down compared to more feature-full Kubernetes distros like k3s\nMinikube, from that list above, is promising, but it is designed for development/testing, and doesn’t support a multi-machine multi node setup\nKind has a similar usecase, and limitation. Despite how easy it is to do rootless on them, that makes them unsuitable for me.\nOkay, I seem to have misunderstood what &lt;rootlesscontaine.rs&gt; want, as compared to what I want. That site documents how to run all the kubernetes components, as rootless using a user namespace. What I want, is using user namespaces to isolate pods.\nKubernetes seems to support this natively: https://kubernetes.io/docs/tasks/configure-pod-container/user-namespaces/\nTo enable user namespaces, all you need to do is set hostUsers: false in your kubernetes yaml… except, how can I override this for an existing helm chart?"
  },
  {
    "objectID": "projects/build-server-4/index.html#package-managerhelm",
    "href": "projects/build-server-4/index.html#package-managerhelm",
    "title": "My server part 4 — Kubernetes",
    "section": "Package Manager/Helm",
    "text": "Package Manager/Helm\nHelm is very nice. However, from what I’ve heard, it is difficult to modify repackaged charts. This is especially concerning to me, because I intend to run all of my containers within user namespaces, and many helm charts don’t provide this. In order to run prepackaged apps in user namespaces, I need to modify existing helm charts.\nhttps://helm.sh/docs/intro/using_helm/#customizing-the-chart-before-installing\nHelm has docs on customizing the chart before installing.\nThere exist some package repositores (similar to dockerhub) for helm:\n\nhttps://artifacthub.io/\n\nHelm charts, package\n\nhttps://operatorhub.io/\n\nNot helm, rather kubernetes yaml managed by a lifecycle manager\nPackages “operators”, like this rook/ceph\nAppeals to me less than artifacthub"
  },
  {
    "objectID": "projects/build-server-4/index.html#kubernetes-distro",
    "href": "projects/build-server-4/index.html#kubernetes-distro",
    "title": "My server part 4 — Kubernetes",
    "section": "Kubernetes Distro",
    "text": "Kubernetes Distro\nHere is a list on the CNCF website\nI see three main options available to me:\n\nKubespray (ansible)\nK3s&gt;\nRKE2\nk0s\n\nHas an ansible playbook\n\nkurl\n\nCustom kubernetes installer, including things like storage\nSupports rook, but I don’t know if it is rook + ceph\ndoesn’t seem to have anywhere near as much activity as I expect it to have… no CI/CD, Longhorn is deprecated\n\nKubernetes the Easier Way\n\nComes with a plethora of features\n\nincluding nginx + letsencrypt, helm, ceph, metallb, prometheus/grafana\nVery appealing, many features\n\n\n\nI don’t really want to opt for manual installation, or anything that’s too complex for too little gains.\nKubespray appeals to me since it’s ansible, and it would be cool to manage the kubernetes cluster installation and the services from the same spot — but it got ruled out:\nFrom the README\n\nSupported Docker versions are 18.09, 19.03, 20.10, 23.0 and 24.0. The recommended Docker version is 24.0. Kubelet might break on docker’s non-standard version numbering (it no longer uses semantic versioning). To ensure auto-updates don’t break your cluster look into e.g. the YUM versionlock plugin or apt pin\n\nIt seems as kubespray deploys kubernetes by installing docker, and then deploying kubespray in docker. Although this is a neat decision, I cannot use docker at all, becuause I intend to deploy openstack using kolla-ansible, which also uses docker. From my testing, the openstack deployment completely destroys docker’s networking, so I probably can’t use both at once.\nIn addition to that, kubespray completely uninstalls podman, meaning I can’t use podman as a provider for kubespray either.\nI found an ansible role to deploy RKE2, but it seems that it just deploys vanilla RKE2, with none of the goodies that I am searching for like a load balancer or external storage.\nI also found something similer for k3s, an ansible role to deploy a “vanilla” kubernetes cluster. However, at the bottom, they mention some other ansible roles to deploy a more than vanilla k3s cluster.\nI found a reddit post where someone open sourced their own ansible playbook — Only 22 days ago (as of the time of writing this), and it’s quite comprehensive. It comes with Argocd, Cilium, Longhorn, Prometheus, Cloudfare Let’s Encrypt certificates with cert-manager, and more.\nI looks very appealing to me, despite the fact that it seems to be opinionated, and designed for personal use. In addition to that, they are simply using ansible’s helm modules to deploy stuff — what would be different from me doing that, with my own deployment choices?"
  },
  {
    "objectID": "projects/build-server-4/index.html#distributed-storage",
    "href": "projects/build-server-4/index.html#distributed-storage",
    "title": "My server part 4 — Kubernetes",
    "section": "Distributed (?) Storage",
    "text": "Distributed (?) Storage\nEventually, I do plan to scale up, and that requires a distributed storage solution. I see two main options:\n\nCeph\nLonghorn (SUSE)\n\nArtifacthub\n\nSeaweedFS\nKadalu (GlusterFS)\n\nHelm chart?\n\nlocal-path-provisioner (SUSE)\n\nAn enhancement to Kubernete’s builtin ability to handle local storage paths, by SUSE\nmaybe this is optimal, since I have just a one node cluster?\n\n\nLonghorn appeals to me, because if I choose to use other Suse products like rancher, then they probably integrate.\nBut now, I’ve chosen to opt for FluxCD to manage my cluster rather than Longhorn. Because of this, I will probably opt for Ceph.\nI see a few options to deploy Ceph:\n\nhttps://artifacthub.io/packages/helm/rook/rook-ceph\n\nHas a severe security vulnerability reported, but is it really that bad?\n\nCSI (link later)\n\nI don’t understand what a CSI is and how it compares to the Rook ceph operator.\nHowever, it seems I’m running into another issue: It’s difficult to run rook-ceph on a single node. There are also other complaints about performance with ceph — the big complaint is that ceph uses up a lot of CPU relative to other distributed storage, but I wasn’t worrying about that. However, I’ve seen multiple claims that ceph requires high end hardware — SSD’s, which I don’t have. (Right now I have just one hard drive).\nLonghorn has a similar issue — at least it seems usable on only one node, but the recommendation is to have at least 3 nodes.\nLocal-path-provisioner is probably what I’m going to use, because I think it is built into k3s (and by extension, RKE2), by default."
  },
  {
    "objectID": "projects/build-server-4/index.html#gitops-software",
    "href": "projects/build-server-4/index.html#gitops-software",
    "title": "My server part 4 — Kubernetes",
    "section": "Gitops Software",
    "text": "Gitops Software\nGitops is a principle of software deployment, where the deployment infrastructure, services, and configuration, are stored in git — hence the name, Git Operations.\nThere are several ways to do Gitops on Kubernetes, but the core challenge I am encountering, is that some Gitops software must be deployed to Kubernetes in order to manage the cluster, but you cannot use that software to deploy itself.\nWhat likely happens is that after you deploy the software to the cluster, then it records itself and adds itself to the state, but I have to ensure this works properly.\nOr maybe the GitOps software stays outside the configuration, eternally untracked, but still self updating?\nI still haven’t selected a GitOps software, but I am looking at:\n\nArgoCD\nFluxCD\n\nSimple enough to bootsrap\nbootstraps itself from Github repo\n\nFleet (made by SUSE, just like k3s, RKE2, rancher, and longhorn)\n\nAfter thinking about it, I can’t find a way to deploy a cluster and the CI/CD software at once, in such a way that it provisions itself. Many deployment methods simply abstract deploying the CI/CD software afterwards.\nIt’s probably best to not rely on abstractions since this is my first time really deploying Kubernetes, instead, I will just have to accept that the Kubernetes deployment will not be stored as code.\nI found something interesting:\n\nhttps://github.com/farcaller/nixhelm\nhttps://github.com/farcaller/nixdockertag\n\nMentioned in a Lemmy comment, it takes helm charts, and is able to convert them to a format that can be consumed by ArgoCD, using Nix.\nOkay, but after more research, I’ve settled on Flux. It seems very easy to bootstrap, and to use helm charts with it. I don’t really need a GUI or multitenancy like ArgoCD provides, or the integrations that Fleet (probably) provides.\nFlux seems “lightweight”. It reads from a Git repo, and applies and reconciles state. In addition to that, it can bootstrap itself. Although, I think I will end up running into a funny catch-22 when I decide to move away from github to a self hosted forgejo, on the kubernetes cluster, everything will be fine… probably.\nMaybe I could have a seperate git server, and that stores the Kubernetes state? Flux seems to support bootstrapping from any git repo.\nA few recommendations on the internet seem to suggest that I should have bootstrap flux from something external to the cluster, rather than from inside the cluster."
  },
  {
    "objectID": "projects/build-server-4/index.html#misc-addonsdeployment",
    "href": "projects/build-server-4/index.html#misc-addonsdeployment",
    "title": "My server part 4 — Kubernetes",
    "section": "Misc Addons/Deployment",
    "text": "Misc Addons/Deployment\n\nMonitoring: kube-prometheus-helm-stack\nSecrets:\n\n: https://github.com/getsops/sops\n\nBasically ansible vault, very appealing\n\n: https://external-secrets.io/latest/\n\nSeems catch-22y, I need an existing external service to manage secrets… but I suppose it is called external secrets\n\n\nIngress\n\nNginx ingress… but how do I get SSL with this setup?\nTraefik ingress (has automatic https)\nCaddy: https://github.com/caddyserver/ingress — WIP software…\n\n\nI need a simple git ssh for bootstrapping flux from.\n\nhttps://github.com/chrisnharvey/simple-git-server\n\nSeems unmaintained\n\nApparently you can just use ssh as a git server"
  },
  {
    "objectID": "projects/build-server-4/index.html#authentik",
    "href": "projects/build-server-4/index.html#authentik",
    "title": "My server part 4 — Kubernetes",
    "section": "Authentik",
    "text": "Authentik\nAuthentik provides documentation on a Kubernetes deployment, along with a Helm chart."
  },
  {
    "objectID": "projects/build-server-4/index.html#forgejo",
    "href": "projects/build-server-4/index.html#forgejo",
    "title": "My server part 4 — Kubernetes",
    "section": "Forgejo",
    "text": "Forgejo\nForgejo has a helm chart: https://codeberg.org/forgejo-contrib/forgejo-helm\nUnlike authentik, forgejo’s helm chart also seems to have some support for rootless/user namespaces."
  },
  {
    "objectID": "projects/build-server-4/index.html#nextcloud",
    "href": "projects/build-server-4/index.html#nextcloud",
    "title": "My server part 4 — Kubernetes",
    "section": "Nextcloud",
    "text": "Nextcloud\nThere is an existing helm chart for nextcloud: https://github.com/nextcloud/helm\nHowever, it says in the above, that it is community maintained, and not truly official.\nGoing to the nextcloud official docs for larger scale deployment recommendations… and it’s paywalled. It’s likely that Nextcloud maintains official helm charts — but only for paying customers."
  },
  {
    "objectID": "projects/build-server-4/index.html#openstack",
    "href": "projects/build-server-4/index.html#openstack",
    "title": "My server part 4 — Kubernetes",
    "section": "Openstack",
    "text": "Openstack\nSince I am using Kubernetes to deploy services, it is worth investing if I can deploy Openstack (or some other self-hosted cloud) on Kubernetes.\n\nYaook\nAtmosphere\n\nThese look appealing, but very hard to deploy."
  },
  {
    "objectID": "projects/build-server-4/index.html#kubernetes",
    "href": "projects/build-server-4/index.html#kubernetes",
    "title": "My server part 4 — Kubernetes",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nMulti-Tenancy\nIn case I can’t obtain multi-tenancy by openstack provisioning kuberntes, there are some alternative solutions I am looking at:\n\nvcluster\ncapsule\nkamaji"
  },
  {
    "objectID": "talks/self-site/revealjs.html#about-me..-and-my-site",
    "href": "talks/self-site/revealjs.html#about-me..-and-my-site",
    "title": "Quarto and GH Pages",
    "section": "About me.. and my site",
    "text": "About me.. and my site\n\nJeffrey Fonseca\n\nUsing linux for almost 6 years\nStarted blogging just last year\n\nSite: moonpiedumplings.github.io"
  },
  {
    "objectID": "talks/self-site/revealjs.html#qrcode-to-the-slides",
    "href": "talks/self-site/revealjs.html#qrcode-to-the-slides",
    "title": "Quarto and GH Pages",
    "section": "QRCode to the Slides",
    "text": "QRCode to the Slides"
  },
  {
    "objectID": "talks/self-site/revealjs.html#benefits-of-a-site",
    "href": "talks/self-site/revealjs.html#benefits-of-a-site",
    "title": "Quarto and GH Pages",
    "section": "Benefits of a site",
    "text": "Benefits of a site\n\nPortfolio and resume\nNotes on longer projects\nSocial Media — A few friends follow my blog"
  },
  {
    "objectID": "talks/self-site/revealjs.html#setup",
    "href": "talks/self-site/revealjs.html#setup",
    "title": "Quarto and GH Pages",
    "section": "Setup",
    "text": "Setup\n\nGo to github.com and create an account\n\nYour site will be located at “username.github.io”\n\nGo to github.com/moonpiedumplings/blog-template and select “use template”\n\nThe repo name should be “username.github.io”\nmake sure to select “include all branches”\nin the github settings, make sure “gh-pages” is selected as the deploy"
  },
  {
    "objectID": "talks/self-site/revealjs.html#directory-structure",
    "href": "talks/self-site/revealjs.html#directory-structure",
    "title": "Quarto and GH Pages",
    "section": "Directory Structure",
    "text": "Directory Structure\nThe files and folders are laid out simply.\n.\n|-- _quarto.yml # Configuration for quarto\n|-- blog\n|   |-- index.md # Directory listing\n|   |-- markdown\n|   |   `-- index.md\n|   `-- quarto\n|       `-- index.md\n|-- index.md"
  },
  {
    "objectID": "talks/self-site/revealjs.html#directory-structure-cont.",
    "href": "talks/self-site/revealjs.html#directory-structure-cont.",
    "title": "Quarto and GH Pages",
    "section": "Directory Structure (cont.)",
    "text": "Directory Structure (cont.)\n\nThe “index.md” within of each folder is what gets rendered into a an html file, a browser page\nThe “index.md” in “blog” is a special format called a “listing”, rather than a normal article, it generates the list of articles, the filtering, etc on the “blog” page"
  },
  {
    "objectID": "talks/self-site/revealjs.html#markdown",
    "href": "talks/self-site/revealjs.html#markdown",
    "title": "Quarto and GH Pages",
    "section": "Markdown",
    "text": "Markdown\n\nMarkdown is markup language\nEnables users to render formatted documents from plaintext\nMost static site generators use markdown"
  },
  {
    "objectID": "talks/self-site/revealjs.html#markdown-code-snippets",
    "href": "talks/self-site/revealjs.html#markdown-code-snippets",
    "title": "Quarto and GH Pages",
    "section": "Markdown Code Snippets",
    "text": "Markdown Code Snippets\n\n`this is code snippets`\n\nThey result in unformated, monospace text:\n\nthis is code snippets"
  },
  {
    "objectID": "talks/self-site/revealjs.html#markdown-basics",
    "href": "talks/self-site/revealjs.html#markdown-basics",
    "title": "Quarto and GH Pages",
    "section": "Markdown Basics",
    "text": "Markdown Basics\n*italics* = italics\n**bold** = bold\n[linktext](example.com) = linktext"
  },
  {
    "objectID": "talks/self-site/revealjs.html#more-markdwon-basics",
    "href": "talks/self-site/revealjs.html#more-markdwon-basics",
    "title": "Quarto and GH Pages",
    "section": "More Markdwon Basics",
    "text": "More Markdwon Basics\n\n# top level header\n\ntop level header\n\n\n## lower level header\n\nlower level header"
  },
  {
    "objectID": "talks/self-site/revealjs.html#useful-html-in-markdown",
    "href": "talks/self-site/revealjs.html#useful-html-in-markdown",
    "title": "Quarto and GH Pages",
    "section": "Useful HTML in Markdown",
    "text": "Useful HTML in Markdown\n&lt;br&gt; = line break\n\n&lt;details&gt;&lt;summary&gt;summary text&lt;/summary&gt;\n\ncollapsable content\n\n&lt;/details&gt;\n\n\n\nsummary text\n\ncollapsable content"
  },
  {
    "objectID": "talks/self-site/revealjs.html#quarto",
    "href": "talks/self-site/revealjs.html#quarto",
    "title": "Quarto and GH Pages",
    "section": "Quarto",
    "text": "Quarto\n\nStatic site generator\nConverts markdown into html, the language powering websites.\nFeature Rich by Default\n\nFull text search\nLight and dark theme"
  },
  {
    "objectID": "talks/self-site/revealjs.html#quarto-1",
    "href": "talks/self-site/revealjs.html#quarto-1",
    "title": "Quarto and GH Pages",
    "section": "Quarto",
    "text": "Quarto\n\nOutputs: website page, pdf, or presentation (html)\nDesigned for Data Science, so has integration with Jupyter Notebooks\nDoes not have any form of template engine"
  },
  {
    "objectID": "talks/self-site/revealjs.html#template-engine",
    "href": "talks/self-site/revealjs.html#template-engine",
    "title": "Quarto and GH Pages",
    "section": "Template Engine",
    "text": "Template Engine\nNot included in Quarto\n{% for i in list %}\n  i.attribute\n  {% if something %}renderif{% endif %}\n{% endfor %}"
  },
  {
    "objectID": "talks/self-site/revealjs.html#questions-comments-anything-you-want-to-do",
    "href": "talks/self-site/revealjs.html#questions-comments-anything-you-want-to-do",
    "title": "Quarto and GH Pages",
    "section": "Questions? Comments? Anything you want to do?",
    "text": "Questions? Comments? Anything you want to do?"
  },
  {
    "objectID": "writeups/nice-challenge-4/index.html",
    "href": "writeups/nice-challenge-4/index.html",
    "title": "Nice Challenge 4",
    "section": "",
    "text": "This is for the NICE challenge.\nI did pretty poorly with this one. I have little to no hands-on experience with pfsense and Windows Active Directory.\nSame as before, there was a simulated chatroomm:\n\n\n\nmeeting notes\n\n\nHowever, there were yet more notes, as mentioned in the simulated chatroom:\nCreate Active Directory security group named DasNetworkAdmins with Gary Thatcher, Brimlock Stones, and yourself as members. Grant members of DasNetworkAdmins the privileges required to sign in to pfSense (Firewall) and make changes on all pages as admins.\n\nFor the pfSense LDAP authentication, I have created a bind user for you on the domain. The credentials are 'pfSenseBind' with the password 'password123'. The system for getting that setup works much better in new versions of pfSense, so you should do that first.\n\nPlease create a new database called production_site and create the user das_user for the website to connect with using the backup located at /ftp on Backup.\ndas_user only needs to have privilege on that database, since we're trying to keep a least privilege model.\n\nPlease patch shellshock on Fileshare\n\nFor the Domain Group Policy we are making for the new security group, DasNetworkAdmins, please:\n\nName it Network-Policy\nPlease ensure that the Computers Security Setting Account Password Policy is set to not allow passwords to be shorter than 14 characters.\n\nPlease enable the Computers Security Setting Local Policy for Auditing Policy Changes on both successes and failures.\n    \n\nPlease enable the Computers Security Setting Local Policy Security Option to limit the amount of cached previous logons to 0.\n    \n\nThen make sure to apply and enforce the new GPO.\n\nMake sure that after migrating the website to the production server that it is all accessible at www.daswebs.com.\n\nDO NOT CHANGE ANY PASSWORDS\n(I used xkcd949.com to send the file to a remote machine, since the Windows command line was not cooperative.)\nAnd here are the checks:\n\n\nDatabase\nI need to start by importing the relevant database from /ftp/wordpress.sql, on the Debian 9 “Backup” machine, onto the database on the Windows “Database” machine\nAn easy way to do that is via the builtin python3 -m http.server command with /ftp as my working directory. I can then simply download the file from 172.16.30.79:8000/wordpress.sql using the Firefox browser on windows.\n\nThen, I can simply import an sql database using the mysql command line\nI also need to create a user\nmysql -u root\n\nCREATE DATABASE production_database;\n\nCREATE USER 'das_user'@'*' IDENTIFIED BY password 'das_password';\n\nGRANT ALL PRIVILEGES ON production_database.* TO 'das_user'@'*';\n\nexit\nAnd then in the normal shell:\nbash # this is important or else &lt; redirect will not work\n\nmysql -u root -p production_site &lt; C:\\\\Users\\playerone.DASWEBS\\Downloads\\wordpress.sql\nHowever, despite all of this, the check still isn’t green.\n\n\nFileshare\nI was tasked with patching Shellshock this machine. “Fileshare” was a Debian 10 machine, which is still receiving minimal LTS support, meaning all I have to do to patch this bug is to upgrade the packages.\n\nsudo apt update\n\nsudo apt upgrade\nThis errored, it recommended running with --fix-missing, and so I did.\nsudo apt upgrade --fix-missing\nAfter this, the “Shellshock” check was green.\n\n\nDomain-Admin\nTo change group policy on the Windows Domain Controller …\n\n\nFirewall\nHere the docs on connecting pfsense to ldap\n\n\nProd-Web\nTo install the “required web server software” (Apache), was simple\nsudo yum install httpd\nHowever, the relevant check does not become green."
  },
  {
    "objectID": "allposts.html",
    "href": "allposts.html",
    "title": "RSS Feed",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 4, 2024\n\n\nCreating a nix flake, the “proper” way\n\n\n \n\n\n\n\nMay 2, 2024\n\n\nContainer images and setuid binaries\n\n\n \n\n\n\n\nApr 6, 2024\n\n\nFree resources to learn various software things\n\n\n \n\n\n\n\nMar 25, 2024\n\n\nYour very own site on GH pages\n\n\n \n\n\n\n\nMar 20, 2024\n\n\nMy server part 4 — Kubernetes\n\n\n \n\n\n\n\nMar 13, 2024\n\n\nKDE 6 — New features\n\n\n \n\n\n\n\nMar 11, 2024\n\n\nExperiments with Running python in the browser\n\n\n \n\n\n\n\nFeb 20, 2024\n\n\nAutomatically provisioning VMs from OVAs\n\n\n \n\n\n\n\nJan 23, 2024\n\n\nHow to Ansible\n\n\n \n\n\n\n\nJan 17, 2024\n\n\nBuilding my server part 3 — The switch to debian\n\n\n \n\n\n\n\nJan 15, 2024\n\n\nPackaging quarto using nix\n\n\n \n\n\n\n\nJan 6, 2024\n\n\nAnsible: Defined is not Truthy\n\n\n \n\n\n\n\nNov 15, 2023\n\n\nCCDC Environment setup\n\n\n \n\n\n\n\nNov 2, 2023\n\n\nLinux and Programming Related Twitch Streamers\n\n\n \n\n\n\n\nOct 28, 2023\n\n\nCCDC Learning Resources\n\n\n \n\n\n\n\nOct 23, 2023\n\n\nContainers (that run on linux)\n\n\n \n\n\n\n\nOct 23, 2023\n\n\nBeing an internet helper\n\n\n \n\n\n\n\nOct 4, 2023\n\n\nA notes format that I can automatically convert to flashcards?\n\n\n \n\n\n\n\nSep 3, 2023\n\n\nPodman vs Docker\n\n\n \n\n\n\n\nAug 29, 2023\n\n\nAttempting to get secure boot and bootable BTRFS snapshots\n\n\n \n\n\n\n\nAug 28, 2023\n\n\nThe CSUN game room has pretty locked down computers…\n\n\n \n\n\n\n\nAug 27, 2023\n\n\nGetting situated in my dorms\n\n\n \n\n\n\n\nAug 9, 2023\n\n\nMy Firefox Setup\n\n\n \n\n\n\n\nAug 7, 2023\n\n\nTmux\n\n\n \n\n\n\n\nAug 3, 2023\n\n\nSwitching to opensuse tumbleweed, research into nixos\n\n\n \n\n\n\n\nAug 2, 2023\n\n\nBuilding my own Server Part 2 — Software\n\n\n \n\n\n\n\nAug 1, 2023\n\n\nBuilding my own Server Part 1 — Hardware\n\n\n \n\n\n\n\nJul 14, 2023\n\n\nAutomating my server config, first nix, then ansible\n\n\n \n\n\n\n\nJul 10, 2023\n\n\nFirst job, an internship at cirrascale\n\n\n \n\n\n\n\nJul 5, 2023\n\n\nIncreasing my security\n\n\n \n\n\n\n\nJun 28, 2023\n\n\nGit\n\n\n \n\n\n\n\nJun 21, 2023\n\n\nThe cruise ship I am on has very locked down computers\n\n\n \n\n\n\n\nJun 15, 2023\n\n\nUrestricted Wifi/Hotspot as long as you have one unlimited device\n\n\n \n\n\n\n\nJun 12, 2023\n\n\nDorking around\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nPackaging Openstack on Nixos\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nA very clever crypto scam\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nCan I include text from other files in quarto?\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nCan I write my resume in python?\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nInstalling the nix package manager and how it’s useful\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nI installed opensuse on my laptop\n\n\n \n\n\n\n\nMay 2, 2023\n\n\nCompiling KasmVNC on NixOS\n\n\n \n\n\n\n\nFeb 28, 2023\n\n\nNginx proxy manager\n\n\n \n\n\n\n\nFeb 14, 2023\n\n\nHow to get a subdomain from duckdns\n\n\n \n\n\n\n\nFeb 1, 2023\n\n\nWhy schools should be less aggressive with content blocking\n\n\n \n\n\n\n\nJan 26, 2023\n\n\nKasmweb setup\n\n\n \n\n\n\n\nSep 30, 2022\n\n\nSetting up cockpit\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/cirrascale-intern/index.html",
    "href": "blog/cirrascale-intern/index.html",
    "title": "First job, an internship at cirrascale",
    "section": "",
    "text": "Cirrascale is a San Diego company that offers managed cloud computing, targeted at AI. Before my vacation, I took a tour of their hosting center. It honestly had me awestruck, seeing rack upon rack of water cooled servers.\n2023-7-10\nFirst day. It was pretty nice. My skills with managing linux, containers, and virtual machines came in handy. I took initiative and stepped up when a task was offered, which was tough because I was so new. My heart was literally racing and I felt so nervous, but I didn’t take on more than I could handle and I made good progress on it in that same day. In addition to that, I collaborated with the other interns on things. Overall, it was a lot of fun, and the people were very friendly.\n2023-7-12\nIn the last two days I feel like I have done a lot, despite being so new. I helped another intern install Ubuntu linux, and I am now working on another project, creating a proxmox image with packer to deploy using maas.\n2023-8-1\nSince then, I have completed created an image, although I haven’t tested yet. In addition to that, I got to post all my code on github:\nhttps://github.com/moonpiedumplings/proxmox-maas\n2023-8-26\nAlright, my original intention was for this to be a regular or semi regular type blog thing, but that was very boring, as most days were simply spent grinding away at whatever problems I was working on.\nSo I’m just going to put an overview and reflection here, after I have done the internship.\nI worked on two major projects, which ate up the majority of my time.\nThe first was proxmox-maas. Maas is an automated deployment method created by canonical, and it allows you to deploy configured ubuntu servers to virtual machines, or usually more valuably, bare metal machines. In Cirrascale, they were using Maas to deploy Ubuntu, and using MAAS to configure things like networking on these machines. MAAS can deploy other operating systems, but you have to create an image which it can deploy. I created a proxmox image, so that they could deploy it to bare metal machines rapidly.\nThe other project I created was assisting in planning a setup where they use rancher to deploy kubernetes. I started out with my notes on onedrive, but eventually I got annoyed and put them on github sites. I created a simple docker-compose file to allow people to quickly up rancher for testing purposes, and I noted it on that page. In addition to that, I also documented installing k3s clusters with calico rather than rancher, installing vcluster, installing kata-containers, and other kubernetes management operations.\nOverall, it was a great learning experience. I had an abstract understanding of what production environments required, but it was cool to work with those in production environments."
  },
  {
    "objectID": "blog/firefox/index.html",
    "href": "blog/firefox/index.html",
    "title": "My Firefox Setup",
    "section": "",
    "text": "So Mozilla, the company behind the Firefox web browser, offers a feature called Firefox sync. It enables you to sync your firefox browser, with your mozilla account, to any device.\nHowever, it has a caveat: it can only sync one firefox profile. Firefox profiles, are basically completely different instances on firefox, capable of having every single thing be seperate, down to things like advanced settings.\nGenerally, what is recommended to do to seperate accounts, is to use firefox multi account containers. These seperates cookies and logins between sites. This lets people protect their privacy, because sites like Facebook can no longer snoop to see what other sites have also given you a facebook cookie.\nHowever, I found multi account containers very frustrating to use. If you are trying to manage multiple accounts on the same site, on a site that doesn’t let you do that natively (google/gmail do, but microsoft doesn’t), it constantly asks you if you are sure that you want to open this site in this container. In addition to that, it does that for other sites as well, like google search, which makes it very frustrating.\nFirefox profiles, don’t have that annoyance, but I also enjoy the greater degree of seperation they give. Because each firefox profile can have completely differenct settings, this means that I can do things like configure one firefox profile for performance, but at the cost of stability. However, my schoolwork profile is configured never to crash, because it needs to be reliable. Multi account containers can’t do that.\n\n\nBecause I use multiple profiles, I would like the option to chose what link each firefox profile opens in.\nSo far, I’ve tried this command xdg-settings set default-web-browser none.desktop, with the goal of a “choose application” screen appearing that then lets me run firefox -p profilename to open a websit ein a specific profile, but it didn’t work.\nI would like to try some further stuff, but if it doesn’t work out, I can just default to the “play” profile which is my profile for personal use and the like"
  },
  {
    "objectID": "blog/firefox/index.html#default-applications",
    "href": "blog/firefox/index.html#default-applications",
    "title": "My Firefox Setup",
    "section": "",
    "text": "Because I use multiple profiles, I would like the option to chose what link each firefox profile opens in.\nSo far, I’ve tried this command xdg-settings set default-web-browser none.desktop, with the goal of a “choose application” screen appearing that then lets me run firefox -p profilename to open a websit ein a specific profile, but it didn’t work.\nI would like to try some further stuff, but if it doesn’t work out, I can just default to the “play” profile which is my profile for personal use and the like"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMar 13, 2024\n\n\nKDE 6 — New features\n\n\n \n\n\n\n\nNov 2, 2023\n\n\nLinux and Programming Related Twitch Streamers\n\n\n \n\n\n\n\nOct 23, 2023\n\n\nBeing an internet helper\n\n\n \n\n\n\n\nAug 27, 2023\n\n\nGetting situated in my dorms\n\n\n \n\n\n\n\nAug 9, 2023\n\n\nMy Firefox Setup\n\n\n \n\n\n\n\nAug 3, 2023\n\n\nSwitching to opensuse tumbleweed, research into nixos\n\n\n \n\n\n\n\nJul 10, 2023\n\n\nFirst job, an internship at cirrascale\n\n\n \n\n\n\n\nJul 5, 2023\n\n\nIncreasing my security\n\n\n \n\n\n\n\nJun 11, 2023\n\n\nI installed opensuse on my laptop\n\n\n \n\n\n\n\nFeb 1, 2023\n\n\nWhy schools should be less aggressive with content blocking\n\n\n \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/switch-tumbleweed/index.html",
    "href": "blog/switch-tumbleweed/index.html",
    "title": "Switching to opensuse tumbleweed, research into nixos",
    "section": "",
    "text": "I recently switched from opensuse leap to opensuse tumbleweed. The upgrade process was almost completely seamless. I simply followed the official documentation from opensuse.\nHowever, after I upgraded to tumbleweed, secure boot wasn’t working. I decided to do a fresh reinstall of tumbleweed… and secure boot still didnt’ work.\nHowever, other than that, I a very happy with opensuse tumbleweed. The repositories are large and kept up do date, and some packages which weren’t available or were more dated on opensuse leap such as gocryptfs, syncthing, or firefox, where available in the repositories. I did not have to use nix to install them.\nIn addition to that, the opi command line helper, lets you easily install third party packages from the open build service, or repositories like microsoft’s, so that you can get packages like microsoft edge or vscode, without having to use nix.\nHowever, secure boot not working is detrimental for me. I did a little bit of asking around, and another user complained about a similar struggle, claiming that they have never gotten opensuse tumbleweed secure boot working, and my struggle was futile.\nHmm, in my research, I found an open issue: https://bugzilla.opensuse.org/show_bug.cgi?id=1209985. It appears to be a bug in opensuse tumbleweed. That sucks.\nIt should be automatic, except it isn’t working. I am considering switching to nixos, so that I can have secure boot working, although I don’t really like the most popular implementatio of secure boot on nixos, lanzaboote, because it stores kernels in the esp system partition, which may run out of space because it is usually very small, and nixos often stores many older kernels.\nRelevant links:\nhttps://nixos.wiki/wiki/Secure_Boot\nhttps://nixos.wiki/wiki/Security#Filesystem_encryption\nreddit post where a commmenter posted instructions\nhttps://github.com/DeterminateSystems/bootspec-secureboot\nThis looks very appealing. Rather than using it with systemd-boot, I might be able to set up grub."
  },
  {
    "objectID": "guides/ccdc-env/index.html",
    "href": "guides/ccdc-env/index.html",
    "title": "CCDC Environment setup",
    "section": "",
    "text": "This will be the guide(s) on how to set up your environment for CCDC playing, and development of related scripts + testing them."
  },
  {
    "objectID": "guides/ccdc-env/index.html#chocolatey",
    "href": "guides/ccdc-env/index.html#chocolatey",
    "title": "CCDC Environment setup",
    "section": "Chocolatey",
    "text": "Chocolatey\nInstall chocolatey here\nUsing chocolatey, install several packages we need for play/development. If you already have an app installed, you don’t need to install it with chocolatey.\nIn an administrative powershell or cmd session, run:\nchoco install -y firefox wezterm vscode\n\nWezterm\nInstall wezterm from either chocolatey or the official website (using the setup.)\nOn a vm, I get an error about the opengl version being incompatible when I attempt to launch wezterm. I have to launch it with `wezterm –config “prefer_egl=true”.\nTo launch the ubuntu session, you can right click on the new tab, and then a menu will appear, from which you can select Ubuntu."
  },
  {
    "objectID": "guides/ccdc-env/index.html#ubuntuwsl-in-windows",
    "href": "guides/ccdc-env/index.html#ubuntuwsl-in-windows",
    "title": "CCDC Environment setup",
    "section": "Ubuntu/WSL in Windows",
    "text": "Ubuntu/WSL in Windows\nYou need to have Windows 11 installed, in order for nested virtualization within WSL to work. If you are not on Windows 10, then you need to either upgrade, or work within a linux distribution.\nAs long as you Windows 10 May 2020 Update (version 2004), you can install wsl with a single command. Otherwise, you have to follow some painful steps\nIn an elevated powershell window:\nwsl --install\nReboot the system.\nThen, launch wsl with wsl from powershell or cmd.\nIt’s going to ask you for some prompts. Because this is a development environment, focusing on speed rather than security, I recommend an easy to type username and password.\nEdit the sudoers file:\nTo edit the sudo file, you can use sudo visudo to edit it safely. This command while check the sudoers file for errors, rather than instantly breaking sudo upon a misconfiguration.\n\n\n/etc/sudoers\n\n# Change the portion of the file referring to env_reset to look like this:\n# Put an \"!\" in front of env_reset, and comment out the part referring to secure path\n\nDefaults        !env_reset\nDefaults        mail_badpass\n#Defaults       secure_path=\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" \n\n\n\n# Change the portion of the file that refers to the sudo group to look like this:\n\n%sudo ALL=(ALL:ALL) NOPASSWD: ALL\n\nEither reboot, or restart the wsl virtual machine.\nTo restart the WSL vm, from powershell or cmd:\nInstall docker and docker-compose (and curl and git, other tools we will need later):\nsudo apt update && sudo apt upgrade\nsudo apt install docker.io docker-compose curl git openssh-client wget virt-manager\nYou might have to reboot here, before the next step will work. Maybe.\nsudo usermod -aG docker $USER # Run docker without sudo\nsudo usermod -aG libvirt $USER # Run libvirt vms without sudo\nRebooting the wsl instance will log you in and out, enabling you to use docker, without sudo, and after you launch virt-manager once, you can access it from the windows gui."
  },
  {
    "objectID": "guides/ccdc-env/index.html#nix",
    "href": "guides/ccdc-env/index.html#nix",
    "title": "CCDC Environment setup",
    "section": "Nix",
    "text": "Nix\nIn macos terminal, in linux, or in WSL, install nix (tool used to manage our development environments):\nDo not run these commands with sudo.\ncurl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install\nRestart your shell. Simply close and reopen the terminal.\nAlso, ensure the nix service is enabled by the\nUpdate the nix channels:\nnix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs\nnix-channel --update\nInstall the zellij terminal multiplexer:\nnix profile install nixpkgs#zellij"
  },
  {
    "objectID": "guides/ccdc-env/index.html#vscode",
    "href": "guides/ccdc-env/index.html#vscode",
    "title": "CCDC Environment setup",
    "section": "Vscode",
    "text": "Vscode\nInstall these vscode extensions, and their dependencies (should be pulled automatically):\n\nAnsible\nQuarto\nIf using vscode with WSL, then WSL\n\nPersonal preference, but I like to disable acceptance of the autocomplete suggestion on enter. This can be particularly annoying in languages like ansible or python, where you have to hit enter to go to the next line."
  },
  {
    "objectID": "guides/ccdc-env/index.html#playing",
    "href": "guides/ccdc-env/index.html#playing",
    "title": "CCDC Environment setup",
    "section": "Playing",
    "text": "Playing\ngit clone https://github.com/CSUN-CCDC/CCDC-2023\nWhen in the root of this git repo, you can run nix-shell to get a full development environment. It includes tools like ansible, ansible checking, vagrant, and more."
  },
  {
    "objectID": "guides/cockpit-setup/index.html",
    "href": "guides/cockpit-setup/index.html",
    "title": "Setting up cockpit",
    "section": "",
    "text": "What is cockpit (and similar softwares)\nAmazon lets us have free servers via EC2. The typical way to manage servers is either by sshing in, or using the cloud shell that Amazon (and Oracle) give. However, there are alternative ways to manage servers. One extremely popular example is pteradactyl, a webpage based gui to manage game (usually minecraft). It lets you download game servers as docker containers, run them, stop them, and maybe manage some basic settings, All the things a casual who just wants video games may need. But when I created a free Oracle server, I wanted something more. By this point, I was an experienced linux user, and I wanted more advanced features. So I searched for a more advanced server management tool, like people use on real servers, and I found cockpit.\nCockpit comes with many benefits. The two things I really like however, are that it’s terminal is not laggy at all, unlike the amazon ec2 cloud terminal, and it also offers a gui to manage docker containers.\n\n\nThe installation process\nThe installation process is simple:\nsudo apt install cockpit\nTo start the server, run:\nsudo systemctl enable --now cockpit\nThis sets the cockpit server to start on boot, and it starts it now.\nHowever, the firewall must open ports to allow the cockpit server through. This opens the default ports for the cockpit server. It should be noted that not every version fo linux uses ufw as a firewall, some use other firewalls with different management commands.\nsudo ufw allow 9090\nAnother important thing is to set the password for the default “ubuntu” user account so that you can login to cockpit.\nsudo passwd ubuntu\nIt will ask for the new password twice, not showing what you are typing.\nReboot the computer for the server to start properly, however, this won’t work as the virtual private cloud must have its ports open. I had to do this when I set up cockpit on my Oracle server, so I knew the gist of the steps.\n\n\nOpening EC2’s VPC ports (Also necessary if you want to host servers on ports other than 22, 80, or 443)\nFirst, go to your EC2 vps, where you would normally click connect from, and click on the link under vpc:\n\nThis should bring you up to a screen like this:\n\nClick the security tab, bringing you to a screen like this:\n\nAnd then click on security groups, bringing you to a screen like this:\n\nAnd then click on the “edit inbound rules”\nFinally, you should get something like this:\n\nAdd an item that matches what I have in the third row. That opens the port to allow cockpits server to escape. You may also need to use this page to open other ports if you are hosting servers on nonstandard ports."
  },
  {
    "objectID": "guides/git/index.html",
    "href": "guides/git/index.html",
    "title": "Git",
    "section": "",
    "text": "Still writing"
  },
  {
    "objectID": "guides/git/index.html#get-your-code-git-clone",
    "href": "guides/git/index.html#get-your-code-git-clone",
    "title": "Git",
    "section": "Get your code: git clone",
    "text": "Get your code: git clone\nThe most basic, starting command.\ngit clone repositoryurl\nGit has it’s own protocol, but https works as well, and it’s easier to use.\nBecause of this, you can use the url of the website that hosts the code as your code url.\ngit clone https://github.com/moonpiedumpling/moonpiedumpling.github.io/\n↑ That’s the url of the repository for this website."
  },
  {
    "objectID": "guides/git/index.html#save-code-to-git",
    "href": "guides/git/index.html#save-code-to-git",
    "title": "Git",
    "section": "Save code to git",
    "text": "Save code to git\nCode stored in a git repo exists in four states: unsaved changes, unstaged changes, staged changes, and committed changes, gone through in that order.\nFirst, you open up a file and edit some code. Then you save your changes to the file. However, git doesn’t save your changes. The changes become unstaged changes, meaning git doesn’t even see them.\nTo stage changes:\ngit add file/folder\nSo to stage everything in the repository: git add .\nNow, git sees your changes, but it still hasn’t saved them. They are staged changes, meaning that git can see them in order to commit them.\ngit commit -m \"message here\" commits your changes with a commit message, which makes keeping track of what you did easier, especially when you need to revert changes.\nYou can commit everything in one go using git commit -am, which is more convinient if you don’t need to stage changes.\ngit status shows staged and unstaged changes."
  },
  {
    "objectID": "guides/git/index.html#interact-with-a-remote-repository",
    "href": "guides/git/index.html#interact-with-a-remote-repository",
    "title": "Git",
    "section": "Interact with a remote repository",
    "text": "Interact with a remote repository\nTo upload your changes to a remote repository: git push\nPretty simple. Although you can push to a specific repo.\ngit push remoterepourl\nAnd to bring changes from a remote repo down: git pull"
  },
  {
    "objectID": "guides/git/index.html#viewing-and-undoing-changes",
    "href": "guides/git/index.html#viewing-and-undoing-changes",
    "title": "Git",
    "section": "Viewing and undoing changes",
    "text": "Viewing and undoing changes\nIf you make a small change, you can just copy and paste stuff around to undo it. But if you are working on a large app, and a massive feature you intended to add didn’t work out, trying to just delete the code won’t really work out.\nThat’s where git comes in.\n\nTo view\ngit log shows the changes. q quits, and / searches. When searching, n to go to the next found string, and N (capital N) to go to the previous. It uses vim keybindings. Each change is noted by their SHA1 hash.\nBut this kind of sucks, so I recommend using a graphical interface to view changes. This is one function of “git forges”, websites like github, where they give you a graphical interface to view changes. However, there are locally installed applications with similar features, like gitg.\n\n\nTo revert to a change\ngit revert HEAD creates a new commit that undoes the previous commit.\ngit revert &lt;SHA1 HASH&gt; creates a new commit that reverts all the way back to the commit designated by the SHA1 hash of a commit.\ngit reset SHA1 has a similar feature, except it simply moves backwards in commits. If you make new commits, this results in a nonlinear search commit history. You’ll create a tree, or an octopus. Some people don’t like this, because it can be a bit of a pain to manage."
  },
  {
    "objectID": "guides/git/index.html#branches",
    "href": "guides/git/index.html#branches",
    "title": "Git",
    "section": "Branches",
    "text": "Branches\nBranches are one of the killer features of version control. Basically, each branch is it’s own copy of the codebase, which can be worked on without affecting other branches.\ngit checkout -b &lt;branch name&gt; creates a new branch.\ngit checkout &lt;branch name&gt; switches to a branch.\nThen, commit away. Switching to another branch and only the changes applied to that branch are present.\nIn the wild, branches are commonly used to manage versioning. An older version, but still maintained version of a program will be kept in another branch, so people can still work on it.\ngit checkout manipulates the repository state. To manipulate branches, you must use git branch:\ngit branch -d &lt;branch name&gt; deletes a branch."
  },
  {
    "objectID": "guides/nix-fun/index.html",
    "href": "guides/nix-fun/index.html",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "On a macos or linux system, you can use the DeterminateSystesm nix-installer to install linux.\nSimply run (not as root):\ncurl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install\nAnd then, also run these two commands below (as root):\nnix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs\nnix-channel --update\n\n\n\nFirstly, open a powershell prompt to run these commands.\nwsl --install debian\nwsl --set-version debian 2\nYou may need to do a:\nwsl --update\nSince nix does not support linux, you need to install it in WSL.\nIn the microsoft store, you can install one of many linux distributions or versions. I recommmend debian\nYou then need to enable systemd, you can follow Microsoft’s official insturctions: https://devblogs.microsoft.com/commandline/systemd-support-is-now-available-in-wsl/\nOnce WSL2 with systemd is installed, you can follow the macos/linux install instructions.\nNow, nix is installed. You should be able to access the nix commands in your terminal. For example, the nix-shell command can be used to create a temporary shell environment with packages, environment variables, and more. For examples, see below.\n\n\n\nHome manager is a way to declaratively manage a user environment, including packages installed, configuration files, and environment variables. It uses nix as the langauge of configuration.\nTo install home manager, you can follow the steps from their docs\nTo use home-manager, you can edit the configuration file located at $HOME/.config/home-manager/home.nix For an example, see my my blog post about this"
  },
  {
    "objectID": "guides/nix-fun/index.html#install-on-macoslinux",
    "href": "guides/nix-fun/index.html#install-on-macoslinux",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "On a macos or linux system, you can use the DeterminateSystesm nix-installer to install linux.\nSimply run (not as root):\ncurl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install\nAnd then, also run these two commands below (as root):\nnix-channel --add https://nixos.org/channels/nixpkgs-unstable nixpkgs\nnix-channel --update"
  },
  {
    "objectID": "guides/nix-fun/index.html#windows",
    "href": "guides/nix-fun/index.html#windows",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "Firstly, open a powershell prompt to run these commands.\nwsl --install debian\nwsl --set-version debian 2\nYou may need to do a:\nwsl --update\nSince nix does not support linux, you need to install it in WSL.\nIn the microsoft store, you can install one of many linux distributions or versions. I recommmend debian\nYou then need to enable systemd, you can follow Microsoft’s official insturctions: https://devblogs.microsoft.com/commandline/systemd-support-is-now-available-in-wsl/\nOnce WSL2 with systemd is installed, you can follow the macos/linux install instructions.\nNow, nix is installed. You should be able to access the nix commands in your terminal. For example, the nix-shell command can be used to create a temporary shell environment with packages, environment variables, and more. For examples, see below."
  },
  {
    "objectID": "guides/nix-fun/index.html#installing-home-manager",
    "href": "guides/nix-fun/index.html#installing-home-manager",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "Home manager is a way to declaratively manage a user environment, including packages installed, configuration files, and environment variables. It uses nix as the langauge of configuration.\nTo install home manager, you can follow the steps from their docs\nTo use home-manager, you can edit the configuration file located at $HOME/.config/home-manager/home.nix For an example, see my my blog post about this"
  },
  {
    "objectID": "guides/nix-fun/index.html#connecting-your-github-account-to-git-from-the-terminal",
    "href": "guides/nix-fun/index.html#connecting-your-github-account-to-git-from-the-terminal",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "Connecting your github account to git from the terminal",
    "text": "Connecting your github account to git from the terminal\nFirst, make sure you have git installed, but you probably do already, if you are here.\nThen: nix-shell -p gh. This installs the github cli tool.\ngh auth login # logs you into github\ngh setup-git And with this, git is configured to use the github cli as a credential helper\ngit config --global credential.helper store with this, git stores the credentials permanently.\nNow, while still in the nix environment:\ngit push while in the local copy of a repo you have stored in github.\nCheck the /home/yourusername/.git-credentials file to make sure your git credentials are stored.\nAnd then you should be good to exit the nix environment. Because your credentials are saved, you can now run git push from the command line."
  },
  {
    "objectID": "guides/nix-fun/index.html#deleting-sensitive-detail-or-large-binary-files-from-a-git-repo",
    "href": "guides/nix-fun/index.html#deleting-sensitive-detail-or-large-binary-files-from-a-git-repo",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "Deleting sensitive detail, or large binary files from a git repo",
    "text": "Deleting sensitive detail, or large binary files from a git repo\nGit tracks every change. So if you store something like an image, or a binary in a git repo, if you delete those files later, they will continue to eat space and be wasteful, even if you commit the deletions later on. Or, if you have sensitive data, like passwords or api keys in a repo, even if you delete them in later commits, they will still be present.\nTo alter every past commit, you can use special tools, which are very easy to install using nix.\nnix-shell -p git-filter-repo\nFrom here, you can use the git-filter-repo command to nuke files or folders:\nFirst, cd into your git repo.\ngit-filter-repo --invert-paths --paths path/or/file. git-filter-repo works by only taking anything that matches an expression, so by inverting that, it takes everything except what matches that path.\nAlternatively, the bfg-repo-cleaner tool can be used. I did not opt for this to clean out the images of my git repo, because it doesn’t seem to be able to delete entire paths, or even individual files, only matching filenames, or doing text replacement. However, apparently, it is much faster than git-filter-repo for large repos, due to a different implementation.\nTo replace sensitive data with bfg:\nnix-shell -p bfg-repo-cleaner\nbfg-repo-cleaner --replace-text passwords.txt\nWhere passwords.txt contains data you want to replace.\n\n\npasswords.txt\n\nsecretapikey ==&gt; ***REMOVED*** is the default if you don't have an arrow\nsecretpassword ==&gt; but you can replace it with anything\nglob:*baddata* ==&gt; glob matches work too\n\nTo push the changes to github, or your remote repository:\ngit push --force --all\nAnd then it should be done. This should be seamless, but if you encounter any hiccups, like I did with slow internet speeds causing it to fail, there are some commands you can run:\ngit gc --aggressive optimizes the repository\ngit config --global http.postBuffer 524288000 if you are on a slower network (high latency), this gives it more grace.\nBut for me, because I am currently on vacation and didn’t have access to the internet speeds I do at home, the only thing that worked was actually getting up and moving my laptop to an area with faster wifi.\nNow, I did this to delete unused images from my repo, however, if you are trying to clean sensitive data off of the internet, there are some extra steps you may want to take."
  },
  {
    "objectID": "guides/nix-fun/index.html#renaming-flash-drives",
    "href": "guides/nix-fun/index.html#renaming-flash-drives",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "Renaming flash drives",
    "text": "Renaming flash drives\nnix-shell -p exfatprogs install utilities to manipulate the exfat file system used by usb flash drives.\nThen:\nsudo extfatlabel &lt;device&gt; &lt;new label&gt;\nYou can see the device with the lsblk tool, which lists all attached drives."
  },
  {
    "objectID": "guides/nix-fun/index.html#installing-packages",
    "href": "guides/nix-fun/index.html#installing-packages",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "Installing packages",
    "text": "Installing packages\nnix profile install nixpkgs#packagename will install a package.\nIf you need a specific version of a package, then you can use Lazamar’s site, or nixhub to search for old revisions of the git repo.\nAnd then:\nnix profile install nixpkgs/revisionhash#packagename"
  },
  {
    "objectID": "guides/tmux/index.html",
    "href": "guides/tmux/index.html",
    "title": "Tmux",
    "section": "",
    "text": "Tmux is what’s called a terminal multiplexer. Essentially, it lets you split one terminal window, into multiple similar to how a your desktop interface lets you use multiple windows at once. It has more features than that, but the multiplexing is what this article is going to focus on."
  },
  {
    "objectID": "guides/tmux/index.html#window-control",
    "href": "guides/tmux/index.html#window-control",
    "title": "Tmux",
    "section": "Window control",
    "text": "Window control\nTmux’s most basic and essential feature is creating more windows. First, input the prefix key (default is control + b), and then another key to tell tmux to do something:\n\n\n\nkey\naction\n\n\n\n\nc\ncreate new window\n\n\np\nprevious window\n\n\nn\nnext window\n\n\n0, 1, 2, …\ngo to specific numbered window\n\n\n&\nkill window"
  },
  {
    "objectID": "guides/tmux/index.html#copyscroll-mode",
    "href": "guides/tmux/index.html#copyscroll-mode",
    "title": "Tmux",
    "section": "Copy/Scroll mode",
    "text": "Copy/Scroll mode\nBecause you can’t scroll normally in tmux, you have to enter a special scroll mode with prefix + [. In this scroll mode, you can use q to quit, Control S to search down, and Control R to search upwards. After searching, n goes to the next item, and Shift+n goes to the previous one. q exits search."
  },
  {
    "objectID": "guides/tmux/index.html#attatchdetatch",
    "href": "guides/tmux/index.html#attatchdetatch",
    "title": "Tmux",
    "section": "Attatch/Detatch",
    "text": "Attatch/Detatch\nD detaches.\nYou can list active sessions with tmux ls\ntmux attach -t [sessionnumber] to reattach. Or tmux a for a shorter verb."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Jeffrey Fonseca",
    "section": "",
    "text": "Over the summer before my freshman year of high school, I took a course at Palomar college, CSNT 110, hardware and OS fundamentals.\nI first learned about the internals of computers during a summer course I took at Palomar college online before my freshman year of highschool — CSNT 110/ OS and Hardware Fundamentals.\nDuring my freshman year, when my laptop starting slowing down, I switched to a Linux based operating system, in an attempt to make it faster, using knowledge I had obtained from that course.\nSince then, I have been tinkering, playing with software, and doing personal projects. However, when I was taking AP Computer Science Principles, during my senior year of high school, my teacher criticized me for being a maverick. I didn’t bother with the group projects, working on my own projects, although I contributed back to the group often, but I didn’t document what I do."
  },
  {
    "objectID": "index.html#technical",
    "href": "index.html#technical",
    "title": "Hi, I’m Jeffrey Fonseca",
    "section": "Technical",
    "text": "Technical\nI used quarto to create this static site. The source code can be found at https://github.com/moonpiedumplings/moonpiedumplings.github.io\nFrom my testing, only 3 things on this site require JavaScript, and everything else can be rendered without JavaScript:\n\nThe fulltext website search that quarto provides\nDark theme. You can’t switch off from light theme when JavaScript is blocked from running.\nThe in browser python code blocks on the interactive, in browser python article require JavaScript/WebAssembly to run."
  },
  {
    "objectID": "playground/arch-secureboot/index.html",
    "href": "playground/arch-secureboot/index.html",
    "title": "Attempting to get secure boot and bootable BTRFS snapshots",
    "section": "",
    "text": "If these steps are followed blindly, you risk compromising your security\n\n\n\n\n\nDo not do what I did unless you have a similar setup to what I have, which is full disk encryption, with /boot/efi being the efi system partition, and the only unencrypted part of the system. This means that everything in /boot, including things like kernels, or grub configurations and supportive files.\n\n\n\nI’ve been at this for some time now, so I will document what I have done, and further steps. Essentially, I am trying to do what this person archive has done.\nThey go more into depth on the setup that we are both using, but essentially, I the only unencrypted part of my system is the grub efi binary. Normally, the kernels and kernel modules are also left unencrypted, and because of that, they must be signed in order to prevent an attacker from modifying them on the hard drive, either by removing the hard drive, or booting another operating system, like by a USB drive. Signing is a process that uses complex math to ensure that the computer can verify the authenticity of a file.\nNormally, since the kernels are left unencrypted (and to cut off another avenue of attack on running systems), the kernels are signed in order to ensure that they haven’t been modified. However, in my setup, since only grub is left unencrypted, it’s the only thing that can be signed.\nBut for some reason, grub still attempts to verify the kernels even when it doesn’t need to. This is my struggle of getting it to not do that.\nThe first thing I did sign grub. I followed the insturctions on the Arch Wiki. Except, rather than signing the kernels, I only signed grub.\nI enabled secure boot, and reboted. Grub loaded up, and asked me for my password, but then gave me an error and put me into a rescue shell.\nerror: prohibited by secure boot policy\nSo I did some searching around. I found someone with the same issue on the Arch Linux BBS. On that page, someone (reply #4) said that an update of grub (grub version 2.06, to be specific) had a change in policy where grub would refuse to load unsigned font files when in secure mode. This is good. There have been CVE’s (security vulnerabilities/issues) in the past where grub loading a malicious grub.cfg or font file can be exploited to make grub do things it’s not supposed to.\nExcept one tiny problem: Grub stores all configuration and font files in /boot/grub, which is encrypted in my setup. An attacker would not be able to modify it, so therefore I have no need to verify it. It’s actually a hassle to verify all this stuff, because they are tracked by BTRFS, and will be restored.\nAfter some more research, I decided to install grub with the shim-lock option.\nBut I got another error:\nerror: verification requested, but nobody cares: (cryptouid:uidhere/boot/grub/x86_64-efi/normal.mod)\nNormal mod was not signed.\nApparently, as long as secure boot it enabled, grub still attempts to verify the files. I did some searching around, before finding this article, which I also linked above. They used sed (text finder and replacer) to replace some things in the grub binary.\nsed 's/SecureBoot/SecureB00t' grubx64.efi\nExcept, after I ran this command:\n[root@lizard boot]# sbctl verify\nVerifying file database and EFI images in /boot/efi...\n✓ /boot/efi/EFI/cachyos/grubx64.efi is signed\nThe grub binary stayed signed. Even after modification. What? Shouldn’t the grub binary being modified, break the signatures? Unless it does (I haven’t rebooted yet), and it’s just that the the sbctl tool doesn’t check whether or not the signatures are valid.\nRegardless, I can’t know until I test, and I didn’t bother. I simmply resigned the the grub binary, but it still didn’t work.\nI forgot to install grub with the --disable-shim-lock option.\nBefore this, I nuked the /boot/efi directory to ensure that nothing extraneous was left. For some reason, cachyos (arch based distro I am using), installs grub twice, once at /boot/efi/EFI/boot and another at /boot/efi/EFI/cachyos. Having only one grub binary further simplifies this process.\nSo my full steps would be (requires root):\ngrub-install --target=x86_64-efi --efi-directory=/boot/efi --disable-shim-lock\nsed -i 's/boot/efi/EFI/cachyos/' grubx64.efi\nAn unorthodox grub hack, but it works.\nIf a grub update ever breaks this, or maybe just to futureproof this, then I’ll probably just use Arch’s PKGBUILD, makepkg, and patch tools to patch the grub_efi_get_secureboot function of sb.c so that grub always thinks it’s not in secure boot. I think I would only need to change one line, return secureboot, or maybe reset the variable to what I want at the end of the function, but I’m not too familier with C code at the moment. I can read it, given context, but I’ve not written it yet."
  },
  {
    "objectID": "playground/crypto-scam/index.html",
    "href": "playground/crypto-scam/index.html",
    "title": "A very clever crypto scam",
    "section": "",
    "text": "So I was browsing around, and I found this public pastebin.\n\nklovenierm6@193.233.202.76 where the password is bDBShj\nIt seemed like a crypto wallet reset password? But why post this in a pastebin? It seemed suspicious, but I decided to log in.\n\nWow. $10000 worth of Bitcoin, right there for me to take. It seems to good to be true.\nSo, I spun up a wallet and attempted to deposit the money into it.\n\nWow. In order to withdraw the money first, to an unverified account, I first have to deposit around $2000 worth of Bitcoin into their account.\nObviously, this is a scam. It’s a fairly common pattern, “Here’s free money, but first, send me some money.” But this is such an interesting technological twist, I couldn’t not blog about it.\nBut I decided to experiment a little further.\n~ ❯ nmap -sV 193.233.202.76\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-06-11 12:46 PDT\nNmap scan report for vm.lan (193.233.202.76)\nHost is up (0.21s latency).\nNot shown: 995 closed tcp ports (conn-refused)\nPORT      STATE    SERVICE       VERSION\n22/tcp    open     ssh           OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n2222/tcp  open     ssh           OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n5900/tcp  filtered vnc\n5901/tcp  filtered vnc-1\n16992/tcp filtered amt-soap-http\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\nNmap is a port scanning utility that tells you what people are running on their server.\nSo I attempted to connect to their other services. I tried to connect to both vnc servers first, but for both, I got a server not found error. This is probably because those services aren’t up for public usage, as shown by the filtered state they return in the nmap scan. I suspect that they have a firewall that only allows for certain IP addresses or something like that.\nI also tried to connect to the ssh service on port 2222, but it kicked me out saying I needed a public key.\nOooh, a scan later in the day (around 4-5 hours later) is differerent.\n~ ❯ nmap -sV 193.233.202.76\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-06-11 22:00 PDT\nNmap scan report for vm.lan (193.233.202.76)\nHost is up (0.21s latency).\nNot shown: 996 closed tcp ports (conn-refused)\nPORT     STATE    SERVICE VERSION\n22/tcp   open     ssh     OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n2222/tcp open     ssh     OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n5900/tcp filtered vnc\n5901/tcp filtered vnc-1\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 32.93 seconds\nThere appear to have taken the amt-soap-http service offline. Good security protocol, getting rid of uneeded services, minimizing the attack surface, but do they really need those vnc servers?\nI decided to do a little more searching.\nI first searched google, and only the same pastebin came up, which has since been taken offline.\nI did find something interesting by searching on github:\n\nBut upon going through that repo, it was simply someone’s pastebin archiving repo. And in addition to that, the servers in those that I found, I have since been taken offline."
  },
  {
    "objectID": "playground/dorking/index.html",
    "href": "playground/dorking/index.html",
    "title": "Dorking around",
    "section": "",
    "text": "So I recently saw Maia Crimew’s (in?)famous blog post, How to completely own an airline in 3 easy steps. In this, she describes how she managed to get ahold of confedential data from major airlines, using little technological knowlege, or actual hacking.\nSh used the zoomeye internet search engine, somewhat an analogue of google, except rather than searching all websites, it searches all internet connected devices — including insecure ones.\nI was inspired by her, to do my own “dorking,” or search engine based hacking, trying to find vulnerable public services. She searched for public jenkins servers, which is a build and deployment system, that seems to be able to leak secrets if not configured correctly.\nI decided to search for something even easier. I searched for xterm, a browser based terminal. It took some tinkering with the terms, but eventually I found some publicly exposed servers. People had left an xterm session running, sometimes even with root privileges enabled.\nI decided to search further. I searched guacamole, a web based connector to remote desktop protocol sessions to see if anyone had left any exposed."
  },
  {
    "objectID": "playground/flashcards/test3.html#concept-here",
    "href": "playground/flashcards/test3.html#concept-here",
    "title": "Jeffrey Fonseca",
    "section": "Concept here",
    "text": "Concept here\n\nBullet 1\nbullet 2\nbullet 3"
  },
  {
    "objectID": "playground/flashcards/test3.html#concept-2-here",
    "href": "playground/flashcards/test3.html#concept-2-here",
    "title": "Jeffrey Fonseca",
    "section": "Concept 2 Here",
    "text": "Concept 2 Here\nShort paragraph with details. Ipsum loereum and stuff.\nTest spacing."
  },
  {
    "objectID": "playground/flashcards/test3.html#content-3",
    "href": "playground/flashcards/test3.html#content-3",
    "title": "Jeffrey Fonseca",
    "section": "Content 3",
    "text": "Content 3\n\nBullet 1\nBullet 2\nBullet 3"
  },
  {
    "objectID": "playground/ova-automation/index.html",
    "href": "playground/ova-automation/index.html",
    "title": "Automatically provisioning VMs from OVAs",
    "section": "",
    "text": "(see the “Goals and Context” section for more context)\nWasabi, head of black (operations and deployment) team sent out a message, after the first qualifiers:\n\n\nI can’t remember, but you guys only recently started playing I believe I had a conversation with other schools on this.. But there’s a huge separation from teams in terms of practice\n\n\nSo my recommendation to all the new teams and all the teams who struggled this year. Don’t use https://archive.wrccdc.org/images/ except as reference. Take our environment and topology and build it yourselves. Spend the time to learn and help each other learn how systems work and how to troubleshoot things. This will help you a ton. The teams in the top do this. If your one of the teams not making it forward please feel free to send me a message for advice and use Q&A channel to get help from Ops / Red / White team for your off season. We are happy to help and excited to see you back next year!\n\nMy original goal of this project was to be able to quickly recreate WRCCDC environments, so we could run mock competitions, since currently we were setting them up manually.\nBut even the 3 or 4 mocks we ran didn’t help us. Ater actually doing first real competition, I agree with Wasabi on this.\nSo I don’t think this project is worth continuing. It would be better having people recreating the competition environments, and really using Linux, Windows Server, and the various firewalls so we have an understanding of how they work.\nSo yeah. The work is still valuable, it just doesn’t get to stay in the “projects” section of my blog — the difference between “playground” and “projects” is not topics, but rather the commmitment I have to them. Projects I will see to either the end, or until I hit significant failure that I can’t overcome. The content in playground, I am free to give up on it at any point, for any reason, and because of that, many of the miniprojects in there are incomplete."
  },
  {
    "objectID": "playground/ova-automation/index.html#proxmox",
    "href": "playground/ova-automation/index.html#proxmox",
    "title": "Automatically provisioning VMs from OVAs",
    "section": "Proxmox",
    "text": "Proxmox\nI’ve created a folder with a shell.nix and a Vagrantfile, in order to automate the creation of a proxmox virtual machine. It’s located inside this git repo, and by extension, this static site.\nDocs to import OVA files on proxmox: https://pve.proxmox.com/wiki/Migration_of_servers_to_Proxmox_VE#Importing\nAfter some testing with that, importovf isn’t too good, because it imports the virtual machine metadata, without any of the actual data. So things we don’t really need to store, like how many vcpu’s or memory, or the network, but none of the actual disk data.\nYou have to run qm importdisk (an alias for qm disk import)\nSo after extracting the ova, you have to extract the vmdk (it’s gzipped), and then it would be something like:\nqm importdisk numberhere image.vmdk --format qcow2|vmware\nThe manpage for qm says that –format specifies the target format, so I’m assuming it’s the format to convert to."
  },
  {
    "objectID": "playground/ova-automation/index.html#lxdincus",
    "href": "playground/ova-automation/index.html#lxdincus",
    "title": "Automatically provisioning VMs from OVAs",
    "section": "LXD/Incus",
    "text": "LXD/Incus\nLXD is a “containervisor”, capable of managing both lxc containers and qemu-kvm virtual machinwees.\nFor this usecase, I only need it’s hypervisor management capibilities, but I also want to the automation capabilities that it may offer, since that seems to be an often criticized lacking feature of proxmox.\nI also tried out incus. I put together an ansible playbook to install incus on debian.\nHowever, I am having trouble with their official web ui. Authentication is certificate based, so it asks for me to import a browser certificate… not fun.\nI am going to collect and test multiple alternatives:\n\nOffical Canonical One\nLXDWare\nhttps://github.com/AdaptiveScale/lxdui\nhttps://github.com/PenningLabs/lxconsole still in beta, according to the readme\n\nI tried lxdware, but when I tried to add a remote, and connect it to the incus daemon, it I get an error “Remote host connection is not trusted”.\nI do some investiagting.\nroot@f6ded1cb73d7:/# incus remote add 192.168.121.103\nGenerating a client certificate. This may take a minute...\nCertificate fingerprint: 7cf0d7d12fed498811e485d8dec4655012873876e9f45e15974cfdb9a8fd810a\nok (y/n/[fingerprint])? y\nTrust token for 192.168.121.103: eyJjbGllbnRfbmFtZSI6ImluY3VzLXVpLTE5Mi4xNjguMTIxLjEwMy5jcnQiLCJmaW5nZXJwcmludCI6IjdjZjBkN2QxMmZlZDQ5ODgxMWU0ODVkOGRlYzQ2NTUwMTI4NzM4NzZlOWY0NWUxNTk3NGNmZGI5YThmZDgxMGEiLCJhZGRyZXNzZXMiOlsiMTkyLjE2OC4xMjEuMTAzOjg0NDMiXSwic2VjcmV0IjoiMjRjNWNjZjkzZTgzMTBmNGRlMzAwMTgyOTc0YWE4Nzg1MDAxMTkzZWQ3NTEyYTk0ZjlmZGRkZWQwMzBkNWJkZSIsImV4cGlyZXNfYXQiOiIwMDAxLTAxLTAxVDAwOjAwOjAwWiJ9\nClient certificate now trusted by server: 192.168.121.103\nroot@d90da14b48e9:/# lxc remote add 192.168.121.103\nAdmin password (or token) for 192.168.121.103:\nError: not authorized\nIn short, it seems like the api is not compatible, since using the lxc client fails, but the incus client succeeds.\nLuckily, I got incus working. In addition to the normal steps, I had to run incus config trust add-certificate certficatefile.crt\nTesting, I was able to download a debian image, and put it up.\nIncus looks very promising, because it also comes with a terraform provider\nIncus also provides docs on migrating machines (canonical lxd version)\nOn that page, it recommends using virt-v2v, which has support for importing from vmware ova’s\n(It seems it requires a newer version of virt-v2v, and rhsrvany, which only debian sid and ubuntu 23 package both right now.)\nOnce you use that tool, you can use incus-migrate to interactively or programmaticly import the raw disk images as incus images.\nSo that’s pretty promising. But what about networking? For a replica of the competition environment, we will need to have firewalls (vyos, pfsense, etc) and to my understanding, that is essentially having a virtual machine act as a bridge. How can I do that on LXD/Incus?\nIt seems to be possible. Here are some guides I found:\nhttps://www.cloudwizard.nl/build-your-own-windows-cloud-with-lxd/\nhttps://forum.netgate.com/topic/154906/how-to-install-pfsense-on-lxc-vm-qemu"
  },
  {
    "objectID": "playground/resume/index.html",
    "href": "playground/resume/index.html",
    "title": "Can I write my resume in python?",
    "section": "",
    "text": "Note\n\n\n\nI’ve changed the python blocks to not auto execute, because the github actions environment does not seem to be able to render them.\n\n\nI am trying to generate a two column resume, but nothign I have tried has been able to output to both pdf and html format. So I am now trying to generate a resume using jupyter and pythons graph generating capabilities.\nI found an example online, but it has syntax errors when I try to run it, and I also don’t know what the code is liscensed under, so I think I will try to write my own.\nFirst, I need to select a graphics library. They used matplotlib, but I will search for an easier way to do it first.\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\n\ndef plot_func(freq):\n    x = np.linspace(0, 2*np.pi)\n    y = np.sin(x * freq)\n    plt.plot(x, y)\n\ninteract(plot_func, freq = widgets.FloatSlider(value=7.5,\n                                               min=1,\n                                               max=5.0,\n                                               step=0.5))\nI found another guide to write a resume with html, and then convert it to pdf with python, but I don’t really want that.\n#| column: body-outset\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\n\n# Configuring the graph\n%matplotlib inline\nplt.figure(figsize=(10,6))\nplt.axis('off')\n\n\n\n# Test text\nTest = \"test\"\nplt.annotate(Test, (.02,.98), weight='regular', fontsize=8, alpha=.75)\n\n# Border lines\n#plt.axvline(x=.99, color='#000000', alpha=0.5, linewidth=300)\nbqplot instead:\nimport bqplot.pyplot as plt\nimport numpy as np\n\nfig = plt.figure(title=\"Sine\")\n\n# create data vectors\nx = np.linspace(-10, 10, 200)\ny = np.sin(x)\n\n# create line mark\nline = plt.plot(x, y)\n\n# renders the figure in the output cell (with toolbar for panzoom, save etc.)\nplt.show()\nThis apparently requires some libraries:\nplayground/resume/bqplot.js (404: Not Found)\n  /playground/resume/@jupyter-widgets/1551f4f60c37af51121f.woff2 (404: Not Found)\n  /playground/resume/@jupyter-widgets/eeccf4f66002c6f2ba24.woff (404: Not Found)\n  /playground/resume/@jupyter-widgets/be9ee23c0c6390141475.ttf (404: Not Found)\n  /playground/resume/@jupyter-widgets/html-manager/dist/1551f4f60c37af51121f.woff2 (404: Not Found)\n  /playground/resume/@jupyter-widgets/html-manager/dist/eeccf4f66002c6f2ba24.woff (404: Not Found)\n  /playground/resume/@jupyter-widgets/html-manager/dist/be9ee23c0c6390141475.ttf (404: Not Found)"
  },
  {
    "objectID": "projects/build-server/index.html",
    "href": "projects/build-server/index.html",
    "title": "Building my own Server Part 1 — Hardware",
    "section": "",
    "text": "I wanted a machine for experimenting with devops and deep learning. That means plenty of ram, cpu, and a modest gpu.\nI was very careful with my selection, and here is what I ended up with:\n\nDell Precision Tower Server 7910, with 2X intel xeon E5 2687 v4. Came with 32 GB of ram.\nMore memory, when added, I will get a total of 128 GB of ECC memory.\nNvidia rtx 4070 GPU, 12 GB vram.\n\n\n\n\nHere is the inside of my server:\n\nAnyway, I need to figure out where to put the GPU. The computer has several PCIe slots, and I want the fastest one.\nUp top:\n\nAnd below:\n\nI need to figure out what each of these pcie ports is. What do the numbers and color mean?\nTo catalog them:\n\nThree black PCIe3x16 slots (all 75W)\nOne blue PCIe3x16 75W + ext 225 w\nOne black PCIe2x4 25 W\nOne tan PCI slot\n\nI think it is safe to assume that the color is related to wattage, not PCIe protocol. Since the computer comes with the necessary power plug that the nvidia gpu wants, it is safe to assume that any of the PCIe3x16 are optimal, although I will try to place the gpu in the best spot for cooling.\nI found a forum post (wayback archive) where someone asked this exact question.\nAn unsourced answer replied that the blue pcie slot was the primary gpu slot, so since I only have one gpu, that is where I put it.\nI found the manual for my system online, but it doesn’t seem to label each pci port in images.\nI also did research into some youtube videos.\nGeneral review\nMemory upgrading process\nThe important thing to note about the memory upgrading process is that the memory shroud (cover) does not interfere with other processes.\nAnyway, I also did more research, attempting to find more manuals and whatnot.\nLinked on this page I found a proper manual which gave visual instructions on how to do things like remove the memory shroud.\n\n\n\nHowever, I am trying to add the gpu, so I can get monitor output, so I can run testing suites to ensure the computer is in proper working order.\nThe GPU, is inserted:\n\n\nAfter plugging the gpu power in:\n\nI can turn the machine on and get monitor output:\n\nNext step is to run the system diagnostics on the memory I currently have. Thankfully, I have an extra keyboard I can connect it to.\n\n\n\nNow that I had the GPU installed I could get video output and see what the BIOS is saying. I installed two hard drives, and got an error: “Alert Hard drive not found”.\nI decided to pause on the hard drives, and run the built in memory test. The memory, and cpu tests ran without error, however, I got an error about not being able to find the hard drives\n\nJust to make sure that the issue was a hardware issue, and not the BIOS merely complaining about not seeing a bootable device, I booted into a live USB and ran lsblk:\n\nI messed around with moving around the hard drive positions, unplugging and plugging in cables, no dice. This issue is definitely at the hardware level, however, and there are several causes:\n\nBad cable. I only have a single cable, an SAS connector on both sides.\nNo power to the hard drives. I will test to see if they light up later.\nMotherboard not working. Maybe the motherboard plug isn’t working, but I tried both of them…\n\nIt’s definitely not defective hard drives, as this happens with both of the hard drives I am testing with, both of which are new. However, they rely on the same cable and same power supply (they attatch to a seperate board which has a cable connecting that to the motherboard and another connecting it to the power supply)\nUpon doing some further research, I found a related youtube video. According to this video, the port I am trying to attatch this to is controlled by the RAID controller, so RAID must be enabled, although I can use RAID 0 for RAID without any of the special RAID features or complications.\nI enabled the RAID controller, and now the BIOS can see the hard drives:\n\nHowever, when I actually inspect these drives via a live usb, they don’t appear to be mountable properly.\n\nIt sees the hard drives as 0 byte, empty drives, which I can’t do anything.\nI need to get into the raid controller bios.\nI found a relevant manual which says that when booting, pressing Control + I to enter the raid configuration screen.\nI am wondering if there is an option to bypass raid entirely, as I have no need for any of it’s features, and striped will limit me to the size of the smallest disk.\nAlright, since I skimmed the manual, I missed something. There are actually three different ways to get to the raid controller interface, depending on what raid controller you have installed. I tested all methods, but the only one that worked was entering the boot options menu, and then going to device configuration.\nThen, I was met with a screen like this:\n\nI browsed around a little bit, and although it could see the physical drives, there was no option to create a volume or do anything with the disks. I think this is because I have two different disks, an nvme SSD, and a sata HDD.\nThis is frustrating. I (might) need to buy another disk, or buy parts to make the disks work via a SATA connection.\nI did more research and found a relevant support article (wayback machine) detailing how to use the raid controller and it seems I missed something. It’s seems that creating volume is not in the drives section, but rather in controller configuration.\nApparently, there is also a command line software designed to work with this raid controller (that works on linux, of course).\nManual (wayback machine)\nRelevant support page by broadcom\nDownloads by IBM\nGuides by Huawei\nBut I don’t really want to use raid. I just want to pass through the disks, or something like that. Upon doing some research, I might be able to put the card into Host Bus Adapter (HBA) mode. In this mode, it will just pass through the disks.\nBased on some precursory research, this may not be an options in the settings that it comes with, and I may need to flash the firmware (wayback link)\nFirmware flashing is scary. Unlike playing with operating systems, if done wrong, firmware flashing can brick (make useless) devices. So I will have to research very carefully.\nSo first, what is this “IT” mode I keep seeing when I research how to passthrough devices. IT mode stands for “initiator target” and it presents each drive individually to the host.\n“IR” stands for integrated RAID, when raid is integrated into the motherboard. I don’t have this.\nAnd finally, “HBA” stands for host bus adapter mode. Devices that do this (they can be more than just drive connectors) connect any other kind of device to the motherboard directly.\nSo I have to find the relevant “IT” firmware.\nBut first, I need out exactly what device I have.\nAccording to the output of lspci, the controller is: 01:00.0 Serial Attached SCSI controller: Broadcom / LSI SAS3008 PCI-Express Fusion-MPT SAS-3 (rev 02)\nI found a relevant youtube video which applies to my exact server model (T7910), but it doesn’t list where to find the files, and the flashing utility.\nBased on some research, it seems that the SAS 3008 and the SAS 9300 HBA have the same hardware, just differing firmware. According to one article, “Supermicro LSI SAS3008 HBAs (which share the same controller as the LSI 9300-8i HBAs)”.\nThe drivers for this can be found on the broadcom website\nI will need to do research into what the “phases” are. I think they are releases/versions? Anyway, I downloaded the P15 version, and I went through it. It appears to contain all the rom files that will be flashed to the device.\n\nSAS9300_8i_IT.bin\nmptsas3.rom\nmpt3x64.rom\n\nHowever, this zip archive appears to only contain the windows flashing utilities. I need to find the linux flashing utility.\nAlright, I’ve located the sas3flash binaries: broadcom website. Here, there are linux sas3flash binaries, as well as uefi binaries, which people seem to like, but I don’t really want to use as I don’t want to have to learn how to interact with the UEFI console, and attatched disks, and whatnot. I’d rather use the familer linux command line interface and tools.\nBased on looking at sas3flash -help, I should be able to use the same commands as the uefi binary, following the instructions on the flash the firmware guide linked above.\nI booted from a live usb, and attempted to flash the rom.\nOn the first step of flashing the firmware, wiping it, I get an error.\n./sas3flash -c 0 -o -e 6 -l log7 \n    Adapter Selected is a Avago SAS: SAS3008(C0)\n\n    Executing Operation: Erase Flash\n\n    Erasing Flash Region...\n\n    ERROR: Erase Flash Operation Failed!\n\n    Resetting Adapter...\n    Reset Successful!\n\n    Due to Exception Command not Executed. IOCStatus=0x47ca, IOCLogInfo=0x0\n    Finished Processing Commands Successfully.\n    Exiting SAS3Flash.\nI suspect this error is related to me not messing around with the jumper cables before hand, but I can’t find them.\nI also can’t flash the IT firmware, and I am assuming this is because it requires that you wipe the firmware first.\n./sas3flash -o -f SAS9300_8i_IT.bin -b mptsas3.rom -b mpt3x64.rom -l flashing.log \n    Adapter Selected is a Avago SAS: SAS3008(C0)\n\n    Executing Operation: Flash Firmware Image\n\n        Firmware Image has a Valid Checksum. \n        Firmware Version 16.00.10.00\n        Firmware Image compatible with Controller. \n\n        Valid NVDATA Image found. \n        NVDATA Major Version 0e.01 \n        Checking for a compatible NVData image... \n\n        NVDATA Device ID and Chip Revision match verified.\n        NVDATA Versions Compatible.\n        Valid Initialization Image verified.\n        Valid BootLoader Image verified.\n\n        ERROR: Cannot Flash IT Firmware over IR Firmware!\n\n        Firmware Image Validation Failed! \n\n    Due to error remaining commands will not be executed.\n    Unable to Process Commands.\n    Exiting SAS3Flash.\nHowever, after doing some research, it appears that only the dos and efi versions of sas3flash are able to flash IT firmware over IR firmware, according to the official broadcom website. ON that site, there is a compatibility table, meaning I need to use the either the efi version, or the dos version. I should have noticed how the existing guides used either the efi version or the dos version.\nSo I set up a USB with the efi version of the flasher, and I attempted to get into the boot menu… only for it not to work. I press F12, or F8 then F12The screen says something along the lines of “Entering the one time boot configuration”, before taking me to the “cannot find hard drive” screen. I have no idea why it’s not working, as it has worked before.\nAccording to one forum post, the issue has something to do with the CD Rom drive, and I should try removing it. However, that question and suggestion is relating to laptops, not workstations, so it might not work for me.\nOkay, I figured out what the issue was. I had unplugged the drives, but when I plugged them back in, they were not seated properly, and that made them get confused.\nNow that I could get to the boot menu, I could boot my sas3flash efi file. Except I couldn’t. It simply errored. I attempted to boot on it on my laptop to make sure that it was not an issue with the desktop and not the efi setup, and it errored there as well.\nHowever, after I booted to a linux distro, I found that I could see the drives, independent of the raid controller. For whatever, reason, my flashing appears to have worked, even if it didn’t. Now that I have drives installed, I can finally begin to set up the software.\n\n\n\nThis comes with two 16 gb sticks, one in each cpu.\nI bought 6 more 16 GB ecc sticks, so I will have a total of 128 GB of memory.\nIn order to ensure that dual channel works, I will need to put them in the right spots. Now, the previous manual that I found has a chart detailing on the channel configuration, but that chart is very complex. Lower down in the manual, in the “errors and warnings” section, there is a error that could appear that makes things very clear to me.\n“Warning: Non-optimal memory population detected. For increased memory bandwidth populate DIMM connectors with white latches before those with black latches.”\nThat makes things very simple for me.\nI added the ram. And now my system won’t boot at all. I don’t get any monitor output. I asked around, and someone mentioned that it could be dust in the ram socket cores, so I will have to take them out, blow the dust out, and then reseat the ram.\nThe same user helping me also said that it might be an issue with extreme memory profiles (xmp), a feature that adjusts how memory works on intel cpus, and that I should disable it from a working bios.\nOkay, I figured out what the issue was. The issue was that I did not seat the memory properly.\nOnce I had set the memory, I ran the memory test built in to the bios, and after some time it finished without any errors."
  },
  {
    "objectID": "projects/build-server/index.html#selection",
    "href": "projects/build-server/index.html#selection",
    "title": "Building my own Server Part 1 — Hardware",
    "section": "",
    "text": "I wanted a machine for experimenting with devops and deep learning. That means plenty of ram, cpu, and a modest gpu.\nI was very careful with my selection, and here is what I ended up with:\n\nDell Precision Tower Server 7910, with 2X intel xeon E5 2687 v4. Came with 32 GB of ram.\nMore memory, when added, I will get a total of 128 GB of ECC memory.\nNvidia rtx 4070 GPU, 12 GB vram."
  },
  {
    "objectID": "projects/build-server/index.html#building",
    "href": "projects/build-server/index.html#building",
    "title": "Building my own Server Part 1 — Hardware",
    "section": "",
    "text": "Here is the inside of my server:\n\nAnyway, I need to figure out where to put the GPU. The computer has several PCIe slots, and I want the fastest one.\nUp top:\n\nAnd below:\n\nI need to figure out what each of these pcie ports is. What do the numbers and color mean?\nTo catalog them:\n\nThree black PCIe3x16 slots (all 75W)\nOne blue PCIe3x16 75W + ext 225 w\nOne black PCIe2x4 25 W\nOne tan PCI slot\n\nI think it is safe to assume that the color is related to wattage, not PCIe protocol. Since the computer comes with the necessary power plug that the nvidia gpu wants, it is safe to assume that any of the PCIe3x16 are optimal, although I will try to place the gpu in the best spot for cooling.\nI found a forum post (wayback archive) where someone asked this exact question.\nAn unsourced answer replied that the blue pcie slot was the primary gpu slot, so since I only have one gpu, that is where I put it.\nI found the manual for my system online, but it doesn’t seem to label each pci port in images.\nI also did research into some youtube videos.\nGeneral review\nMemory upgrading process\nThe important thing to note about the memory upgrading process is that the memory shroud (cover) does not interfere with other processes.\nAnyway, I also did more research, attempting to find more manuals and whatnot.\nLinked on this page I found a proper manual which gave visual instructions on how to do things like remove the memory shroud."
  },
  {
    "objectID": "projects/build-server/index.html#gpu",
    "href": "projects/build-server/index.html#gpu",
    "title": "Building my own Server Part 1 — Hardware",
    "section": "",
    "text": "However, I am trying to add the gpu, so I can get monitor output, so I can run testing suites to ensure the computer is in proper working order.\nThe GPU, is inserted:\n\n\nAfter plugging the gpu power in:\n\nI can turn the machine on and get monitor output:\n\nNext step is to run the system diagnostics on the memory I currently have. Thankfully, I have an extra keyboard I can connect it to."
  },
  {
    "objectID": "projects/build-server/index.html#hard-drives",
    "href": "projects/build-server/index.html#hard-drives",
    "title": "Building my own Server Part 1 — Hardware",
    "section": "",
    "text": "Now that I had the GPU installed I could get video output and see what the BIOS is saying. I installed two hard drives, and got an error: “Alert Hard drive not found”.\nI decided to pause on the hard drives, and run the built in memory test. The memory, and cpu tests ran without error, however, I got an error about not being able to find the hard drives\n\nJust to make sure that the issue was a hardware issue, and not the BIOS merely complaining about not seeing a bootable device, I booted into a live USB and ran lsblk:\n\nI messed around with moving around the hard drive positions, unplugging and plugging in cables, no dice. This issue is definitely at the hardware level, however, and there are several causes:\n\nBad cable. I only have a single cable, an SAS connector on both sides.\nNo power to the hard drives. I will test to see if they light up later.\nMotherboard not working. Maybe the motherboard plug isn’t working, but I tried both of them…\n\nIt’s definitely not defective hard drives, as this happens with both of the hard drives I am testing with, both of which are new. However, they rely on the same cable and same power supply (they attatch to a seperate board which has a cable connecting that to the motherboard and another connecting it to the power supply)\nUpon doing some further research, I found a related youtube video. According to this video, the port I am trying to attatch this to is controlled by the RAID controller, so RAID must be enabled, although I can use RAID 0 for RAID without any of the special RAID features or complications.\nI enabled the RAID controller, and now the BIOS can see the hard drives:\n\nHowever, when I actually inspect these drives via a live usb, they don’t appear to be mountable properly.\n\nIt sees the hard drives as 0 byte, empty drives, which I can’t do anything.\nI need to get into the raid controller bios.\nI found a relevant manual which says that when booting, pressing Control + I to enter the raid configuration screen.\nI am wondering if there is an option to bypass raid entirely, as I have no need for any of it’s features, and striped will limit me to the size of the smallest disk.\nAlright, since I skimmed the manual, I missed something. There are actually three different ways to get to the raid controller interface, depending on what raid controller you have installed. I tested all methods, but the only one that worked was entering the boot options menu, and then going to device configuration.\nThen, I was met with a screen like this:\n\nI browsed around a little bit, and although it could see the physical drives, there was no option to create a volume or do anything with the disks. I think this is because I have two different disks, an nvme SSD, and a sata HDD.\nThis is frustrating. I (might) need to buy another disk, or buy parts to make the disks work via a SATA connection.\nI did more research and found a relevant support article (wayback machine) detailing how to use the raid controller and it seems I missed something. It’s seems that creating volume is not in the drives section, but rather in controller configuration.\nApparently, there is also a command line software designed to work with this raid controller (that works on linux, of course).\nManual (wayback machine)\nRelevant support page by broadcom\nDownloads by IBM\nGuides by Huawei\nBut I don’t really want to use raid. I just want to pass through the disks, or something like that. Upon doing some research, I might be able to put the card into Host Bus Adapter (HBA) mode. In this mode, it will just pass through the disks.\nBased on some precursory research, this may not be an options in the settings that it comes with, and I may need to flash the firmware (wayback link)\nFirmware flashing is scary. Unlike playing with operating systems, if done wrong, firmware flashing can brick (make useless) devices. So I will have to research very carefully.\nSo first, what is this “IT” mode I keep seeing when I research how to passthrough devices. IT mode stands for “initiator target” and it presents each drive individually to the host.\n“IR” stands for integrated RAID, when raid is integrated into the motherboard. I don’t have this.\nAnd finally, “HBA” stands for host bus adapter mode. Devices that do this (they can be more than just drive connectors) connect any other kind of device to the motherboard directly.\nSo I have to find the relevant “IT” firmware.\nBut first, I need out exactly what device I have.\nAccording to the output of lspci, the controller is: 01:00.0 Serial Attached SCSI controller: Broadcom / LSI SAS3008 PCI-Express Fusion-MPT SAS-3 (rev 02)\nI found a relevant youtube video which applies to my exact server model (T7910), but it doesn’t list where to find the files, and the flashing utility.\nBased on some research, it seems that the SAS 3008 and the SAS 9300 HBA have the same hardware, just differing firmware. According to one article, “Supermicro LSI SAS3008 HBAs (which share the same controller as the LSI 9300-8i HBAs)”.\nThe drivers for this can be found on the broadcom website\nI will need to do research into what the “phases” are. I think they are releases/versions? Anyway, I downloaded the P15 version, and I went through it. It appears to contain all the rom files that will be flashed to the device.\n\nSAS9300_8i_IT.bin\nmptsas3.rom\nmpt3x64.rom\n\nHowever, this zip archive appears to only contain the windows flashing utilities. I need to find the linux flashing utility.\nAlright, I’ve located the sas3flash binaries: broadcom website. Here, there are linux sas3flash binaries, as well as uefi binaries, which people seem to like, but I don’t really want to use as I don’t want to have to learn how to interact with the UEFI console, and attatched disks, and whatnot. I’d rather use the familer linux command line interface and tools.\nBased on looking at sas3flash -help, I should be able to use the same commands as the uefi binary, following the instructions on the flash the firmware guide linked above.\nI booted from a live usb, and attempted to flash the rom.\nOn the first step of flashing the firmware, wiping it, I get an error.\n./sas3flash -c 0 -o -e 6 -l log7 \n    Adapter Selected is a Avago SAS: SAS3008(C0)\n\n    Executing Operation: Erase Flash\n\n    Erasing Flash Region...\n\n    ERROR: Erase Flash Operation Failed!\n\n    Resetting Adapter...\n    Reset Successful!\n\n    Due to Exception Command not Executed. IOCStatus=0x47ca, IOCLogInfo=0x0\n    Finished Processing Commands Successfully.\n    Exiting SAS3Flash.\nI suspect this error is related to me not messing around with the jumper cables before hand, but I can’t find them.\nI also can’t flash the IT firmware, and I am assuming this is because it requires that you wipe the firmware first.\n./sas3flash -o -f SAS9300_8i_IT.bin -b mptsas3.rom -b mpt3x64.rom -l flashing.log \n    Adapter Selected is a Avago SAS: SAS3008(C0)\n\n    Executing Operation: Flash Firmware Image\n\n        Firmware Image has a Valid Checksum. \n        Firmware Version 16.00.10.00\n        Firmware Image compatible with Controller. \n\n        Valid NVDATA Image found. \n        NVDATA Major Version 0e.01 \n        Checking for a compatible NVData image... \n\n        NVDATA Device ID and Chip Revision match verified.\n        NVDATA Versions Compatible.\n        Valid Initialization Image verified.\n        Valid BootLoader Image verified.\n\n        ERROR: Cannot Flash IT Firmware over IR Firmware!\n\n        Firmware Image Validation Failed! \n\n    Due to error remaining commands will not be executed.\n    Unable to Process Commands.\n    Exiting SAS3Flash.\nHowever, after doing some research, it appears that only the dos and efi versions of sas3flash are able to flash IT firmware over IR firmware, according to the official broadcom website. ON that site, there is a compatibility table, meaning I need to use the either the efi version, or the dos version. I should have noticed how the existing guides used either the efi version or the dos version.\nSo I set up a USB with the efi version of the flasher, and I attempted to get into the boot menu… only for it not to work. I press F12, or F8 then F12The screen says something along the lines of “Entering the one time boot configuration”, before taking me to the “cannot find hard drive” screen. I have no idea why it’s not working, as it has worked before.\nAccording to one forum post, the issue has something to do with the CD Rom drive, and I should try removing it. However, that question and suggestion is relating to laptops, not workstations, so it might not work for me.\nOkay, I figured out what the issue was. I had unplugged the drives, but when I plugged them back in, they were not seated properly, and that made them get confused.\nNow that I could get to the boot menu, I could boot my sas3flash efi file. Except I couldn’t. It simply errored. I attempted to boot on it on my laptop to make sure that it was not an issue with the desktop and not the efi setup, and it errored there as well.\nHowever, after I booted to a linux distro, I found that I could see the drives, independent of the raid controller. For whatever, reason, my flashing appears to have worked, even if it didn’t. Now that I have drives installed, I can finally begin to set up the software."
  },
  {
    "objectID": "projects/build-server/index.html#memory",
    "href": "projects/build-server/index.html#memory",
    "title": "Building my own Server Part 1 — Hardware",
    "section": "",
    "text": "This comes with two 16 gb sticks, one in each cpu.\nI bought 6 more 16 GB ecc sticks, so I will have a total of 128 GB of memory.\nIn order to ensure that dual channel works, I will need to put them in the right spots. Now, the previous manual that I found has a chart detailing on the channel configuration, but that chart is very complex. Lower down in the manual, in the “errors and warnings” section, there is a error that could appear that makes things very clear to me.\n“Warning: Non-optimal memory population detected. For increased memory bandwidth populate DIMM connectors with white latches before those with black latches.”\nThat makes things very simple for me.\nI added the ram. And now my system won’t boot at all. I don’t get any monitor output. I asked around, and someone mentioned that it could be dust in the ram socket cores, so I will have to take them out, blow the dust out, and then reseat the ram.\nThe same user helping me also said that it might be an issue with extreme memory profiles (xmp), a feature that adjusts how memory works on intel cpus, and that I should disable it from a working bios.\nOkay, I figured out what the issue was. The issue was that I did not seat the memory properly.\nOnce I had set the memory, I ran the memory test built in to the bios, and after some time it finished without any errors."
  },
  {
    "objectID": "projects/quarto-via-nix/index.html",
    "href": "projects/quarto-via-nix/index.html",
    "title": "Packaging quarto using nix",
    "section": "",
    "text": "Update 2024-1-15\nBug. See here: https://github.com/NixOS/nixpkgs/issues/256074\nBecause of this, I returned to the vendored version of pandoc to fix this:\n\n\nShow code\n\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n\n    python3 = pkgs.python311;\n    pythonDeps = ps: with ps; [ \n        jupyter #notebook qtconsole jupyter_console\n        # nbconvert\n        # ipykernel ipywidgets\n        # matplotlib\n        pip\n\n        ];\n     quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n         preFixup = ''\n          wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : ${pkgs.dart-sass}/bin/dart-sass \\\n            --prefix QUARTO_PYTHON : \"${pkgs.python3.withPackages pythonDeps}/bin/python3\" \\\n          '';\n          installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook postInstall\n        '';\n    });\nin\n    pkgs.mkShell {\n        PYTHONPATH = \"${pkgs.python3.withPackages pythonDeps}/bin/python3\";\n        QUARTO_PYTHON = \"${pkgs.python3.withPackages pythonDeps}/bin/python3\";\n        QUARTO_PANDOC = \"${quarto}/bin/tools/pandoc\";\n        packages = with pkgs; [ \n            nodePackages_latest.npm\n            (python3.withPackages pythonDeps)\n            quarto\n            texlive.combined.scheme-full\n            ];\n    }\n\nAnd with this, this little bug is fixed.\nHowever, there is another issue where quarto does not properly detect the version of jupyter installed in the nix environment.\n[nix-shell:~/vscode/moonpiedumplings.github.io]$ quarto check\n\n....\n\n[✓] Checking Python 3 installation....OK\n      Version: 3.11.6\n      Path: /nix/store/aa115421digfydv0j6w1ad33f822z2sz-python3-3.11.6-env/bin/python3.11\n      Jupyter: (None)\n\n      Jupyter is not available in this Python installation.\n      Install with python3 -m pip install jupyter\nDespite the fact that it is installed.\nDespite the fact that it is installed.\nNevermind, after updating my nixpkgs, this issue was fixed:\n[✓] Checking Python 3 installation....OK\n      Version: 3.11.7\n      Path: /nix/store/37hbd3y9qfv6nfqg5rh4r27kxlfwrnx5-python3-3.11.7-env/bin/python3.11\n      Jupyter: 5.7.1\n      Kernels: python3\n\n\nQuarto and pandoc were updated in nixpkgs!\nI’ve been following the matrix channel for haskell and nixpkgs, and they’ve been working on this for some time now, since I first wrote this article. But recently, they updated the relevant haskell packages, and the quarto version in nixpkgs-unstable is now the latest version.\n\n\nOriginal Article Below\nSkip to the conclusion if you want to know how to use the efforts of my hard work, although it only works on x86_64 linux.\n\n\nWhat is quarto?\nTo explain quarto, I first have to explain jupyter notebooks, and quarto’s predecessor, fastpages.\nFastpages is a blogging platform built on jekyll, a static site generator. Static sites are websites that do not connect to a backend server, the user’s browser does all the rendering. Static site generators do all the hard work of creating these, by converting a very human readable format, like markdown, to pretty looking html, which is what browsers render.\nFastpages adds onto the features of jekyll, by adding support for jupyter notebooks. Jupyter is a technology that allows users to combine code, multimedia, and text into a single document, for any purposes that one might use it. It’s usually popular for data science, as code is used to generate diagrams, but I really like it for testing snippets of code, as you can have multiple pieces of code in one document, and then run and debug them independently of eachother. Fastpagess can convert all of this, even automatically running code and creating any interactive elements, and then putting it all up on the internet as a static site.\nFastpages is also deprecated. On the github page, which is archived, it recommends you switch to quarto.\nQuarto adds more features on top of fastpages, while also removing some features. Fastpages is primarily designed for blogging, whereas quarto also has support for generating books, pdf’s, and websites.\nHowever, quarto lacks some features, as it uses it’s own static site generator, rather than jekyll. The biggest and most noticable one, is the Liquid template language that my computer science teacher uses to dynamically render his schedule page.\nBut for my purposes, quarto works fine. This blogpost you are reading, was generated using quarto.\n\n\nWhat is nix?\nNix is multiple things. Nix is a linux distribution, an package repository, a package manager, a programming language, and a configuration as code system.\nRight now, I am trying to use it as a package manager — specifically, to give myself the quarto tool.\nI’ve selected nix because it focuses on reproducible builds, across Mac, and Linux and x86_64, and arm64. This enables a multitude of devices to get packages with an identical configuration to me.\n\n\nUsing Nix\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno ];\n    }\nThis is a sample shell.nix file. If you run the nix-shell command line tool while in the same working directory, or using the filename as an argument, it will use this bit of nix code to create an shell environment for you.\nNix is a functional programming langauge. Unlike a language like python or java, where everything is an object, in nix, everything is a function. The colon : declares the arguments for a function. The ? declares a default argument for the variable, pkgs, to be used in the function. This is important, because without this declarion, the program does now know where to get packages from.\nThe above shell.nix works great. However, it installs an older version of quarto, 1.2, as only an older version of quarto is packaged in the nixpkgs repository. I want the newest version, 1.3.\nHere is the code used to create the quarto package, called a derivation:\n\n\nShow derivation\n\n\n{ stdenv\n, lib\n, pandoc\n, esbuild\n, deno\n, fetchurl\n, nodePackages\n, rWrapper\n, rPackages\n, extraRPackages ? []\n, makeWrapper\n, python3\n, extraPythonPackages ? ps: with ps; []\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"quarto\";\n  version = \"1.2.475\";\n  src = fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-oyKjDlTKt2fIzirOqgNRrpuM7buNCG5mmgIztPa28rY=\";\n  };\n\n  nativeBuildInputs = [\n    makeWrapper\n  ];\n\n  patches = [\n    ./fix-deno-path.patch\n  ];\n\n  postPatch = ''\n    # Compat for Deno &gt;=1.26\n    substituteInPlace bin/quarto.js \\\n      --replace 'Deno.setRaw(stdin.rid, ' 'Deno.stdin.setRaw(' \\\n      --replace 'Deno.setRaw(Deno.stdin.rid, ' 'Deno.stdin.setRaw('\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n    wrapProgram $out/bin/quarto \\\n      --prefix PATH : ${lib.makeBinPath [ deno ]} \\\n      --prefix QUARTO_PANDOC : ${pandoc}/bin/pandoc \\\n      --prefix QUARTO_ESBUILD : ${esbuild}/bin/esbuild \\\n      --prefix QUARTO_DART_SASS : ${nodePackages.sass}/bin/sass \\\n      --prefix QUARTO_R : ${rWrapper.override { packages = [ rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n      --prefix QUARTO_PYTHON : ${python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n  '';\n\n  installPhase = ''\n      runHook preInstall\n\n      mkdir -p $out/bin $out/share\n\n      rm -r bin/tools\n\n      mv bin/* $out/bin\n      mv share/* $out/share\n\n      runHook preInstall\n  '';\n\n  meta = with lib; {\n    description = \"Open-source scientific and technical publishing system built on Pandoc\";\n    longDescription = ''\n        Quarto is an open-source scientific and technical publishing system built on Pandoc.\n        Quarto documents are authored using markdown, an easy to write plain text format.\n    '';\n    homepage = \"https://quarto.org/\";\n    changelog = \"https://github.com/quarto-dev/quarto-cli/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ mrtarantoga ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nI don’t want to bore you with details, but in short, it downloads an older version of quarto than the newest.\n\n\nPackaging quarto\nNix is very poorly documented. The recommended way of getting help with nix is to ask for help on the discord. So that is what I did. The first thing I asked was how to get a newer version of quarto:\nI started out by asking how to update the version of the quarto package. I started out to do so on my own, by cloning the nixpkgs github repo, and attempting to build nixpkgs, but I couldn’t figure out how to build it at first, which is when I asked.\n\nWhich didn’t work, because I did not want to download the whole nixpkgs. I wanted to store the nix derivation to build. The answer: use an ovveride:\nI was then told to use the overrideAttrs function, which overrides specific attributes, essentially variables, of the derivation, another type of function, used to build the program.\nMy first attempt was not too good.\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\nA simple shell.nix that replaced the version attribute of quarto. I shared this excitedly, thinking I had figured this out on my own, only to be told that this change was purely cosmetic, and the new version of quarto wasn’t actually installed. And they were right.\n[nix-shell:~/vscode/quartotest]$ which quarto\n/nix/store/9qy0kpll3r755c6i0717405dilhffdrd-quarto-1.3.361/bin/quarto\nIt looks right, until you check deeper:\n[nix-shell:~/vscode/quartotest]$ quarto --version\n1.2.475\nSo a deeper override was needed. I needed to override the src attribute, which determines where to download the files used to package the application.\n\n\nSecond attempt!\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n            src = fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-VEYUEI4xzQPXlyTbCThAW2npBCZNPDJ5x2cWnkNz7RE=\";\n  };\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nBut this errored as well.\n~/vscode/quartotest master !4 ?1 ❯ nix-shell                                                                                                                         2m 56s\nerror: undefined variable 'fetchurl'\n\n       at /home/moonpie/vscode/quartotest/shell.nix:5:19:\n\n            4|             version = \"1.3.361\";\n            5|             src = fetchurl {\n             |                   ^\n            6|     url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-lin\nI was confused? Why did this error? I had copied exactly what was in the derivation used to build the package?\nLater, I figured out why. When a package is built, the dependencies are declared in the beginning of the package:\n{ \n  Dependencies_Here\n} : stdenv.mkDerivation.restofpackage\nIn nix, every single thing is a function. When creation a function in nix, the curly brackets before the function declare the arguments that the function will take:\nfunction = {arg1, arg2} : functionhere\nBut when creating a package, this syntax plays another role. The arguments of the function act as a dependency list, by declaring what packages are necessary to build the derivation. This prevents the build from being tainted by anything that is not explicitly declared. However, because overrides are not the same as derivations, they act differently.\nBut to get around this error when using the override function:\n\n\nSuccess!\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n            src = pkgs.fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n  };\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nAnd it worked:\n[nix-shell:/tmp/test]$ quarto --version\n1.3.361\nExcept it didn’t. When I actually tried to render my project:\n[nix-shell:~/vscode/quartotest]$ quarto render\n[1/4] about.qmd\nCould not find data file templates/styles.citations.html\nSince this file couldn’t be found on my system, I tried to find it on the internet.\nAnd find it I did, in the data-files section of the information about the pandoc 3.1 package\ndata-files:\n                 -- templates\n                 data/templates/styles.html\n                 data/templates/styles.citations.html\nFirst, I checked what version of the pandoc that Nix had in their repositories. They only had 2.1.9, which was too old for the version of quarto I had.\nBut just in case, I asked on the github discussions page for quarto. And yes, quarto 1.3, the version I wanted, did require pandoc 3.0, which nix did not have packaged.\nExcept it did, although the package wasn’t in the dependencies. So I first tried to install it independently, using the nix-shell -p package tool\n\n\nExcept I got an error\n\n~/vscode/test ❯ nix-shell -p haskellPackages.pandoc_3_1_2\nthis derivation will be built:\n  /nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv\nbuilding '/nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv'...\nsetupCompilerEnvironmentPhase\nBuild with /nix/store/4wjl91hrizxghwqy18a1337gq2y9mh40-ghc-9.2.7.\nunpacking sources\nunpacking source archive /nix/store/7ncxphrr3nff9jb3j4w9ksl6ggznqhm6-pandoc-3.1.2.tar.gz\nsource root is pandoc-3.1.2\nsetting SOURCE_DATE_EPOCH to timestamp 1000000000 of file pandoc-3.1.2/xml-light/Text/Pandoc/XML/Light/Types.hs\npatching sources\ncompileBuildDriverPhase\nsetupCompileFlags: -package-db=/build/tmp.yaRUozaznX/setup-package.conf.d -j16 +RTS -A64M -RTS -threaded -rtsopts\n[1 of 1] Compiling Main             ( /nix/store/4mdp8nhyfddh7bllbi7xszz7k9955n79-Setup.hs, /build/tmp.yaRUozaznX/Main.o )\nLinking Setup ...\nconfiguring\nconfigureFlags: --verbose --prefix=/nix/store/7983f3r6gpgvf17dn1k2c05wma708xdn-pandoc-3.1.2 --libdir=$prefix/lib/$compiler --libsubdir=$abi/$libname --datadir=/nix/store/zdc55i48g6hpbwckiwk6s6iraf30hh99-pandoc-3.1.2-data/share/ghc-9.2.7 --with-gcc=gcc --package-db=/build/tmp.yaRUozaznX/package.conf.d --ghc-options=-j16 +RTS -A64M -RTS --disable-split-objs --enable-library-profiling --profiling-detail=exported-functions --disable-profiling --enable-shared --disable-coverage --enable-static --disable-executable-dynamic --enable-tests --disable-benchmarks --enable-library-vanilla --disable-library-for-ghci --ghc-option=-split-sections -f-trypandoc --extra-lib-dirs=/nix/store/4g9phbpakh51bbw2n391vipz9r5z56kw-ncurses-6.4/lib --extra-lib-dirs=/nix/store/mnq0hqsqivdbaqzmzc287l0z9zw8dp15-libffi-3.4.4/lib --extra-lib-dirs=/nix/store/0ssnwyy41aynhav7jr4dz1y01lfzi86f-gmp-with-cxx-6.2.1/lib\nUsing Parsec parser\nConfiguring pandoc-3.1.2...\nSetup: Encountered missing or private dependencies:\ndoctemplates &gt;=0.11 && &lt;0.12,\ngridtables &gt;=0.1 && &lt;0.2,\njira-wiki-markup &gt;=1.5.1 && &lt;1.6,\nmime-types &gt;=0.1.1 && &lt;0.2,\npandoc-types &gt;=1.23 && &lt;1.24,\ntexmath &gt;=0.12.7 && &lt;0.13\n\nerror: builder for '/nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv' failed with exit code 1\n\nExcept all the dependencies that the error message wanted, existed in nixpkgs.\nHere’s doctemplates\nHere’s gridtables\nAnd so on. It was like, even though pandoc required these packages, it couldn’t see them. They all had the format packagename_version, as opposed to simply packagename, which would be an older package.\nSo I asked on discord, again.\n\nThis one user, NobbZ, helps people so much that people joke that he is the documentation.\nI tried their solution, and it didn’t work. Apparently, this solution was designed for the newer feature of nix, flakes, which I wasn’t using.\nBut with some adjustment, I managed to figure out how to use the override feature on my own, with the same solution that NobbZ sent me seconds later:\n\n\nTrying to get override working\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n   /* pandoc = pkgs.haskellPackages.callCabal2nix \"pandoc\" (fetchTarball {\n        url = \"https://github.com/jgm/pandoc/archive/refs/tags/3.1.2.tar.gz\";\n        sha256 = \"1h928w4ghbxg5whq7d9nkrfll2abvmbkc45adfgv35rfhcpkiiv9\";\n    }) {};*/\n    doctemplates = pkgs.haskellPackages.doctemplates_0_11;\n    gridtables = pkgs.haskellPackages.gridtables_0_1_0_0;\n    jira-wiki-markup = pkgs.haskellPackages.jira-wiki-markup_1_5_1;\n    mime-types = pkgs.haskellPackages.mime-types_0_1_1_0;\n    pandoc-types = pkgs.haskellPackages.pandoc-types_1_23;\n    texmath = pkgs.haskellPackages.texmath_0_12_7_1;\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.override {inherit doctemplates gridtables jira-wiki-markup mime-types pandoc-types texmath;};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nExcept this errors:\nWarning:\n    This package indirectly depends on multiple versions of the same package. This is very likely to cause a compile failure.\n      package http-client (http-client-0.7.13.1-52kzOBAMbxmJrzoQZgatPf) requires mime-types-0.1.0.9-Gdz1G1mhqziCfo3C8KZHz7\n      package pandoc (pandoc-3.1.2) requires mime-types-0.1.1.0-4FUch8wD40c6kQtGdyJOSM\n      package texmath (texmath-0.12.7.1-BbrGid5okuSI4hfeGBAcF8) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package commonmark-pandoc (commonmark-pandoc-0.2.1.3-OwUzhyyJ0cDzxfYXzbAci) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package citeproc (citeproc-0.8.1-LP74PTBZCEoHiNCfXfUYdM) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package pandoc (pandoc-3.1.2) requires pandoc-types-1.23-AC7tSm0fcRIGMZsmro9kaK\n      package pandoc (pandoc-3.1.2) requires pandoc-types-1.23-AC7tSm0fcRIGMZsmro9kaK\n** abort because of serious configure-time warning from Cabal\nerror: builder for '/nix/store/ibawyigbdn9bs1gs9hc0mgzqraqfxhy0-pandoc-3.1.2.drv' failed with exit code 1\nEssentially, a dependency error. Doctemplates also required a package that wasn’t under the default of packagename, but rather packagename_version.\nAnother user proceeded to chime in with their solution:\n\n\nSee cdepillabout’s solution\n\nlet\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n    doctemplates = hprev.doctemplates_0_11;\n    gridtables = hprev.gridtables_0_1_0_0;\n    jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n    mime-types = hprev.mime-types_0_1_1_0;\n    pandoc-types = hprev.pandoc-types_1_23;\n    texmath = hprev.texmath_0_12_7_1;\n  });\nin\n  pkgs.mkShell {\n    packages = [pandoc];\n  }\n\nAnd this worked! Except it didn’t.\n[nix-shell:~/vscode/quartotest]$ pandoc\nbash: pandoc: command not found\n\n[nix-shell:~/vscode/quartotest]$ which pandoc\nwhich: no pandoc in (/nix/store/kbcrs84s1x8yd5bp1nq6q6ihda8nd2lp-bash-interactive-5.2-p15/bin:/nix/store/a9q4y7vw1fgs990bs5mpd3p50mc0iz27-python3-3.10.11/bin:/nix/store/nh8iz5l2zn5nbk19qxdw575a5fhfcajw-quarto-1.3.361/bin:/nix/store/ar2lzr4kr4pi1zgx3w8hl6fkny3bql53-python3.10-notebook-6.5.2/bin:/nix/store/ai5lxg5vzjsfk9zkyn65ndq81na2mm5c-python3.10-babel-2.12.1/bin:/nix/store/95cxzy2hpizr23343b8bskl4yacf4b3l-python3-3.10.11/bin:/nix/store/5ii8sm9yh01ny05bl1wjdv6pkdjb8bw0-python3.10-jupyter-core-5.2.0/bin:/nix/store/pkgr71n4dy7h9lp00paf6k3llfa95ig0-python3.10-Send2Trash-1.8.1b0/bin:/nix/store/x1kk4hlx0zl12igvr6v0pk2cq2720fbh-python3.10-jupyter_client-8.0.3/bin:/nix/store/9icvaw0dgk7258m564xlh513nz6xis1m-python3.10-nbformat-5.7.3/bin:/nix/store/6svh49hf9pq5hwavgyb642v5a0pjnn4a-python3.10-jsonschema-4.17.3/bin:/nix/store/15jn0r39wg0ripjzjfxj9arcv53qxck9-python3.10-nbclassic-0.5.2/bin:/nix/store/jjy30kw6pw2mq54ig6lrm84ds91a9snf-python3.10-ipython-8.11.0/bin:/nix/store\nApparently, in pandoc 3.0, the binary and the library have been split into two seperate packages. In nixpkgs, the library can be found in the haskellPackages.pandoc_3_1_2, and the binary can be found in haskellPackages.pandoc-cli.\nnix-shell -p haskellPackages.pandoc-cli\n\nSetup: Encountered missing or private dependencies:\ndoctemplates &gt;=0.11 && &lt;0.12, pandoc &gt;=3.0\n...\nerror: builder for '/nix/store/hzzqnffj08r9qc0xi3b4ydi7w91dn4m0-pandoc-server-0.1.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/arbq7vgw539xyd4l0y5x3jyhhra30v91-pandoc-cli-0.1.1.drv' failed to build\nThe pandoc-cli package is broken, for the exact same error that the pandoc library won’t compile for.\n\n\nSo I try my own override:\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n        doctemplates = hprev.doctemplates_0_11;\n        gridtables = hprev.gridtables_0_1_0_0;\n        jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n        mime-types = hprev.mime-types_0_1_1_0;\n        pandoc-types = hprev.pandoc-types_1_23;\n        texmath = hprev.texmath_0_12_7_1;\n      });\n    pandoc-cli = pkgs.haskellPackages.pandoc-cli.overrideScope (hfinal: hprev: {\n        hslua-core = hprev.hslua-core_2_3_1;\n        lua = hprev.lua_2_3_1;\n    });\nin\n    pkgs.mkShell {\n        packages = [ pkgs.python310Full quarto pkgs.jupyter pandoc pandoc-cli pkgs.deno ];\n    }\n\nBut this also errors:\n\n\nShow error\n\nUsing Parsec parser\nConfiguring tasty-hslua-1.0.2...\n\nSetup: Encountered missing or private dependencies:\nhslua-core &gt;=2.0 && &lt;2.3\n\nerror: builder for '/nix/store/nmhz8xnc24xw86q573v515n3q8m9l0y5-tasty-hslua-1.0.2.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/8q4ni7s2am50xbbbkcdjk85szyvq3jk8-hslua-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/hcysycpqkqv4kd3qmkwzyi7pkaqszqyy-hslua-marshalling-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/h4c32n03d305njipsiw4rzc8rq52l2bc-hslua-packaging-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/y655vw1bdq8a9j818k16y7228nlsf86y-hslua-cli-1.4.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/xkkp4cj1yfwjpczc6k7y08gxdqdbfh4n-pandoc-2.19.2.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/sc44mnc1ngxfxi7h3f6qrrvnvldla4w3-pandoc-cli-0.1.1.drv' failed to build\n~/vscode/quartotest master !4 ?9 ❯                                                                                        \n\nBut the hslua-core version I want was packaged in nixpkgs, similar to doctemplates, or gridtables. So I did a further override.\n\n\nShow/hide\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n        doctemplates = hprev.doctemplates_0_11;\n        gridtables = hprev.gridtables_0_1_0_0;\n        jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n        mime-types = hprev.mime-types_0_1_1_0;\n        pandoc-types = hprev.pandoc-types_1_23;\n        texmath = hprev.texmath_0_12_7_1;\n        tasty-hslua = hprev.tasty-hslua_1_1_0;\n        hslua-marshalling = hprev.hslua-marshalling_2_3_0;\n        hslua-aeson = hprev.hslua-aeson_2_3_0_1;\n        hslua = hprev.hslua_2_3_0;\n      });\n    pandoc-cli = pkgs.haskellPackages.pandoc-cli.overrideScope (hfinal: hprev: {\n        hslua-core = hprev.hslua-core_2_3_1;\n        lua = hprev.lua_2_3_1;\n        tasty-hslua = hprev.tasty-hslua_1_1_0;\n        hslua-marshalling = hprev.hslua-marshalling_2_3_0;\n        hslua-aeson = hprev.hslua-aeson_2_3_0_1;\n        hslua = hprev.hslua_2_3_0;\n    });\nin\n    pkgs.mkShell {\n        packages = [ pkgs.python310Full quarto pkgs.jupyter pandoc pandoc-cli pkgs.deno ];\n    }\n\nWhich still errors:\nUsing Parsec parser\nConfiguring hslua-typing-0.1.0...\n\nSetup: Encountered missing or private dependencies:\nhslua-core &gt;=2.3 && &lt;2.4, hslua-marshalling &gt;=2.3 && &lt;2.4\n\nerror: builder for '/nix/store/kbmfxjy0ycwwg6r6zsp9q9v1pfkmggnw-hslua-typing-0.1.0.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/nj4smnyrkaf52qx98r1wa0r1gdnjbwxk-hslua-2.3.0.drv' failed to build\nI quickly realized, that the updated set of haskell packages in nixpkgs, is broken all the way down. I found a relevant github issue. In this issue, somene had modified the derivation of the haskellPackages, to get pandoc-cli to work.\nI used their fork of nixpkgs to give myself pandoc-cli.\n~/vscode/quartotest master +3 !1 ❯ nix-shell -p haskellPackages.pandoc-cli -I nixpkgs=https://github.com/seam345/nixpkgs/archive/89e6e477c8357a087e863db562d2fa8d9fe5ba29.tar.gz\n[nix-shell:~/vscode/quartotest]$ pandoc --version\npandoc 3.1\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\nThis actually worked, and I got pandoc-cli with the 3.0 version of pandoc. However, I couldn’t get quarto to use pandoc-cli rather than the normal pandoc version, so quarto still wasn’t working.\nLater, cdepillabout chimed in again. Here is their solution:\n\n\nShow/hide\n\nlet\n  nixpkgs-src = fetchTarball {\n    # nixpkgs-unstable as of 2023-05-31\n    url = \"https://github.com/NixOS/nixpkgs/archive/58c85835512b0db938600b6fe13cc3e3dc4b364e.tar.gz\";\n    sha256 = \"0bkhaiaczj25s6hji2k9pm248jhfbiaqcfcsfk92bbi7kgzzzpif\";\n  };\n\n  my-overlay = final: prev: {\n\n    pandoc_1_3 =\n      let\n        inherit (final.haskell.lib.compose) disableCabalFlag markUnbroken;\n      in\n      final.lib.pipe\n        final.haskellPackages.pandoc-cli\n        [\n          markUnbroken\n          (disableCabalFlag \"lua\")\n          (p: p.overrideScope (hfinal: hprev: {\n            doctemplates = hprev.doctemplates_0_11;\n            gridtables = hprev.gridtables_0_1_0_0;\n            hslua-cli = null;\n            jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n            mime-types = hprev.mime-types_0_1_1_0;\n            pandoc = hprev.pandoc_3_1_2;\n            pandoc-lua-engine = null;\n            pandoc-server = markUnbroken hprev.pandoc-server;\n            pandoc-types = hprev.pandoc-types_1_23;\n            texmath = hprev.texmath_0_12_7_1;\n          }))\n        ];\n\n    quarto_1_3 =\n      let\n        quarto-version = \"1.3.361\";\n      in\n      (final.quarto.override { pandoc = final.pandoc_1_3; }).overrideAttrs (oldAttrs: {\n        version = quarto-version;\n        src = final.fetchurl {\n          url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${quarto-version}/quarto-${quarto-version}-linux-amd64.tar.gz\";\n          sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n      });\n  };\n\n  pkgs = import nixpkgs-src { overlays = [ my-overlay ]; };\n\nin\n\npkgs.quarto_1_3\n\nAlthough I modified it a bit, to be\n...\nin\npkgs.mkShell {\n  packages = [pkgs.quarto_1_3];\n}\nBecause it is the mkShell package that creates the shell environment.\nThis solution works, and gets me quarto 1.3, and also replaces the default dependency of quarto, on pandoc 2.1.9, with one on pandoc-cli.\n[nix-shell:~/vscode/quartotest]$ quarto pandoc --version\npandoc 3.1.2\nFeatures: +server -lua\nScripting engine: none\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\nHowever, it has a caveat. It compiles pandoc without lua support, as those packages where the ones that were broken in nixpkgs.\nBut apparently, quarto needs lua suppport.\n[nix-shell:~/vscode/quartotest]$ quarto render\n[1/9] about.qmd\nThis version of pandoc has been compiled without Lua support.\nSo yeah. That doesn’t work.\nBut meanwhile, the quarto team responded to a question I asked on their github discussion page. I had asked if quarto requires either the pandoc binary, or the pandoc library.\nThey replied, and said that a pandoc binary, is actually included inside the tarball and the packages they have created. As part of the build system, the package is fairly self reliant, not needing much in terms of external dependencies.\nSo that’s what I did.\nI modified the derivation so that it uses the built in pandoc, rather than replacing it with an external one.\n\n\nShow/hide\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        patches = [];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : ${pkgs.nodePackages.sass}/bin/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nAnd this works:\n[nix-shell:/tmp/test]$ quarto pandoc --version\npandoc 3.1.1\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\n\n[nix-shell:/tmp/test]$ quarto render\n[1/4] about.qmd\n[2/4] posts/post-with-code/index.qmd\n[3/4] posts/welcome/index.qmd\n[4/4] index.qmd\n\nOutput created: _site/index.html\n\n\n[nix-shell:/tmp/test]$ \nExcept not really:\n[nix-shell:/tmp/test]$ quarto check\n\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\nERROR: TypeError: Invalid Version: 1.62.1 compiled with dart2js 2.19.6\n\nTypeError: Invalid Version: 1.62.1 compiled with dart2js 2.19.6\n    at new SemVer (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:48564:19)\n    at Range.test (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:48974:23)\n    at satisfies (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:49191:18)\n    at checkVersion (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104009:14)\n    at checkVersions (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104035:5)\n    at async check (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:103989:13)\n    at async Command.fn (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104212:5)\n    at async Command.execute (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:8437:13)\n    at async quarto (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:127540:5)\n    at async file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:127558:9\nSo I did some experimenting. First, I replaced all the dependencies with the versions that came with the pandoc package, and then I didn’t get this error with the quarto check command, which checks the installation of quarto, python, pandoc, and R. I then removed the dependencies on by one, to see which one broke it. It ended up being dart-sass.\nSo I tried again:\n\n\nShow shell.nix\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        patches = [];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nAnd this works. The only thing that goes wrong is it gives me a warning when I use the preview function.\nWARNING: Specified QUARTO_PYTHON '/nix/store/xs35q9yb940cxsy1y0qcs84239zmd2jn-python3-3.10.11-env/bin/python3:/bin/python' does not exist.\nI’ve found no wayh to get rid of this warning, and since it is just a warning, I will ignore it. Here is my new shell.nix.\n\n\nShow shell.nix\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        buildInputs = with pkgs; [ python3 jupyter ];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3}/bin/python3\n        '';\n        installPhase = ''\n            echo \"this is the quarto python ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python\"\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nHowever, this is messy. Fitting an entire set of overrides into a single shell.nix file is definitely not the neatest way to do this. And there are some other flaws, like things that aren’t necessary as a dependency. Fortunately, there is a neater way.\nThe nix callPackage function allows for a nix function to call it’s own derivation. Rather than using an override, I can write my own derivation and use the callPackage fucnction to call upon it.\nHere is my derivation:\n\n\nShow derivation:\n\n{ stdenv\n, lib\n, esbuild\n, deno\n, fetchurl\n, nodePackages\n, rWrapper\n, rPackages\n, extraRPackages ? []\n, makeWrapper\n, python3\n, extraPythonPackages ? ps: with ps; []\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"quarto\";\n  version = \"1.3.361\";\n        src = fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n\n  nativeBuildInputs = [\n    makeWrapper\n  ];\n\n  patches = [\n    ./fix-deno-path.patch\n  ];\n\n  postPatch = ''\n    # Compat for Deno &gt;=1.26\n    substituteInPlace bin/quarto.js \\\n      --replace 'Deno.setRaw(stdin.rid, ' 'Deno.stdin.setRaw(' \\\n      --replace 'Deno.setRaw(Deno.stdin.rid, ' 'Deno.stdin.setRaw('\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${lib.makeBinPath [ deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${rWrapper.override { packages = [ rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${python3}/bin/python3\n    '';\n\n  installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n\n  meta = with lib; {\n    description = \"Open-source scientific and technical publishing system built on Pandoc\";\n    longDescription = ''\n        Quarto is an open-source scientific and technical publishing system built on Pandoc.\n        Quarto documents are authored using markdown, an easy to write plain text format.\n    '';\n    homepage = \"https://quarto.org/\";\n    changelog = \"https://github.com/quarto-dev/quarto-cli/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ mrtarantoga ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nAnd here is the shell.nix that summons this package:\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n    quarto = pkgs.callPackage ./env/quarto.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\n\nConclusion and How to Use\nNow, users who have cloned the repo for this blog, can simply install nix, and run nix-shell in the root directory of the repo in order for them to get quarto, python, and jupyter, the dependencies I rely on for this project. Because of the way nix, works it is easy to modify my shell.nix, and add more dependencies, like R or more extra language support via juptyer kernels.\nMy usual workflow is to open a terminal in this git repo, and type nix-shell, and then code ., which gives me vscode with quarto (I have the quarto vscode extension installed), python, and jupyter.\nI realized in hindsight that this only works on x86_64 linux, because my derivation works by taking the quarto x86_64 linux binary and packaging it using nix. But I did learn a lot about writing derivations through this, which I can apply to other things.\n\n\nTry 2, creating a multi architechture and multi OS package\nVery frustrating, quarto has no compilation instructions in the readme. However, thankfully, use github actions to do testing, and builds of the applications, which means that the build steps are technically public, just not immediately apparent.\n\nGithub actions works by reading a .yml file for the instuctions on what do do, and this file can be found in the github actions menu, as workflow file.\nI don’t need the whole thing, however, just the tarball building steps.\n\n\nShow steps\n\nobs:\n  configure:\n    runs-on: ubuntu-latest\n    outputs:\n      version: ${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number}}\n      version_base: ${{steps.config.outputs.version}}\n      tag_name: v${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number }}\n      release: v${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number }}\n      changes: ${{ steps.config.outputs.changes }}\n    if: github.event_name != 'schedule' || (github.event_name == 'schedule' && github.repository == 'quarto-dev/quarto-cli')\n    steps:\n      - name: Install libc6-div\n        run: sudo apt-get install libc6-dev\n\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: config\n        id: config\n        run: |\n          source ./configuration\n          CHANGES=\n          # CHANGES=$(git log $(git describe --tags --abbrev=0)..HEAD --oneline)\n          # Escape \\n, \\r to preserve multiline variable\n          # See https://github.community/t/set-output-truncates-multiline-strings/16852/2\n          # CHANGES=\"${CHANGES//'%'/'%25'}\"\n          # CHANGES=\"${CHANGES//$'\\n'/'%0A'}\"\n          # CHANGES=\"${CHANGES//$'\\r'/'%0D'}\"\n          # echo \"changes=$CHANGES\" &gt;&gt; $GITHUB_OUTPUT\n          QUARTO_BUILD_NUMBER=$(($QUARTO_BUILD_RUN_OFFSET + $GITHUB_RUN_NUMBER))\n          echo \"version=$QUARTO_VERSION\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"changes=$CHANGES\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"build_number=$QUARTO_BUILD_NUMBER\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: News\n          path: ./news/changelog-${{steps.config.outputs.version}}.md\n\n  make-source-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Make Tarball\n        run: |\n          tar -zcvf  quarto-${{needs.configure.outputs.version}}.tar.gz *\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Source\n          path: ./quarto-${{needs.configure.outputs.version}}.tar.gz\n\n  make-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Configure\n        run: |\n          ./configure.sh\n      - name: Prepare Distribution\n        run: |\n          pushd package/src/\n          ./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --log-level info\n          popd\n      - name: Make Tarball\n        run: |\n          pushd package/\n          mv pkg-working quarto-${{needs.configure.outputs.version}}\n          tar -cvf  quarto-${{needs.configure.outputs.version}}-linux-amd64.tar quarto-${{needs.configure.outputs.version}}\n          gzip quarto-${{needs.configure.outputs.version}}-linux-amd64.tar\n          mv quarto-${{needs.configure.outputs.version}} pkg-working\n          popd\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Deb Zip\n          path: ./package/quarto-${{needs.configure.outputs.version}}-linux-amd64.tar.gz\n\n  make-arm64-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Configure\n        run: |\n          ./configure.sh\n      - name: Prepare Distribution\n        run: |\n          pushd package/src/\n          ./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --arch aarch64 --log-level info\n          popd\n      - name: Make Tarball\n        run: |\n          pushd package/\n          mv pkg-working quarto-${{needs.configure.outputs.version}}\n          tar -cvf  quarto-${{needs.configure.outputs.version}}-linux-arm64.tar quarto-${{needs.configure.outputs.version}}\n          gzip quarto-${{needs.configure.outputs.version}}-linux-arm64.tar\n          mv quarto-${{needs.configure.outputs.version}} pkg-working\n          popd\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Deb Arm64 Zip\n          path: ./package/quarto-${{needs.configure.outputs.version}}-linux-arm64.tar.gz\n\nSo to convert github actions into somethign easier to read:\nAMD64 build:\nsource ./configuration\n./configure.sh\npushd package/src/ # pushd is like a more advnaced version of cd\n/quarto-bld prepare-dist --set-version ${{needs.configure.outputs.   version}} --log-level info \npopd\npushd package/\nmv pkg-working quarto-${{needs.configure.outputs.version}}\ntar -cvf  quarto-${{needs.configure.outputs.version}}-linux-amd64.tar quarto-${{needs.configure.outputs.version}}\ngzip quarto-${{needs.configure.outputs.version}}-linux-amd64.tar\nmv quarto-${{needs.configure.outputs.version}} pkg-working\npopd\nAnd then based on the github actions, the generated tarball is located at ./package/quarto-${{needs.configure.outputs.version}}-linux-amd64.tar.gz\nARM64 build (very similar steps):\n./configure.sh\npushd package/src/\n./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --arch aarch64 --log-level info\npopd\npushd package/\nmv pkg-working quarto-${{needs.configure.outputs.version}}\ntar -cvf  quarto-${{needs.configure.outputs.version}}-linux-arm64.tar quarto-${{needs.configure.outputs.version}}\ngzip quarto-${{needs.configure.outputs.version}}-linux-arm64.tar\nmv quarto-${{needs.configure.outputs.version}} pkg-working\npopd\nAnd then based on the github actions, the generated file is located at ./package/quarto-${{needs.configure.outputs.version}}-linux-arm64.tar.gz\nI downloaded these files from releases, and they appear to be extremely similar. The quarto “binary” is a bash script that includes a check for architechture. So the only difference is in the configure step, which seems to download a different architechture.\nHowever, when experimenting with trying to compile quarto on my phone (arm) using termux, I get an error. deno: cannot execute: required file not found. So far, googling this error has gotten me nowhere. I may have to create an arm vm to test later."
  },
  {
    "objectID": "talks/containers/revealjs.html#what-are-containers",
    "href": "talks/containers/revealjs.html#what-are-containers",
    "title": "Linux Containerization Technology",
    "section": "What are containers?",
    "text": "What are containers?\n\n\nThe operating system is made up of many building blocks, almost always placed unevenly\nContainer runtimes are a commmon platform that can be built off of\n\n\n\n\nGiven a lego car, can you build a house on top of it?\nWhat if you put a platform on that lego car?"
  },
  {
    "objectID": "talks/containers/revealjs.html#history",
    "href": "talks/containers/revealjs.html#history",
    "title": "Linux Containerization Technology",
    "section": "History",
    "text": "History\n\nOpenVZ: Built custom kernel to make containers\nDid not update this custom kernel (2.6 is their stable, 3.10 is dev).\n\nCurrent Kernel is 6.5+"
  },
  {
    "objectID": "talks/containers/revealjs.html#benefits-and-downsides",
    "href": "talks/containers/revealjs.html#benefits-and-downsides",
    "title": "Linux Containerization Technology",
    "section": "Benefits and Downsides",
    "text": "Benefits and Downsides\n\nContainers use less memory, because\n\nshare the host kernel\nonly contain what is necessary to use an application\nIdentical parts of containers (layers) are shared in memory, saving more memory\n\nNot as secure as virtual machines\n\nContainer escape CVE’s are rare but fatal\nVery easy to configure, but misconfigurations can be fatal\nCTF will be a container escape one (with no CVEs)"
  },
  {
    "objectID": "talks/containers/revealjs.html#lxc-and-modern-containers",
    "href": "talks/containers/revealjs.html#lxc-and-modern-containers",
    "title": "Linux Containerization Technology",
    "section": "LXC and modern containers",
    "text": "LXC and modern containers\n\nMany groups needed containers in mainline, so they contributed\n\nnamespaces, cgroups (Google), seccomp, virtual filesystems, etc\n\nEventually, LXC was the first iteration of modern container software (IBM)\n\nDocker was based of off lxc, but eventually switched to their own runtime"
  },
  {
    "objectID": "talks/containers/revealjs.html#os-vs-application-type",
    "href": "talks/containers/revealjs.html#os-vs-application-type",
    "title": "Linux Containerization Technology",
    "section": "OS vs Application Type",
    "text": "OS vs Application Type\n\nOS\n\nDesigned to simulate a linux system\nOften used for virtual private servers\nE.G: LXC, Openvz\n\nApplication\n\nDesigned for ease of development and distribution of apps/services\nOCI based runners, like docker, podman, kubernetes, nomad"
  },
  {
    "objectID": "talks/containers/revealjs.html#oci-standard",
    "href": "talks/containers/revealjs.html#oci-standard",
    "title": "Linux Containerization Technology",
    "section": "OCI Standard",
    "text": "OCI Standard\n\nNot to be confused with Oracle Cloud\nOpen Container Initiative\n\nLinux Foundation Project, by Docker, CoreOS, and others\nStandardized Format for application container images\n\nMake distributing services very, very easy"
  },
  {
    "objectID": "talks/containers/revealjs.html#most-popular-docker-and-kubernetes",
    "href": "talks/containers/revealjs.html#most-popular-docker-and-kubernetes",
    "title": "Linux Containerization Technology",
    "section": "Most Popular: Docker and Kubernetes",
    "text": "Most Popular: Docker and Kubernetes\n\nDocker runs OCI images\n\nDocker Hub: Massive repository of images for every type of application\n\nKubernetes: Clustering software\n\nCan run OCI images across multiple machines at once"
  },
  {
    "objectID": "talks/containers/revealjs.html#docker",
    "href": "talks/containers/revealjs.html#docker",
    "title": "Linux Containerization Technology",
    "section": "Docker",
    "text": "Docker\n\nSimple\nOne daemon that runs as root, that autostarts, and handles containers as services\nDocker socket (/var/run/docker.sock), which can be served over the network, or mounted inside a container or vm (it’s also a file)"
  },
  {
    "objectID": "talks/containers/revealjs.html#docker-cont.",
    "href": "talks/containers/revealjs.html#docker-cont.",
    "title": "Linux Containerization Technology",
    "section": "Docker (cont.)",
    "text": "Docker (cont.)\n\nDownside: Runs as root:\n\nHaving access to the docker socket means you are effectively root\nEscaping a container as root means you are root on the host\n\n“One process, one container”"
  },
  {
    "objectID": "talks/containers/revealjs.html#kubernetes",
    "href": "talks/containers/revealjs.html#kubernetes",
    "title": "Linux Containerization Technology",
    "section": "Kubernetes",
    "text": "Kubernetes\n\nA.K.A “k8s”\nExtremely powerful\nMassively complex, many moving parts"
  },
  {
    "objectID": "talks/containers/revealjs.html#kubernetes-cont.",
    "href": "talks/containers/revealjs.html#kubernetes-cont.",
    "title": "Linux Containerization Technology",
    "section": "Kubernetes (cont.)",
    "text": "Kubernetes (cont.)\n\nDifferent Distributions of Kubernetes"
  },
  {
    "objectID": "talks/containers/revealjs.html#others-container-orchetstrators",
    "href": "talks/containers/revealjs.html#others-container-orchetstrators",
    "title": "Linux Containerization Technology",
    "section": "Others Container Orchetstrators",
    "text": "Others Container Orchetstrators\n\nDocker Swarm Mode\n\nLiterally docker, but across multiple machines\n\nHashicorp Nomad\n\nIntegrates with many other Hashi products\nsupports other, unorthox runtimes (e.g: qemu)"
  },
  {
    "objectID": "talks/containers/revealjs.html#runtimes",
    "href": "talks/containers/revealjs.html#runtimes",
    "title": "Linux Containerization Technology",
    "section": "Runtimes",
    "text": "Runtimes\n\nHigh level:\n\nDocker Engine (deprecated, docker now uses containerd + runc)\nContainerd\nCri-o\n\nLow Level\n\nrunc\ncrun (default of podman)\nKata (virtual machine)\nNVIDIA"
  },
  {
    "objectID": "talks/containers/revealjs.html#high-performance-computing-hpc",
    "href": "talks/containers/revealjs.html#high-performance-computing-hpc",
    "title": "Linux Containerization Technology",
    "section": "High Performance Computing (HPC)",
    "text": "High Performance Computing (HPC)\n\nScience needs reproducibility\n\nSimulations ran using programs need to be ran using the same code\n\nSlurm, runs any jobs across a cluster, but is not reproducible\n\nBy using slurm to run singularity/apptainer (OCI runtimes), you get more reproducibility in science\n\nKubernetes is used for machine learning\n\nNVIDIA provides OCI images with drivers for GPU’s baked in"
  },
  {
    "objectID": "talks/containers/revealjs.html#cloud",
    "href": "talks/containers/revealjs.html#cloud",
    "title": "Linux Containerization Technology",
    "section": "Cloud",
    "text": "Cloud\n\nAWS, GCP, Oracle, etc offer containers as a service\nAlso offer managed kubernetes"
  },
  {
    "objectID": "talks/containers/revealjs.html#when-not-to-use-containers",
    "href": "talks/containers/revealjs.html#when-not-to-use-containers",
    "title": "Linux Containerization Technology",
    "section": "When NOT to use containers",
    "text": "When NOT to use containers\n\nMulti-langauge projects, e.g: replit\nUnprivileged environments (no unshare?)\nMulti-OS support (OCI runners use a virtual machine on non-linux)\nSecurity: Do not mistake containers with security"
  },
  {
    "objectID": "talks/containers/revealjs.html#ctf",
    "href": "talks/containers/revealjs.html#ctf",
    "title": "Linux Containerization Technology",
    "section": "CTF",
    "text": "CTF\n\nI will pass out virtual machines\nCTF IP: http://172.16.24.27\nOn the virtual machine: http://vm-ip:3000\nFlags are in roots home directory\nDouble container escape"
  },
  {
    "objectID": "writeups/index.html",
    "href": "writeups/index.html",
    "title": "Writeups",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nApr 20, 2024\n\n\nNice Challenge 4\n\n\nJeffrey Fonseca\n\n\n\n\nFeb 13, 2024\n\n\nNice Challenge 3\n\n\nJeffrey Fonseca\n\n\n\n\nFeb 6, 2024\n\n\nNice Challenge 2\n\n\nJeffrey Fonseca\n\n\n\n\nJan 30, 2024\n\n\nNice Challenge 1\n\n\nJeffrey Fonseca\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "writeups/nice-challenge/index.html",
    "href": "writeups/nice-challenge/index.html",
    "title": "Nice Challenge 1",
    "section": "",
    "text": "This is for the NICE challenge. It’s a blue team challenge where you are tasked with hardening machines. Perhaps there is more, but all I was asked to do for this challenge was some simple hardening.\nThe name of this specific challenge was “Engineer’s Audit Advice - Managing Critical Systems”.\nThe introduction was a simulated chatroom.\n\n\nJoomia\nJoomia is the Debian 9.3 machine, with an apache2 web server.\nI began by looking at /home/playerone/audit.log\n[ Lynis 2.4.0 ]\n\n################################################################################\n  Lynis comes with ABSOLUTELY NO WARRANTY. This is free software, and you are\n  welcome to redistribute it under the terms of the GNU General Public License.\n  See the LICENSE file for details about using this software.\n\n  2007-2016, CISOfy - https://cisofy.com/lynis/\n  Enterprise support available (compliance, plugins, interface and tools)\n################################################################################\n\n\n[+] Initializing program\n------------------------------------\n- Detecting OS... [ DONE ]\n- Checking profiles... [ DONE ]\n\n  ---------------------------------------------------\n  Program version:           2.4.0\n  Operating system:          Linux\n  Operating system name:     Debian\n  Operating system version:  9.3\n  Kernel version:            4.9.0\n  Hardware platform:         x86_64\n  Hostname:                  Prod-Joomla\n  ---------------------------------------------------\n  Profiles:                  /etc/lynis/default.prf\n  Log file:                  /var/log/lynis.log\n  Report file:               /var/log/lynis-report.dat\n  Report version:            1.0\n  Plugin directory:          /etc/lynis/plugins\n  ---------------------------------------------------\n  Auditor:                   [Not Specified]\n  Test category:             all\n  Test group:                all\n  ---------------------------------------------------\n\n  Suggestions (45):\n  ----------------------------\n  * Install a PAM module for password strength testing like pam_cracklib [AUTH-9262] \n      https://cisofy.com/controls/AUTH-9262/\n\n  * Configure minimum password age in /etc/login.defs [AUTH-9286] \n      https://cisofy.com/controls/AUTH-9286/\n\n  * Configure maximum password age in /etc/login.defs [AUTH-9286] \n      https://cisofy.com/controls/AUTH-9286/\n\n  * Install Apache mod_evasive to guard webserver against DoS/brute force attempts [HTTP-6640] \n      https://cisofy.com/controls/HTTP-6640/\n\n  * Install Apache mod_qos to guard webserver against Slowloris attacks [HTTP-6641] \n      https://cisofy.com/controls/HTTP-6641/\n\n  * Install Apache modsecurity to guard webserver against web application attacks [HTTP-6643] \n      https://cisofy.com/controls/HTTP-6643/\n\n  * Install fail2ban to automatically ban hosts that commit multiple authentication errors. [DEB-0880]\n      https://cisofy.com/controls/DEB-0880/\n\nNote from asteele: In addition to the password aging and cracklib, we should also set a password policy to require passwords be of a minimum strength. I feel the default password policy for pam_cracklib is too weak, therefore you should implement a policy that is stronger than the default given. I would suggest a minimum password length of at least 10. I would also like you to not address the maximum password age as experts suggest that setting a maximum password age encourages the creation of weaker passwords, however setting the minimum password age to at least 3 days is advised.\nSo these are some security things I should do. Coincidentally, the numbers also aligned with the checks we were given:\n\nI followed an online guide (internet archive) to enable apache mod_evasive, and sure enough, the check was passed:\n\nThe process was fairly simple, I installed libapache2-mod-evasive, and then uncommented the relevant options in /etc/apache2/mods-enabled/evasive.conf\nInstalling fail2ban and apache mod_qos were easy:\nsudo apt install libapache2-mod-qos fail2ban\nI used a similar process, by uncommenting options in /etc/apache2/mods-enabled to enable mod_qos, and pass the check. Here’s the new config file:\n&lt;IfModule qos_module&gt;\n  # minimum request rate (bytes/sec at request reading):\n  QS_SrvRequestRate                                 120\n\n  # limits the connections for this virtual host:\n  QS_SrvMaxConn                                     100\n\n  # allows keep-alive support till the server reaches 600 connections:\n  QS_SrvMaxConnClose                                600\n\n  # allows max 50 connections from a single ip address:\n  QS_SrvMaxConnPerIP                                 50\n&lt;/IfModule&gt;\nThe next thing I did was login.defs. They had PASS_MAX_AGE set to 9999, which I left. I did, however, change PASS_MIN_AGE to 3, per the suggesion in audit.log.\nTo configure pam and the minimum of the password, I edited `/etc/pam.d/common-password:\n\n\n\"/etc/pam.d/common-password\n\n\n# here are the per-package modules (the \"Primary\" block)\npassword    requisite           pam_cracklib.so retry=3 minlen=10 difok=3\npassword    [success=1 default=ignore]  pam_unix.so obscure use_authtok try_first_pass sha512\n# here's the fallback if no module succeeds\npassword    requisite           pam_deny.so\n# prime the stack with a positive return value if there isn't one already;\n# this avoids us returning an error just because nothing sets a success code\n# since the modules above will each just jump around\npassword    required            pam_permit.so\n# and here are more per-package modules (the \"Additional\" block)\n# end of pam-auth-update config\n\nAnd with this, Joomia is done. Except not really, as one of the checks won’t pass, it’s reported as a “known issue”\n\n\nFileshare\nThe Fileshare is the Ubuntu 16 machine.\n\n\n/home/playerone/audit.log\n\n\n  Suggestions:\n  ----------------------------\n  - Install a PAM module for password strength testing like pam_cracklib [AUTH-9262]\n      https://cisofy.com/controls/AUTH-9262/\n  \n  * Configure minimum password age in /etc/login.defs [AUTH-9286] \n      https://cisofy.com/controls/AUTH-9286/\n\n  * Configure maximum password age in /etc/login.defs [AUTH-9286] \n      https://cisofy.com/controls/AUTH-9286/\n\n  - Enable process accounting [ACCT-9622]\n      https://cisofy.com/controls/ACCT-9622/\n\n  - Install fail2ban to automatically ban hosts that commit multiple authentication errors. [DEB-0880]\n      https://cisofy.com/controls/DEB-0880/\n\nNote from asteele: In addition to the password aging and cracklib, we should also set a password policy to require passwords be of a minimum strength. I feel the default password policy for pam_cracklib is too weak, therefore you should implement a policy that is stronger than the default given. I would suggest a minimum password length of at least 10. I would also like you to not address the maximum password age as experts suggest that setting a maximum password age encourages the creation of weaker passwords, however setting the minimum password age to at least 3 days is advised.\n\nThis time, I clipped the lynis output for brevity.\nI started with installing fail2ban again, since that was simple.\nProcess accounting was simple as well. I installed acct, and then ran sudo acctn on.\nI copied the same process as the first machine for configuring /etc/login.defs, changing PASS_MIN_AGE to 3.\nFor cracklib, I also copied the same process as the above.\nAnd finally, all checks are passed:\n\nThe one check that won’t pass, is actually broken, and won’t pass. They documented this in “Known Issues”"
  },
  {
    "objectID": "talks/containers/index.html",
    "href": "talks/containers/index.html",
    "title": "Containers (that run on linux)",
    "section": "",
    "text": "The layer8 Cybersecurity club as Cal State Northridge asked me to do a talk on linux containers (not to be confused with LXC).\nIn this presentation, I talk a bit about the history of containers, types, and the various ways you can run a container on a system.\nI recorded it, although the recording isn’t very good. In addition to that, the slides are also public, and they are linked here"
  },
  {
    "objectID": "blog/switch-to-opensuse/index.html",
    "href": "blog/switch-to-opensuse/index.html",
    "title": "I installed opensuse on my laptop",
    "section": "",
    "text": "School had just ended. During the school year, I had been using a hybrid graphics (dual-gpu) laptop, that otherwise worked normally, but had horrible, terrible battery life. I had struggled to get 2 hours out of it, because the dedicated nvidia gpu would not get turned off properly.\nSo I decided to switch to my second laptop, which has been unused so far. But it has pure intel graphics, and the laptop has much better linux support, and consequently, a much better battery life.\nBefore setting it up, I had a few requiremnts.\nSecurity. Previously, I didn’t care about this, because I carried my laptop around with me at all times, but now I was going to be leaving my laptop unattended, maybe for extended periods of time. That meant I needed full disk encryption, and secure/trusted boot. However, I had limited time to set this up, so I neede to find a distro that did this the easiest way possible. I eventually settled on opensuse, which had an option to set up secure boot and encryption in the installer, under guided partitioning. After some hiccups, it installed just fine.\nAnd the other, is my tools and packages. I didn’t worry too much about this, because I had decided beforehand on using nix and home-manager to install packages not available in the repositories. Home manager is a tool that allows for declarative configuration of a user environmetn, including packages, configuration files, or environment variables, using the nix programming language. Notably, it can be useed on almost all linux distros.\nHere is my current home.nix, as of writing this, the file home-manager takes as an input.\n\n\nShow\n\n\n\nhome.nix\n\n{\n  pkgs ? import &lt;nixpkgs&gt; {},\n  config,\n  lib,\n  ... \n}:\nlet\n  nixgl = import &lt;nixgl&gt; {};\n  nixGlWrapper = import ./nixglwrapper.nix {inherit nixgl pkgs lib config;};\nin\nwith import ./quarto.nix {inherit pkgs config lib;};\nwith import ./nixglwrapper.nix {inherit pkgs config lib nixgl;};\n{\n  # Home Manager needs a bit of information about you and the paths it should\n  # manage.\n  home.username = \"moonpie\";\n  home.homeDirectory = \"/home/moonpie\";\n  targets.genericLinux.enable = true;\n\n  nixpkgs.config.allowUnfree = true;\n  nix.settings.experimental-features = [\"nix-command\" \"flakes\"];\n\n  # This value determines the Home Manager release that your configuration is\n  # compatible with. This helps avoid breakage when a new Home Manager release\n  # introduces backwards incompatible changes.\n  #\n  # You should not change this value, even if you update Home Manager. If you do\n  # want to update the value, then make sure to first check the Home Manager\n  # release notes.\n  nix.package = pkgs.nix;\n  home.stateVersion = \"23.11\"; # Please read the comment before changing.\n\n  # The home.packages option allows you to install Nix packages into your\n  # environment.\n  #fonts.fontconfig.enable = true;\n  xdg.mime.enable = true;\n  home.packages = [\n    #nixgl\n    nixgl.nixGLIntel\n    nixgl.nixVulkanIntel\n\n    (nixGLWrap pkgs.vscode)\n    (nixGLWrap pkgs.microsoft-edge)\n    (nixGLWrap pkgs.firefox)\n\n\n    pkgs.micro\n    pkgs.calibre\n    pkgs.languagetool\n    pkgs.git\n    pkgs.soundwireserver\n\n    quarto\n    pkgs.jupyter\n    pkgs.python3\n\n    pkgs.yt-dlp\n\n\n    pkgs.macchanger\n    pkgs.nmap\n    pkgs.wireshark\n\n    pkgs.gocryptfs\n    # # Adds the 'hello' command to your environment. It prints a friendly\n    # # \"Hello, world!\" when run.\n    # pkgs.hello\n\n    # # It is sometimes useful to fine-tune packages, for example, by applying\n    # # overrides. You can do that directly here, just don't forget the\n    # # parentheses. Maybe you want to install Nerd Fonts with a limited number of\n    # # fonts?\n    # (pkgs.nerdfonts.override { fonts = [ \"FantasqueSansMono\" ]; })\n\n    # # You can also create simple shell scripts directly inside your\n    # # configuration. For example, this adds a command 'my-hello' to your\n    # # environment:\n    # (pkgs.writeShellScriptBin \"my-hello\" ''\n    #   echo \"Hello, ${config.home.username}!\"\n    # '')\n  ];\n\n  # Home Manager is pretty good at managing dotfiles. The primary way to manage\n  # plain files is through 'home.file'.\n  home.file = {};\n  home.sessionVariables = {\n    # EDITOR = \"emacs\";\n  };\n\n  # Let Home Manager install and manage itself.\n  programs = {\n    home-manager.enable = true;\n    bash.enable = true;\n    gh.enable = true;\n  };\n}\n\n\nOf course, the home.nix file isn’t all there is too it. There are also some imports, which take info from files that aren’t home.nix.\nI have two imports, as of right now.\nUsing nix to run applications on non-nixos distros mostly works, but has some quirks. One quirk is that hardware accelerated graphics (opengl, vulkan) is lacking. In order to get around this, I use a program called nixgl. However, nixgl is essentially a wrapper, and it works by calling hte program you want to run as a command line argument. Someone automated that in nix, and I integrated that into my code.\n\n\nShow\n\n\n\nnixglwrapper.nix\n\n{ config, pkgs, lib, nixgl } :\n{\nnixGLWrap = pkg: pkgs.runCommand \"${pkg.name}-nixgl-wrapper\" {} ''\n    mkdir $out\n    ln -s ${pkg}/* $out\n    rm $out/bin\n    mkdir $out/bin\n    for bin in ${pkg}/bin/*; do\n     wrapped_bin=$out/bin/$(basename $bin)\n     echo \"exec ${lib.getExe nixgl.nixGLIntel} $bin \\\"\\$@\\\"\" &gt; $wrapped_bin\n     chmod +x $wrapped_bin\n    done\n  '';\n  nixVulkanWrap = pkg: pkgs.runCommand \"${pkg.name}-nixgl-wrapper\" {} ''\n    mkdir $out\n    ln -s ${pkg}/* $out\n    rm $out/bin\n    mkdir $out/bin\n    for bin in ${pkg}/bin/*; do\n     wrapped_bin=$out/bin/$(basename $bin)\n     echo \"exec ${lib.getExe nixgl.nixVulkanIntel} $bin \\\"\\$@\\\"\" &gt; $wrapped_bin\n     chmod +x $wrapped_bin\n    done\n  '';\n}\n\n\nAnd of course, finally my custom quarto package that I had made in another post\n\n\nShow\n\n\n\nquarto.nix\n\n{ pkgs, config, lib, ... } :\n\nlet \n    pandoc = null;\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\nin\n {\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        buildInputs = with pkgs; [ ];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\n}\n\n\nHowever, in order for running some programs with sudo to work, I had to edit opensuse’s default sudo configuration to keep environment variables, and not change the default path. This is an understandable thing to do on a multi user system, but on my single user system where I want to use some packages installed via nix with sudo, it is just annoying.\n\n\n/etc/sudoers\n\n...\n##\n## Defaults specification\n##\n## Prevent environment variables from influencing programs in an\n## unexpected or harmful way (CVE-2005-2959, CVE-2005-4158, CVE-2006-0151)\n#Defaults always_set_home\n## Use this PATH instead of the user's to find commands.\n#Defaults secure_path=\"/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:/usr/local/sbin\"\nDefaults !env_reset\n## Change env_reset to !env_reset in previous line to keep all environment variables\n....\n\nI’ve now uploaded my home.nix to a github repo: https://github.com/moonpiedumplings/home-manager\nHere is the home.nix that is currently in the main branch of my github repo (this is dynamically rendered and updated every update of this blog)\n\n\nhome.nix\n\n{\n  pkgs ? import &lt;nixpkgs&gt; {},\n  config,\n  lib,\n  ... \n}:\nlet\n  nixgl = import &lt;nixgl&gt; {};\nin\nwith import ./quarto.nix {inherit pkgs config lib;};\nwith import ./nixglwrapper.nix {inherit pkgs config lib nixgl;};\n{\n  # Home Manager needs a bit of information about you and the paths it should\n  # manage.\n  home.username = \"moonpie\";\n  home.homeDirectory = \"/home/moonpie\";\n  targets.genericLinux.enable = true;\n\n  nixpkgs.config.allowUnfree = true;\n  nix = {\n    settings = {\n    experimental-features = [\"nix-command\" \"flakes\"];\n    };\n  };\n\n  # This value determines the Home Manager release that your configuration is\n  # compatible with. This helps avoid breakage when a new Home Manager release\n  # introduces backwards incompatible changes.\n  #\n  # You should not change this value, even if you update Home Manager. If you do\n  # want to update the value, then make sure to first check the Home Manager\n  # release notes.\n  nix.package = pkgs.nix;\n  home.stateVersion = \"23.11\"; # Please read the comment before changing.\n\n  # The home.packages option allows you to install Nix packages into your\n  # environment.\n  #fonts.fontconfig.enable = true;\n  xdg.mime.enable = true;\n  home.packages = [\n    #nixgl\n    nixgl.nixGLIntel\n    nixgl.nixVulkanIntel\n\n    #nixgl wrapped stuff\n    (nixGLWrap pkgs.vscode)\n    (nixGLWrap pkgs.microsoft-edge)\n    (nixGLWrap pkgs.firefox-bin)\n\n\n    #general tools and utilities\n    pkgs.micro\n    pkgs.calibre\n    pkgs.languagetool\n    pkgs.htop\n\n    #git tools\n    pkgs.git\n    pkgs.bfg-repo-cleaner\n    pkgs.git-filter-repo\n\n    #share sound with android devices.\n    pkgs.soundwireserver\n\n    # development enviroment stuff\n    quarto # see the imports above.\n    (pkgs.python311.withPackages(ps: with ps; [ jupyter ]))\n    pkgs.poetry\n\n    # general cli tools\n    pkgs.yt-dlp\n\n    #hacking\n    pkgs.macchanger\n    pkgs.nmap\n    pkgs.wireshark\n    pkgs.metasploit\n    pkgs.aircrack-ng\n    pkgs.mdk4\n    pkgs.airgeddon\n\n    #creativity\n    pkgs.manuskript\n\n    # storage and encryption\n    pkgs.rclone\n    pkgs.gocryptfs\n    pkgs.syncthing\n    # # It is sometimes useful to fine-tune packages, for example, by applying\n    # # overrides. You can do that directly here, just don't forget the\n    # # parentheses. Maybe you want to install Nerd Fonts with a limited number of\n    # # fonts?\n    # (pkgs.nerdfonts.override { fonts = [ \"FantasqueSansMono\" ]; })\n\n    # # You can also create simple shell scripts directly inside your\n    # # configuration. For example, this adds a command 'my-hello' to your\n    # # environment:\n    # (pkgs.writeShellScriptBin \"my-hello\" ''\n    #   echo \"Hello, ${config.home.username}!\"\n    # '')\n  ];\n\n  # Home Manager is pretty good at managing dotfiles. The primary way to manage\n  # plain files is through 'home.file'.\n  home.file = {};\n  home.sessionVariables = {\n    # EDITOR = \"emacs\";\n  };\n\n  # Let Home Manager install and manage itself.\n  programs = {\n    home-manager.enable = true;\n    bash.enable = true;\n    gh.enable = true;\n  };\n  services.syncthing.enable = true;\n}"
  },
  {
    "objectID": "playground/flashcards/tests.html#section",
    "href": "playground/flashcards/tests.html#section",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Ablbedo\n\nHow reflective something is."
  },
  {
    "objectID": "playground/flashcards/tests.html#section-1",
    "href": "playground/flashcards/tests.html#section-1",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Get in bed\nCount sheep"
  },
  {
    "objectID": "playground/flashcards/tests.html#section-2",
    "href": "playground/flashcards/tests.html#section-2",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "Item 1\n        Item 2\n        Item 3\n        Item 4\n        Item 5"
  },
  {
    "objectID": "playground/flashcards/tests.html#section-3",
    "href": "playground/flashcards/tests.html#section-3",
    "title": "Jeffrey Fonseca",
    "section": "",
    "text": "This is Block 1.\n\n\nThis is Block 2.\n\n\nThis is Block 3."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html",
    "href": "projects/setting-up-kasm/index.html",
    "title": "Kasmweb setup",
    "section": "",
    "text": "School chromebooks cannot be used for computer science. Due to a content blocking setup that is more restrictive than the Great Firewall of China, school chromebooks cannot install the necessary digital tools for software development.\nCurrently, you must have your own device to be able to participate in computer science classes. Students who are unable to obtain their own device for whatever reason are denied participation.\nAlthough getting the chromebooks unlocked to be able to install software is a complex, potentially legal problem, there are alternatives.\nKasm is a remote desktop software. It runs a computer on a remote server, that can be accessed through a browser, or a chromebook. Because there are no restrictions on what can be installed when using Kasm, it makes it possible to use development tools on a chromebook. This blog post is me optimizing kasm so that it is more resource efficient, and also enabling software development tools to be easily installed on it.\nRoadblocks/steps:"
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "href": "projects/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "title": "Kasmweb setup",
    "section": "Turns out, memory deduplication is on by default for docker containers",
    "text": "Turns out, memory deduplication is on by default for docker containers\nGithub issue where someone asked about this. The documents linked were very unclear, so I’ll break it down.\nIf you are using overlayfs or aufs, you have memory deduplication. If you are using other storage drivers, you sacrifice memory for more i/o (write/read) performance.\nFrom here:\n\nOn my ubuntu virtual machine, and the AWS ubuntu machine we are working on, Overlay2 is the storage driver:"
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#kernel-same-page-merging",
    "href": "projects/setting-up-kasm/index.html#kernel-same-page-merging",
    "title": "Kasmweb setup",
    "section": "Kernel Same page merging",
    "text": "Kernel Same page merging\nPreviously, I tried instructions from here: https://wiki.openvz.org/KSM_(kernel_same-page_merging). However, I noticed only a minimal space saved using the LD_PRELOAD steps. Not useful.\nI then tried cachyos fork of uksmd: https://github.com/CachyOS/uksmd, a daemon to go through userspace tasks and dedupe them.\nOnly works with a kernel that has the pmadv_ksm() syscall. Exists in most kernels optimized for desktop usage, like linux-zen, linux-liqourix, or pf-kernel (the original creators of uksmd)\nTo check if your currently running kernel has the feature:\n\non Archlinux, check if the files sys_enter_pmadv_ksm and sys_exit_pmadv_ksm exist in /sys/kernel/debug/tracing/events/syscalls (default does not have this feature, but linux-zen does)\non Ubuntu check if lines containing pmadv exist in the file /proc/kallsyms\n\n\n\n\nuksmstats\n\n\nHalf a gig of ram saved on a normal desktop. Expect to see much more when multiple almost identical docker containers are launched. Very useful. It saves a lot of ram. However, there might be a better way for docker, without jumping through hoops.\nDoes cost a miniscule amount of cpu power, but we have more cpu power and less ram on our servers.\nTo install uksmd on ubuntu, you need to switch kernels.\n\nCompiling UKSMD\nSteps to do so on Ubuntu 22 (you must have switched kernels):\nsudo apt-get install debhelper build-essential dh-make meson pkg-config libprocps-dev libcap-ng-dev # I think it can either be pkg-conf or pkg-config\ngit clone https://github.com/insilications/uksmd-clr\nRename the directory to be something compatible with below steps, like uksmd-1 before you cd into it.\nFollow steps from here\ndh_make --createorig\ndh_auto_configure --buildsystem=meson\ndpkg-buildpackage -rfakeroot -us -uc -b\nThe debian package will be build in the directory above the source directory.\nInstall your debian package!\nIf you want the uksmdstats command for monitoring purposes, you can only get it from the cachyos github (or make your own, it’s just a shell script).\nsudo curl https://raw.githubusercontent.com/CachyOS/uksmd/master/uksmdstats -o /usr/bin/uksmdstats\n\n\nSwitching Kernels\nstatus: complete\ncurl 'https://liquorix.net/install-liquorix.sh' | sudo bash from the liqourix kernel website\nI checked if liqourix has the necessary features, and yes it does.\n\nSetting the Default Kernel\nstatus: researching\nReddit post where I ask how to set default kernel in grub\nOn that reddit post, I talk about some flawed solutions I have found. Mainly they don’t seem to be truly persistent, not surviving kernel updates, updates of grub, or installation of new kernels. This endeavor is pretty risky, because a broken grub means we will have no choice but to delete our aws and start over. I need a 100% solution.\nAfter grilling chatgpt through 3 wrong answers, which chatgpt presented with the absolute confidence that an AI has, it finally presented me a solution that seems like it doesn’t have a risk of breaking the AWS system we are working on.\n\nI need to make some adjustments, but I should be able to select for the term “liqourix” while removing the term “recovery” to select the correct kernel (but not the recovery kernel) with complete consistency even through kernel updates, grub updates, or installation of new kernels.\nI searched around for how to do this, but I eventually gave up and asked chatgpt again, getting this:\n\nMy 20_linux_xen is not the same as what chatgpt wants, so I asked again, and it gave me this code to put in /etc/default/grub:\n# Set the default menu entry based on the title of the menu entry\n# that contains the word \"liqourix\" while ignoring the term \"recovery\"\nGRUB_DEFAULT=\"$(grep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}')\"\ngrep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}'\nI will test this in a virtual machine.\n\n\n\nWeaker alternative: ksm_preload\nstatus: won’t be used\ngit clone https://github.com/binfess/ksm_preload\ncmake .\nmake\nsudo make install\nI added LD_PRELOAD=/usr/local/share/ksm_preload/libksm_preload.so to the file /etc/environment\nI haven’t tested the above, but I saw very minimal space saved, only about 0.11 **megabytes* saved, on my desktop. Tests on my sample server are similarly discouraging:\n\nThe above was with 2 kasm sessions open. Nearly useless. In addition to that, uksmd seems to make this completely obsolete."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#zram",
    "href": "projects/setting-up-kasm/index.html#zram",
    "title": "Kasmweb setup",
    "section": "zram",
    "text": "zram\nstatus: immplemented on my personal systems, but not on the ubuntu vm yet.\nTo install zram, sudo apt install systemd-zram-generator\nThen, you can configure zram by editing /etc/systemd/zram-generator.conf\nThis works, but apparently, things in a swap file aren’t deduplicated by uksmd. Rather, zram handles it’s own deduplication, with the compression algorithms. Becuase it must hash pages, this can potentially lead to more cpu usage.\nBecause I don’t know about this, I made yet another reddit post asking about this topic."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#zswap",
    "href": "projects/setting-up-kasm/index.html#zswap",
    "title": "Kasmweb setup",
    "section": "zswap",
    "text": "zswap\nStatus: Partially done, but dropped in favor of zswap\n\nzswap is disabled by default on my ubuntu virtual machine. Odd that both are disabled by default.\nParameters for zswap can be found in /sys/module/zswap/parameters/\nTo set parameters at boot, use kernel boot paremeters, like zswap.enabled=1 zswap.compressor=lz4 zswap.max_pool_percent=20 zswap.zpool=z3fold\nI will need to tinker to see what is the most optimized zswap setup\nzswap has it’s own memory deduplication feature, which is enabled by default on both my ubuntu vm and the aws ubuntu server:\nSee above, my comments about zswap’s memory deduplication feature."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html",
    "href": "projects/openstack-on-nixos/index.html",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Hyperconverged infrastrucucture, is when multiple aspects of computing can all be managed from the same platform. This is usually done with virtualization, like virtualized servers, or virtualized storage.\nPromox Virtual Environment is one of the most popular examples of this for homelabbers, or people who manage their own servers for personal use. It offers a web based interface to configure virtual machines, virtualized storage, and clustering. When researching what software to use to manage my server, I considered proxmox.\nProxmox is based on debian linux, and is very tightly integrated into that ecosystem. It is nearly impossible to run proxmox on any other linux distro, and I disliked this inflexibility.\nOpenstack is an open source, public and private cloud solution, containing hyperconverged infrastructure, and more. It’s used when people don’t want to rely on external cloud solutions, like Amazon Web Services (AWS). For example, a university may decide that it is cheaper to manage and maintain their own cloud than to rely on AWS.\nOpenstack is massive, consisting of multiple components that must be installed and configured independently of eachother, yet set up to work with eachother. Becuase of this, openstack is usually deployed as configuration as code. The two most popular solutions, from my research, openstack-ansible, and kolla-ansible, work by deploying containerized, preconfigured installs of openstack, that connect to the bare metal portions of the system through standardized API’s like libvirt, which are easy to configure on the base system using ansible. On the other hand, because openstack is much more complex, it is easy to simply make a preconfigured container image, and distribute it out for people to use.\n\n\n\nNixos is an operating system that uses the nix package manager to install packages, but also the nix language for configuration. Because of this, it is a form of configuration as code.\nFor example, I used nix’s ability to create a shell environment to create a shell environment with quarto on linux\nI asked, and searched around, looking for if there was a Nixos way to set up something like proxmox or openstack.\nI first asked, but then I realized that I could search github for the nix programming language, and keywords I desired. I did so, and I found someone’s lxdware configurations\n\n\nShow someone else’s configs for lxdware on nixos\n\n{ config, pkgs, lib, ... }: {\n  systemd.services.docker-create-network-lxdware = {\n    enable = true;\n    description = \"Create lxdware docker network\";\n    path = [ pkgs.docker ];\n    serviceConfig = {\n      Type = \"oneshot\";\n      RemainAfterExit = \"yes\";\n      ExecStart = pkgs.writeScript \"docker-create-network-lxdware\" ''\n        #! ${pkgs.runtimeShell} -e\n        ${pkgs.docker}/bin/docker network create lxdware || true\n      '';\n    };\n    after = [ \"network-online.target\" ];\n    wantedBy = [ \"multi-user.target\" ];\n  };\n\n  virtualisation.oci-containers.containers.\"lxdware\" = {\n    autoStart = true;\n    image = \"docker.io/lxdware/dashboard:latest\";\n    volumes = [ \"/services/lxdware/lxdware:/var/lxdware\" ];\n    dependsOn = [ \"create-network-lxdware\" ];\n    extraOptions = [\n      # networks\n      \"--network=lxdware\"\n      # labels\n      \"--label\"\n      \"traefik.enable=true\"\n      \"--label\"\n      \"traefik.docker.network=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.rule=Host(`lxd.local.bspwr.com`)\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.entrypoints=websecure\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls=true\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls.certresolver=letsencrypt\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.service=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.middlewares=local-allowlist@file, default@file\"\n      \"--label\"\n      \"traefik.http.services.lxdware.loadbalancer.server.port=80\"\n    ];\n  };\n}\n\nLxdware is a web based frontend for LXD, a type of hyperconverged infrastructure. LXD is a daemon, or background process, for managing containers (ran via LXC), virtual machines, and to an extent, virtualized storage. It appealed to me, when I was searching for a hyperconverged infrastructure solution for my home lab.\n\nIt’s feature rich, and mature. However, I dislike the particular implementation used in the above configuration. They ran lxdware, in a docker container. This works, and probably works well, but this isn’t a very nixos way of doing things. Nix offers reproducibility, so docker isn’t needed, and is generally frowned upon because it brings some disadvantages. I wanted to configure lxdware using nix myself.\nI later looked at openstack, because I wanted to see if there was an ideal way to configure this with nixos. Nix makes it so easy to configure so many other services, just a few lines of nix code in the configuration.nix file to set up webservers, or other services.\nIn fact, people even discussed this in a thread posted on the Nixos discourse forums\n\nIs there anyone actively working on being able to run an OpenStack cloud using NixOS? Shouldn’t “we” be able to do what the Kayobe project does but without the Ansible stuff? https://docs.openstack.org/kayobe/latest/configuration/reference/kolla-ansible.html 9\n\nOne person replied, saying that they had done a bit, but work had “rotten since”. And indeed, when I searched around github, I found an old project, in a github repo titled nixstack\nThis person, or group of people, had packaged openstack to be easy to enable. But as I read through the code, I realized that the app is from the era of python2, so this code is really old."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#what-is-hyperconverged-infrastrucuture",
    "href": "projects/openstack-on-nixos/index.html#what-is-hyperconverged-infrastrucuture",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Hyperconverged infrastrucucture, is when multiple aspects of computing can all be managed from the same platform. This is usually done with virtualization, like virtualized servers, or virtualized storage.\nPromox Virtual Environment is one of the most popular examples of this for homelabbers, or people who manage their own servers for personal use. It offers a web based interface to configure virtual machines, virtualized storage, and clustering. When researching what software to use to manage my server, I considered proxmox.\nProxmox is based on debian linux, and is very tightly integrated into that ecosystem. It is nearly impossible to run proxmox on any other linux distro, and I disliked this inflexibility.\nOpenstack is an open source, public and private cloud solution, containing hyperconverged infrastructure, and more. It’s used when people don’t want to rely on external cloud solutions, like Amazon Web Services (AWS). For example, a university may decide that it is cheaper to manage and maintain their own cloud than to rely on AWS.\nOpenstack is massive, consisting of multiple components that must be installed and configured independently of eachother, yet set up to work with eachother. Becuase of this, openstack is usually deployed as configuration as code. The two most popular solutions, from my research, openstack-ansible, and kolla-ansible, work by deploying containerized, preconfigured installs of openstack, that connect to the bare metal portions of the system through standardized API’s like libvirt, which are easy to configure on the base system using ansible. On the other hand, because openstack is much more complex, it is easy to simply make a preconfigured container image, and distribute it out for people to use."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#what-is-nixos",
    "href": "projects/openstack-on-nixos/index.html#what-is-nixos",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Nixos is an operating system that uses the nix package manager to install packages, but also the nix language for configuration. Because of this, it is a form of configuration as code.\nFor example, I used nix’s ability to create a shell environment to create a shell environment with quarto on linux\nI asked, and searched around, looking for if there was a Nixos way to set up something like proxmox or openstack.\nI first asked, but then I realized that I could search github for the nix programming language, and keywords I desired. I did so, and I found someone’s lxdware configurations\n\n\nShow someone else’s configs for lxdware on nixos\n\n{ config, pkgs, lib, ... }: {\n  systemd.services.docker-create-network-lxdware = {\n    enable = true;\n    description = \"Create lxdware docker network\";\n    path = [ pkgs.docker ];\n    serviceConfig = {\n      Type = \"oneshot\";\n      RemainAfterExit = \"yes\";\n      ExecStart = pkgs.writeScript \"docker-create-network-lxdware\" ''\n        #! ${pkgs.runtimeShell} -e\n        ${pkgs.docker}/bin/docker network create lxdware || true\n      '';\n    };\n    after = [ \"network-online.target\" ];\n    wantedBy = [ \"multi-user.target\" ];\n  };\n\n  virtualisation.oci-containers.containers.\"lxdware\" = {\n    autoStart = true;\n    image = \"docker.io/lxdware/dashboard:latest\";\n    volumes = [ \"/services/lxdware/lxdware:/var/lxdware\" ];\n    dependsOn = [ \"create-network-lxdware\" ];\n    extraOptions = [\n      # networks\n      \"--network=lxdware\"\n      # labels\n      \"--label\"\n      \"traefik.enable=true\"\n      \"--label\"\n      \"traefik.docker.network=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.rule=Host(`lxd.local.bspwr.com`)\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.entrypoints=websecure\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls=true\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls.certresolver=letsencrypt\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.service=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.middlewares=local-allowlist@file, default@file\"\n      \"--label\"\n      \"traefik.http.services.lxdware.loadbalancer.server.port=80\"\n    ];\n  };\n}\n\nLxdware is a web based frontend for LXD, a type of hyperconverged infrastructure. LXD is a daemon, or background process, for managing containers (ran via LXC), virtual machines, and to an extent, virtualized storage. It appealed to me, when I was searching for a hyperconverged infrastructure solution for my home lab.\n\nIt’s feature rich, and mature. However, I dislike the particular implementation used in the above configuration. They ran lxdware, in a docker container. This works, and probably works well, but this isn’t a very nixos way of doing things. Nix offers reproducibility, so docker isn’t needed, and is generally frowned upon because it brings some disadvantages. I wanted to configure lxdware using nix myself.\nI later looked at openstack, because I wanted to see if there was an ideal way to configure this with nixos. Nix makes it so easy to configure so many other services, just a few lines of nix code in the configuration.nix file to set up webservers, or other services.\nIn fact, people even discussed this in a thread posted on the Nixos discourse forums\n\nIs there anyone actively working on being able to run an OpenStack cloud using NixOS? Shouldn’t “we” be able to do what the Kayobe project does but without the Ansible stuff? https://docs.openstack.org/kayobe/latest/configuration/reference/kolla-ansible.html 9\n\nOne person replied, saying that they had done a bit, but work had “rotten since”. And indeed, when I searched around github, I found an old project, in a github repo titled nixstack\nThis person, or group of people, had packaged openstack to be easy to enable. But as I read through the code, I realized that the app is from the era of python2, so this code is really old."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#pip2nix",
    "href": "projects/openstack-on-nixos/index.html#pip2nix",
    "title": "Packaging Openstack on Nixos",
    "section": "Pip2nix",
    "text": "Pip2nix\nI install and setup pip2nix and attempt to use it. It first complains about python36 being too old, but after I switch to the python39 version of pip2nix:\n[nix-shell:~/vscode/keystone]$ pip2nix generate .\nProcessing /home/moonpie/vscode/keystone\n... # lots of extraneous output\npip2nix generate seems to take the same inputs as pip, so to install a python package from the current working directory, you could usually do pip install ., but here I do a pip2nix generate . instead. This generates a python-packages.nix file with all the python packages, including openstack keystone.\nI played around with trying to install openstack keystone in a nix-shell environment.\n{\n        pkgs ? import &lt;nixpkgs&gt; {}\n} : \nwith import ./python-packages.nix {inherit pkgs;};\nlet\n        keystone-nix = callPackage keystone;\nin\npkgs.mkShell {\n        packages = [ keystone-nix ];\n}\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       error: function 'anonymous lambda' called without required argument 'fetchurl'\n\n       at /home/moonpie/vscode/keystone/python-packages.nix:4:1:\n\n            3|\n            4| { pkgs, fetchurl, fetchgit, fetchhg }:\n             | ^\n            5|\n\nI remember getting a similar error when I was trying to package quarto, and I was told to use callPackage. I don’t think I am using callPackage correctly here.\nI tried tinkering with some code that I got from searching github\n{\n    pkgs ? import &lt;nixpkgs&gt; {}\n} : \nwith pkgs;\nlet\n  python = python3;\n  pythonPackages = python.pkgs;\n\n  # generated with `pip2nix generate -r requirements.txt`\n  generatePipPackages = import ./python-packages.nix {\n    inherit pkgs;\n    inherit (pkgs) fetchurl fetchgit fetchhg;\n  };\n\n  pipPackages =  generatePipPackages pipPackages pythonPackages;\nin\npkgs.mkShell {\n    packages = with pipPackages; [ keystone ];\n}\nBut this errors:\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: attribute 'setuptools' missing\n\n       at /home/moonpie/vscode/keystone/python-packages.nix:106:7:\n\n          105|     propagatedBuildInputs = [\n          106|       self.\"setuptools\"\n             |       ^\n          107|       self.\"six\"\n\nI don’t really know why this errors. o this is good advice. I just graduated, and although I have an internship I will need to do this anix-shell error: … in the left operand of the update (//) operator\n     at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/lib/fixed-points.nix:69:64:\n\n       68|   #\n       69|   extends = f: rattrs: self: let super = rattrs self; in super // f self super;\n         |                                                                ^\n       70|\n\n   … in the left operand of the update (//) operator\n\n     at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/lib/fixed-points.nix:69:64:\n\n       68|   #\n       69|   extends = f: rattrs: self: let super = rattrs self; in super // f self super;\n         |                                                                ^\n       70|\n\n   (stack trace truncated; use '--show-trace' to show the full trace)\n\n   error: function 'anonymous lambda' called with unexpected argument 'self'\n\n   at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/development/interpreters/python/passthrufun.nix:37:6:\n\n       36|     # - applies overrides from `packageOverrides` and `pythonPackagesOverlays`.\n       37|     ({ pkgs, stdenv, python, overrides }: let\n         |      ^\n       38|       pythonPackagesFun = import ./python-packages-base.nix {\n\n&lt;/details&gt;\n\nI literally changed nothing. Except for adding setuptools to the start of the requirements.txt, but I still got a differing error than the one I got yesterday. \n\n\nI recloned the keystone repo (reverting back to the default requirements.txt file), and regenerated the nix files using pip2nix, however I still get the same error. Weird.\n\n2 days later, I still get the same error. So I decided to try another tool. Although I am aware that I could, in theory package every python package manually, I have opted not to do that when programattic solutions exist. Manual packaging is a last resort.\n\n## Mach-nix\n\n### Flying Blind\n\nSo I tried another tool, [mach-nix](https://github.com/DavHau/mach-nix), which is unmaintained, but worked when I tried it a little bit ago.\n\nFor some reason, trying to get the non-flake version of the package didn't work:\n\n&lt;details&gt;&lt;summary&gt;Show error&lt;/summary&gt;\n\n```{.default .code-overflow-wrap}\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell -p '(callPackage (fetchTarball https://github.com/DavHau/mach-nix/tarball/3.5.0) {}).mach-nix'\nthis derivation will be built:\n  /nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv\nbuilding '/nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv'...\nSourcing python-remove-tests-dir-hook\nSourcing python-catch-conflicts-hook.sh\n...\n...\n/nix/store/74l4x6m97bvry7ccxv51h952ayvg2j46-stdenv-linux/setup: line 1596: pop_var_context: head of shell_variables not a function context\nerror: builder for '/nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv' failed with exit code 1;\n       last 10 log lines:\n       &gt;   File \"/nix/store/pv3psrncam37dc8n1v2q80jfvnw601ln-python3.9-setuptools-67.4.0/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 305, in get_finalized_command\n       &gt;     cmd_obj.ensure_fio this is good advice. I just graduated, and although I have an internship I will need to do this ations()\n       &gt;   File \"/nix/store/pv3psrncam37dc8n1v2q80jfvnw601ln-python3.9-setuptools-67.4.0/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 220, in finalize_options\n       &gt;     parsed_version = packaging.version.Version(self.egg_version)\n       &gt;   File \"/nix/store/pv3psrncam37dc8n1v2q80jfvnw601ln-python3.9-setuptools-67.4.0/lib/python3.9/site-packages/setuptools/_vendor/packaging/version.py\", line 197, in __init__\n       &gt;     raise InvalidVersion(f\"Invalid version: '{version}'\")\n       &gt; setuptools.extern.packaging.version.InvalidVersion: Invalid version: 'master'\n       &gt; /nix/store/74l4x6m97bvry7ccxv51h952ayvg2j46-stdenv-linux/setup: line 1596: pop_var_context: head of shell_variables not a function context\n       For full logs, run 'nix-store -l /nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv'.\n\nSo I had to use the flake version of mach-nix\n\nmoonpie@localhost:~/vscode/keystone&gt; nix shell github:DavHau/mach-nix\nmoonpie@localhost:~/vscode/keystone&gt; mach-nix env ./env -r requirements.txt\npath is '/nix/store/ahaz90hy6lins0a56mdivrd2fjj3rcb5-554d2d8aa25b6e583575459c297ec23750adb6cb'\n\nInitialized python environment in:                ./env\nTo change python requirements, modify the file:   ./env/requirements.txt\n\nTo activate the environment, execute:             nix-shell ./env\nHowever, when I actually tried to activate the environment, it simply stalled forever, until I forcefully exited the program using Control + C. After which, it would spit out an error:\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell ./env\n^Cerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/5n402azp0s9vza4rziv4z5y88v2cv1mq-nixpkgs/pkgs/stdenv/generic/make-derivation.nix:205:7\n\n       … while evaluating attribute 'buildInputs' of derivation 'nix-shell'\n\n         at /nix/store/5n402azp0s9vza4rziv4z5y88v2cv1mq-nixpkgs/pkgs/stdenv/generic/make-derivation.nix:247:7:\n\n          246|       depsHostHost                = lib.elemAt (lib.elemAt dependencies 1) 0;\n          247|       buildInputs                 = lib.elemAt (lib.elemAt dependencies 1) 1;\n             |       ^\n          248|       depsTargetTarget            = lib.elemAt (lib.elemAt dependencies 2) 0;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: download of 'https://github.com/DavHau/pypi-deps-db/tarball/e00b22ead9d3534ba1c448e1af3076af6b234acf' was interrupted\nI could report an issue, but I decided to simply try the new project that the developer is working on instead, dream2nix.\n\nA kind user helped me.\nA user on matrix responded to me asking for help with dream2nix\nThey said:\n\nHave you tried to use mach-nix pinned to rev = “65266b5cc867fec2cb6a25409dd7cd12251f6107”; and versionPy ? “39” to use the req.txt file from keystone 23.0.0 (works for me)\n\nSo I created a small shell.nix to that simply calls my derivation:\n\n\nshell.nix\n\n{\n    pkgs ? import &lt;nixpkgs&gt; {},\n    \n} :\nlet \n    okeystone = pkgs.callPackage ./keystone.nix {}; # okeystone becuase packages with the name keystone already exists.\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ okeystone ];\n    }\n\nAnd my derivation\n\n\nShow derivation\n\n\n\nkeystone.nix\n\n{\n    nixpkgs ? import &lt;nipxkgs&gt;,\n    #fetchFromGithub,\n    fetchgit,\n    python3\n} : \n\nlet\n    mach-nix = import (fetchgit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {};\nin\n\nmach-nix.buildPythonApplication {\n    pname =  \"keystone\";\n    version = \"placeholder\";\n    src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n    };\n}\n\n\nAnd I get the same error as above, after I forcefully quit the stalled program.\nI realized a little bit later that I forgot to declare versionPy ? \"39\".\nLater, I finally made some progress.\n\n\nShow\n\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    fetchgit,\n    versionPy ? \"39\"\n} : \n\nlet\n    #python = pkgs.\"python${versionPy}Full\";\n    #pythonPackages = pkgs.\"python${versionPy}Packages\";\n    mach-nix = import (fetchGit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    #sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {\n    python = \"python39\";\n    pythonPackages = \"python39Packages\";\n\n    pypiDataRev = \"e9571cac25d2f509e44fec9dc94a3703a40126ff\";\n    pypiDataSha256 = \"1rbb0yx5kjn0j6lk0ml163227swji8abvq0krynqyi759ixirxd5\";\n    };\nin\n\nmach-nix.buildPythonApplication {\n    pname =  \"keystone\";\n    version = \"placeholder\";\n\n    src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };\n    \n}\n\n\nThis doesn’t fail instantly, but rather does some stuff before giving me a differing, but easy to understand error.\n\n\nShow error\n\nmoonpie@localhost:~/vscode/moonpiedumplings.github.io/projects/openstack-on-nixos/mach-nix&gt; nix-shell\nbuilding '/nix/store/lwpd31yfdr3jcpi8f1ik21dgyhp063iw-python3-3.9.9-env.drv'...\ncreated 223 symlinks in user environment\nbuilding '/nix/store/iag09dlfh5z4l4f5zq87f38wpwqhdg5h-python3-3.9.9-env-patched.drv'...\nFile /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/distutils/core.py is read-only; trying to patch anyway\npatching file /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/distutils/core.py\n/nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/site-packages/setuptools/__init__.py\nFile /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/site-packages/setuptools/__init__.py is read-only; trying to patch anyway\npatching file /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/site-packages/setuptools/__init__.py\nHunk #1 succeeded at 239 (offset -12 lines).\nbuilding '/nix/store/zj70xbgg1gm8kils98insy3xzjzmphqx-package-requirements.drv'...\nunpacking sources\nunpacking source archive /nix/store/22jqj6c8na5k547qjrhlpjpagzq3wvhc-keystone-040e6d0\nsource root is keystone-040e6d0\ninstalling\nextracting dependencies\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: Automatic requirements extraction failed for keystone:placeholder.\n       Please manually specify 'requirements'\n\nClear, and easy to understand. For whatever reason, mach-nix is unable to get requirements.txt from keystone, and it wants me to manually specify them.\nI will see if there is a way to get around this.\nSo I found a way to get around this:\nmach-nix.buildPythonApplication rec {\n    pname =  \"keystone\";\n    version = \"placeholder\";\n\n    src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };\n    requirements =  builtins.readFile \"${src}/requirements.txt\";\n    \n}\nBut of course, this still errors.\nAfter a lot of compiling and building, I get this:\n\n\nShow error\n\nERROR:root:Error parsing\nTraceback (most recent call last):\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/core.py\", line 111, in pbr\n    attrs = util.cfg_to_args(path, dist.script_args)\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/util.py\", line 272, in cfg_to_args\n    pbr.hooks.setup_hook(config)\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/__init__.py\", line 25, in setup_hook\n    metadata_config.run()\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/base.py\", line 27, in run\n    self.hook()\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/metadata.py\", line 25, in hook\n    self.config['version'] = packaging.get_version(\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/packaging.py\", line 874, in get_version\n    raise Exception(\"Versioning for this project requires either an sdist\"\nException: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\nerror in setup command: Error parsing /build/keystone-040e6d0/setup.cfg: Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\nerror: builder for '/nix/store/gyj2647z1nk1g0dq5wh8jbf1lhv91b9x-python3.9-keystone-placeholder.drv' failed with exit code 1;\n       last 10 log lines:\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/__init__.py\", line 25, in setup_hook\n       &gt;     metadata_config.run()\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/base.py\", line 27, in run\n       &gt;     self.hook()\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/metadata.py\", line 25, in hook\n       &gt;     self.config['version'] = packaging.get_version(\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/packaging.py\", line 874, in get_version\n       &gt;     raise Exception(\"Versioning for this project requires either an sdist\"\n       &gt; Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n       &gt; error in setup command: Error parsing /build/keystone-040e6d0/setup.cfg: Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n       For full logs, run 'nix-store -l /nix/store/gyj2647z1nk1g0dq5wh8jbf1lhv91b9x-python3.9-keystone-placeholder.drv'.\n\nAt least the error is explicit. I will look into this more later.\nI found something that worked. Looking at the old nixstack code, there is an attribute that can be set:\nPBR_VERSION = \"${version}\";\nand this works. My current keystone.nix:\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    fetchgit,\n    fetchPypi,\n    #fetchFromGitHub,\n    versionPy ? \"39\",\n} : \n\nlet\n    #python = pkgs.\"python${versionPy}Full\";\n    #pythonPackages = pkgs.\"python${versionPy}Packages\";\n    mach-nix = import (fetchgit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {\n    python = \"python39\";\n    pythonPackages = \"python39Packages\";\n\n    pypiDataRev = \"e9571cac25d2f509e44fec9dc94a3703a40126ff\";\n    pypiDataSha256 = \"1rbb0yx5kjn0j6lk0ml163227swji8abvq0krynqyi759ixirxd5\";\n    };\nin\n\nmach-nix.buildPythonPackage rec {\n    pname =  \"keystone\";\n    version = \"23.0.0\";\n    PBR_VERSION = \"${version}\";\n    src = builtins.fetchGit {\n        url = \"https://github.com/openstack/keystone/\";\n        ref = \"refs/tags/23.0.0\";\n        rev = \"c08d97672dcd40f8d927f91e59049053cfe3b5e4\";\n    };\n    requirements =  builtins.readFile \"${src}/requirements.txt\";\n    \n}\n\nBut I don’t really want to use mach-nix, as it is unmaintained. I will see if there is a way to set this up with dream2nix.\nFor those trying to help me, here is a live version, updated every git push, of the files I am working on, because I don’t usually update my blog as I work on things, only after.\n\n\nShow\n\n\n\nshell.nix\n\n{\n    pkgs ? import &lt;nixpkgs&gt; {},\n    \n} :\nlet \n    okeystone = pkgs.callPackage ./keystone.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ okeystone ];\n    }\n\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    fetchgit,\n    fetchPypi,\n    #fetchFromGitHub,\n    versionPy ? \"39\",\n} : \n\nlet\n    #python = pkgs.\"python${versionPy}Full\";\n    #pythonPackages = pkgs.\"python${versionPy}Packages\";\n    mach-nix = import (fetchgit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {\n    python = \"python39\";\n    pythonPackages = \"python39Packages\";\n\n    pypiDataRev = \"e9571cac25d2f509e44fec9dc94a3703a40126ff\";\n    pypiDataSha256 = \"1rbb0yx5kjn0j6lk0ml163227swji8abvq0krynqyi759ixirxd5\";\n    };\nin\n\nmach-nix.buildPythonPackage rec {\n    pname =  \"keystone\";\n    version = \"23.0.0\";\n    PBR_VERSION = \"${version}\";\n\n    /*src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };*/\n    /*src = fetchTarball {\n        url = \"https://github.com/openstack/keystone/archive/eff960e124e2f28922067800547e23f1931d3c4a.tar.gz\";\n        sha256 = \"\";\n    };*/\n    src = builtins.fetchGit {\n        url = \"https://github.com/openstack/keystone/\";\n        ref = \"refs/tags/23.0.0\";\n        rev = \"c08d97672dcd40f8d927f91e59049053cfe3b5e4\";\n        #sha256 = \"sha256-JYP29APY27BpX9GSyayW/y7rskdn8zW5mVsjdBXjCus=\";\n    };\n    /*src = fetchPypi {\n        inherit pname version;\n        sha256 = \"sha256-t0ravo9+H2nYcoGkvoxn5YxHOTf68vSon+VTJFn6INY=\";\n    };*/\n   /*src = fetchFromGitHub {\n        owner = \"openstack\";\n        repo = \"keystone\";\n        tag = \"23.0.0\";\n   };*/\n   /*src = fetchTarball {\n        url = \"https://github.com/openstack/keystone/archive/refs/tags/23.0.0.tar.gz\";\n        sha256 = \"\";\n    };*/\n    #src = /home/moonpie/vscode/keystone; # currently in correct tag\n\n    requirements =  builtins.readFile \"${src}/requirements.txt\";\n    \n}"
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#dream2nix",
    "href": "projects/openstack-on-nixos/index.html#dream2nix",
    "title": "Packaging Openstack on Nixos",
    "section": "Dream2nix",
    "text": "Dream2nix\nI decided to follow the their python project instructions.\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix run .#resolveImpure\nwarning: Git tree '/home/moonpie/vscode/keystone' is dirty\nResolving:: Name: main; Subsystem: python; relPath: \n/tmp/tmp.rB9TAH7neG ~/vscode/keystone\nreading setup requirements from pyproject.toml\nError: [Errno 2] No such file or directory: './source/pyproject.toml'\ninstall setup dependencies from extraSetupDeps\n\n[notice] A new release of pip available: 22.2.2 -&gt; 23.1.2\n[notice] To update, run: /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/bin/python3.10 -m pip install --upgrade pip\ndownload setup dependencies from pyproject.toml\n\n[notice] A new release of pip available: 22.2.2 -&gt; 23.1.2\n[notice] To update, run: /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/bin/python3.10 -m pip install --upgrade pip\ndownload files according to requirements\nrealpath: ./install/lib/python3.10/site-packages: No such file or directory\nProcessing ./source\n  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  × python setup.py egg_info did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [19 lines of output]\n      /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/lib/python3.10/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n        warnings.warn(\n      WARNING: The wheel package is not available.\n      Error parsing\n      Traceback (most recent call last):\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/core.py\", line 105, in pbr\n          attrs = util.cfg_to_args(path, dist.script_args)\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/util.py\", line 272, in cfg_to_args\n          pbr.hooks.setup_hook(config)\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/hooks/__init__.py\", line 25, in setup_hook\n          metadata_config.run()\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/hooks/base.py\", line 27, in run\n          self.hook()\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/hooks/metadata.py\", line 25, in hook\n          self.config['version'] = packaging.get_version(\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/packaging.py\", line 874, in get_version\n          raise Exception(\"Versioning for this project requires either an sdist\"\n      Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n      error in setup command: Error parsing /tmp/tmp.rB9TAH7neG/source/setup.cfg: Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─&gt; See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n[notice] A new release of pip available: 22.2.2 -&gt; 23.1.2\n[notice] To update, run: /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/bin/python3.10 -m pip install --upgrade pip\n\nAnd an error. Interestingly, this is the exact same error that I got with mach-nix. This means that the error proabbly has the same solution, and if I find it, I’d like to stick to dream2nix is it is actively maintained, unlike mach-nix.\nSo I started doing some research. The first thing was something another user on matrix sent to me, a stackoverflow post\nThis post had several interesting ramifications. I was recommended to attempt to set the version of keystone using an environment variable, but I noticed that PBR, the tool used to build/install keystone, should be able to get teh current version from git tags, without that hassle. So I did more research as to why this wasn’t working.\nI found a keystone specific bug, on launchpad, a bug reporting site. Someone not using nix encountered my exact error. Linked below was a fix, created and merged into other openstack repos, but not the keystone one. Apparently, git was changed in such a way that it cannot do any operations on a repo not owned by the current user, in order to fix a security vulnerability. This may not bug normal users, but with nix, the git repo is downloaded into the nix store, where is is owned by the root user. Then, the nixbld user’s operate on the store paths and files. This means that nix is probably encountering this specific git behavior. I am guessing my next step is do do something that makes it okay for git to operate on git repo’s not owned by the current user.\nYou should be able to adjust git configs to add “safe” directories, that is, declare directories that are ok to operate on not by the current user.\nOne stackoverflow answer recommended this:\ngit config --global --add safe.directory *\nBut this edits the git configuration files for that user. There might be a way to edit just for that repository, since git stores some configs in the repo itself, but I doubt that, as that creates a very easy workaround for the fix designed to squash the security issue.\nThere is the git config --system option, but if I ran that inside of a nix chroot, where would it store it?\nMaybe I could use a modified version of git with nix?\nAnother thing to consider is, for the dream2nix approach, does it store a copy of the repository in the nix store, or does it work directly on the repository I run it in?\nPreviously, I had run this all on my newer laptop, which has a multi user install, meaning the /nix directory is owned by root, and the nixbld* user’s do the operations. However, I decided to try it on my other laptop, which has a single user nix install. This means that the nix store is owned by my user moonpie, and the operations are also done by my user, moonpie. However, I encountered the same error.\nOkay, I don’t need to mess with git, I just need to set the PBR_VERSION environment variable. How can I set environment variables with dream2nix?\nOkay, so on the matrix thread where I am being helped, I received this as a reply when I asked how to set up environment variables in dream2nix:\n\nBasically, dream2nix doesn’t have this feature yet. I don’t know where I am going to go from here. I think I should simply wait until dream2nix is more mature, and put this project on hold, although I might see if there is a way to hack it, although my understanding of the nix langauge is still lacking."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#poetry2nix",
    "href": "projects/openstack-on-nixos/index.html#poetry2nix",
    "title": "Packaging Openstack on Nixos",
    "section": "Poetry2nix",
    "text": "Poetry2nix\nPoetry is a python dependency manager similar to nix. poetry2nix uses poetry’s specifications to do things like download package, package applications, or create shell environments using nix.\nFirst I needed to convert the requirements.txt to poetry.\nI used the dephell tool to do so. Specifically, dephell deps convert\ndephell deps convert --from pip --to poetry\nAfter that, I used poetry2nix to create a shell environment, to test if the dependencies were downloaded.\n\n\nshell.nix\n\n{ \npkgs ? import &lt;nixpkgs&gt; {},\npoetry2nix ? import &lt;poetry2nix&gt; {}\n} :\n\npoetry2nix.mkPoetryEnv {\n    projectDir = ./.;\n}\n\nAfter that, using pip list to list the python packages that pip can see confirms that this works to download all dependencies.\nBut I don’t really like doing this, because dephell is depreceated. So I’m going to look into some other converion tools, tools to convert the pip file format/requirements.txt to the poetry file format/pyproject.toml.\n\n\n\nProject\nNotes\n\n\n\n\ndephell\nWorks, but is archive/unmaintained, and complains about a piece being deprecated when ran…\n\n\nstanza\nLast commit was a year ago\n\n\npoeareq\nmaintained by one person, last commit a year ago. Present in pypi\n\n\ntigerhawk’s gist\njust a github gist, a user complimented in a comment one month ago. But the script was last updated in 2019\n\n\nvarious shell one liners\nprovided by stackoverflow. They work, but are hacky, and apparently have edge cases they don’t cover.\n\n\npip-poetry-migration\nuntested/unresearched\n\n\npoetry-import\nuntested/unresearched. In pypi.\n\n\npoetrify\nlast updated 2 years ago. Says that requirements.txt is still in trial in description.\n\n\npoetry import plugin\npoetry plugin, so it may be easier to install/automate. Last updated 9 months ago.\n\n\nmicropipenv\nI don’t know if it can actually do what I want. last updated 2 months ago. Maintained by Red Hat.\n\n\n\nHere is a live version of what I am working on for poetry, updated every git push.\n\n\nShow\n\n\n\nshell.nix\n\n{\n    pkgs ? import &lt;nixpkgs&gt; {},\n    \n} :\nlet \n    okeystone = pkgs.callPackage ./keystone.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ okeystone ];\n    }\n\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    #poetry2nix ? (import fetchtarball {\"https://github.com/nix-community/poetry2nix/archive/215afa14f7077ca0610882d6d18ac3454f48fa65.tar.gz\"}),\n    fetchgit,\n} : \n\nlet\n    /*poetryConverter = src: pkgs.runCommand \"${pkg.name}-nixgl-wrapper\" {} ''\n    ${pkgs.poetry}/bin/poetry self add poetry-plugin-import\n    mkdir $out\n    ln -s ${pkg}/* $out\n    rm $out/bin\n    mkdir $out/bin\n  '';*/\n    poetry2nix = import (builtins.fetchTarball {\n    url = \"https://github.com/nix-community/poetry2nix/archive/215afa14f7077ca0610882d6d18ac3454f48fa65.tar.gz\";\n    sha256 = \"0k0blf48ln6bcj7c76cjjcdx41l1ygscpczi2k2d2vqv14a5bzan\";\n  }) {};\nin\npoetry2nix.mkPoetryApplication {\n    /*src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };\n    */\n    src = \"$HOME/vscode/keystone\"; # Not gonna post anything online yet, espcially in such an alpha state. \n}"
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#packaging-them-myself",
    "href": "projects/openstack-on-nixos/index.html#packaging-them-myself",
    "title": "Packaging Openstack on Nixos",
    "section": "Packaging them myself",
    "text": "Packaging them myself\nOkay, I did find something else interesting. When I was looking at one of the openstack dependencies, packaged in nixpkgs, I found something interesting in the meta section:\nmeta = with lib; {\n    description = \"Oslo test framework\";\n    homepage = \"https://github.com/openstack/oslotest\";\n    license = licenses.asl20;\n    maintainers = teams.openstack.members;\n  };\nFor maintainers, nix has “teams,”” or groups of people dedicated to packaging a group of packages. And apparently, they seem to have an openstack team:\nSure enough, in the nixpkgs list of teams, they can be found:\nopenstack = {\n    members = [\n      emilytrau\n      SuperSandro2000\n    ];\n    scope = \"Maintain the ecosystem around OpenStack\";\n    shortName = \"OpenStack\";\n  };\nThese people maintain the ecosystem around openstack, but not openstack itself. Perhaps all openstack dependencies are already packaged?\nI would probably have to do overrides on some of the depedencies to ensure that the versions are correct, since sometimes newer packages have breaking changes, but if stuff is already packaged, that would be great.\nHere is the list of the packages from the requirements.txt. I will link each package to it’s equivalent in nixpkgs.\n\n\nShow list\n\npbr!=2.1.0,&gt;=2.0.0 # Apache-2.0 nixpkgs\nWebOb&gt;=1.7.1 # MIT nixpkgs\nFlask!=0.11,&gt;=1.0.2 # BSD nixpkgs\nFlask-RESTful&gt;=0.3.5 # BSD nixpkgs\ncryptography&gt;=2.7 # BSD/Apache-2.0 nixpkgs\nSQLAlchemy&gt;=1.4.0 # MIT nixpkgs\nsqlalchemy-migrate&gt;=0.13.0 # Apache-2.0 nixpkgs\nstevedore&gt;=1.20.0 # Apache-2.0 nixpkgs\npasslib&gt;=1.7.0 # BSD nixpkgs\npython-keystoneclient&gt;=3.8.0 # Apache-2.0 nixpkgs\nkeystonemiddleware&gt;=7.0.0 # Apache-2.0 NOT FOUND\nbcrypt&gt;=3.1.3 # Apache-2.0 nixpkgs\nscrypt&gt;=0.8.0 # BSD nixpkgs\noslo.cache&gt;=1.26.0 # Apache-2.0 NOT FOUND\noslo.config&gt;=6.8.0 # Apache-2.0 nixpkgs\noslo.context&gt;=2.22.0 # Apache-2.0 nixpkgs\noslo.messaging&gt;=5.29.0 # Apache-2.0 NOT FOUND\noslo.db&gt;=6.0.0 # Apache-2.0 nixpkgs\noslo.i18n&gt;=3.15.3 # Apache-2.0 nixpkgs\noslo.log&gt;=3.44.0 # Apache-2.0 nixpkgs\noslo.middleware&gt;=3.31.0 # Apache-2.0 NOT FOUND\noslo.policy&gt;=3.10.0 # Apache-2.0 NOT FOUND\noslo.serialization!=2.19.1,&gt;=2.18.0 # Apache-2.0 nixpkgs\noslo.upgradecheck&gt;=1.3.0 # Apache-2.0 NOT FOUND\noslo.utils&gt;=3.33.0 # Apache-2.0 nixpkgs\noauthlib&gt;=0.6.2 # BSD nixpkgs\npysaml2&gt;=5.0.0 nixpkgs\nPyJWT&gt;=1.6.1 # MIT nixpkgs\ndogpile.cache&gt;=1.0.2 # BSD nixpkgs\njsonschema&gt;=3.2.0 # MIT nixpkgs\npycadf!=2.0.0,&gt;=1.1.0 # Apache-2.0 NOT FOUND\nmsgpack&gt;=0.5.0 # Apache-2.0 nixpkgs\nosprofiler&gt;=1.4.0 # Apache-2.0 NOT FOUND\npytz&gt;=2013.6 # MIT nipxkgs\n\nHowever, I needed a way to check the compatibility of a python package, to a version of python. Thankfully, I found an easyish way to do so.\npip install --python-version 311 --no-deps --target test-pkg packagename==missing\nBecause openstack keystone is actually packaged in pypi repos, downloaded by pip, this works:\nmoonpie@localhost:~/vscode/keystone&gt; pip install --python-version 311 --no-deps --target test-pkg keystone==missing\nERROR: Could not find a version that satisfies the requirement keystone==missing (from versions: 12.0.2, 12.0.3, 13.0.2, 13.0.3, 13.0.4, 14.0.0, 14.0.1, 14.1.0, 14.2.0, 15.0.0.0rc1, 15.0.0.0rc2, 15.0.0, 15.0.1, 16.0.0.0rc1, 16.0.0.0rc2, 16.0.0, 16.0.1, 16.0.2, 17.0.0.0rc1, 17.0.0.0rc2, 17.0.0, 17.0.1, 18.0.0.0rc1, 18.0.0, 18.1.0, 19.0.0.0rc1, 19.0.0.0rc2, 19.0.0, 19.0.1, 20.0.0.0rc1, 20.0.0, 20.0.1, 21.0.0.0rc1, 21.0.0, 22.0.0.0rc1, 22.0.0, 23.0.0.0rc1, 23.0.0)\nERROR: No matching distribution found for keystone==missing\nSo openstack supports python 3.11, which is good. Nix doesn’t have many of the above libraries packaged for python 3.8 and below."
  },
  {
    "objectID": "resume/index.html",
    "href": "resume/index.html",
    "title": "Jeffrey Fonseca — Resume",
    "section": "",
    "text": "Resume QR Code\n\n\nShow QR Code\n\n\n\n    \n\n\nContact me\nEmail: jeff.fonseca777 [at] gmail.com\n\n\nSummary\nAvid user of Linux and related technologies for 5 years, intimately familiar with them through personal projects, daily usage, and teaching others. Proficient in both operations, and technical writing/documentation.\nI have a site where I document all my work, write about tech, and post guides for other to use: https://moonpiedumplings.github.io/ Many things on this resume will link to the relevant articles on my site.\n\n\nProfessional Experience\n\n\n\n\n\n\n\nPaid Internship at Cirrascale Cloud\n\n\nDate: 7/10/2023 - 8/26/2023\n\n\n\nInstalled PFSense on proxmox\nUsed packer to automate creation of a MAAS image that deploys proxmox: https://github.com/moonpiedumplings/proxmox-maas/\nDocumented K3's kubernetes, vcluster, and Kata-containers on Nvidia GPU's for machine learning operations: link to docs\nAssisted in building physical servers, including doing tasks like installing storage, or Nvidia GPU's.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWUOTT — Behavioral Health Clinic\n\n\nDate: 2016 - Still ongoing. It's my mother's business, so I help out a little bit when needed.\n\n\n\nGeneral helpdesk and tech support\nDeployed Meshcentral on an Ubuntu VPS using Docker so I could access, manage, and troubleshoot  Windows devices remotely\n\n\n\n\n\n\n\n\n\nIndependent Organizations\n\n\n\n\n\n\n\nLayer8 Club at Cal State Northridge\n\n\nDate: Fall 2023 to Ongoing\n\n\nCSUN competes in a cybersecurity competition called CCDC. Much of the work here is for that.\n\n\n\nAutomated many mundane linux tasks using ansible View repo stats. View linux roles, most of which is my work\nCreated a guide for reproducible development and testing environments on various host operating systems, using Nix and Vagrant\nIntroduced and taught people Linux, concepts, and related software.\nDid a presentation on containers that run on linux.\n\n\n\n\n\n\n\nPersonal/Educational Projects\n\n\n\n\n\n\n\n\nDeployed a Minecraft server using Podman to an Oracle Cloud VPS\nBuilt CI/CD Pipelines using GitHub actions and Docker, to deploy to an AWS EC2 VPS, for a class project for AP CSP\nDockerized Node.js and Python applications\nUsed the Nix package manager to distribute packages and applications to students in AP CSP\nWrote technical documentation to assist students and teachers in AP CSP (For example: a short guide on duckdns)\nDeployed and documented the deployment of Kasmweb: see relevant blog post\nWorked towards packaging KasmVNC and OpenStack using Nix.\nDeployed a point to site wireguard connection between my router and a vps\nInstalled Openstack on Rocky Linux using Kolla-Ansible\n\n\n\n\n\n\nCurrent personal projects can be found on my website, where I document my work as go. I make it a point to pick challenges beyond my current ability. I don't always succeed, but I always grow in the process.\n\n\nEducation\nThe items in this section are in chronological order, things I did first to things I did most recently.\n\n\nPalomar Community College\n\nIntro to OS/Hardware Fundamentals (CSNT 110)\n\nGraduated from Del Norte High School\n\nAP Calculus AB\nAP Computer Science Principles (AP CSP)\n\nCal State Northridge Computer Science BS — expected graduation 05/27\n\nCurrently Enrolled in:\n\nIntroduction to Algorithms and Programming (COMP 110)\nCalculus 2 (CALC150B)"
  }
]