[
  {
    "objectID": "projects/setting-up-kasm/index.html",
    "href": "projects/setting-up-kasm/index.html",
    "title": "Kasmweb setup",
    "section": "",
    "text": "School chromebooks cannot be used for computer science. Due to a content blocking setup that is more restrictive than the Great Firewall of China, school chromebooks cannot install the necessary digital tools for software development.\nCurrently, you must have your own device to be able to participate in computer science classes. Students who are unable to obtain their own device for whatever reason are denied participation.\nAlthough getting the chromebooks unlocked to be able to install software is a complex, potentially legal problem, there are alternatives.\nKasm is a remote desktop software. It runs a computer on a remote server, that can be accessed through a browser, or a chromebook. Because there are no restrictions on what can be installed when using Kasm, it makes it possible to use development tools on a chromebook. This blog post is me optimizing kasm so that it is more resource efficient, and also enabling software development tools to be easily installed on it.\nRoadblocks/steps:"
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "href": "projects/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "title": "Kasmweb setup",
    "section": "Turns out, memory deduplication is on by default for docker containers",
    "text": "Turns out, memory deduplication is on by default for docker containers\nGithub issue where someone asked about this. The documents linked were very unclear, so I’ll break it down.\nIf you are using overlayfs or aufs, you have memory deduplication. If you are using other storage drivers, you sacrifice memory for more i/o (write/read) performance.\nFrom here:\n\nOn my ubuntu virtual machine, and the AWS ubuntu machine we are working on, Overlay2 is the storage driver:"
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#kernel-same-page-merging",
    "href": "projects/setting-up-kasm/index.html#kernel-same-page-merging",
    "title": "Kasmweb setup",
    "section": "Kernel Same page merging",
    "text": "Kernel Same page merging\nPreviously, I tried instructions from here: https://wiki.openvz.org/KSM_(kernel_same-page_merging). However, I noticed only a minimal space saved using the LD_PRELOAD steps. Not useful.\nI then tried cachyos fork of uksmd: https://github.com/CachyOS/uksmd, a daemon to go through userspace tasks and dedupe them.\nOnly works with a kernel that has the pmadv_ksm() syscall. Exists in most kernels optimized for desktop usage, like linux-zen, linux-liqourix, or pf-kernel (the original creators of uksmd)\nTo check if your currently running kernel has the feature:\n\non Archlinux, check if the files sys_enter_pmadv_ksm and sys_exit_pmadv_ksm exist in /sys/kernel/debug/tracing/events/syscalls (default does not have this feature, but linux-zen does)\non Ubuntu check if lines containing pmadv exist in the file /proc/kallsyms\n\n\n\n\nuksmstats\n\n\nHalf a gig of ram saved on a normal desktop. Expect to see much more when multiple almost identical docker containers are launched. Very useful. It saves a lot of ram. However, there might be a better way for docker, without jumping through hoops.\nDoes cost a miniscule amount of cpu power, but we have more cpu power and less ram on our servers.\nTo install uksmd on ubuntu, you need to switch kernels.\n\nCompiling UKSMD\nSteps to do so on Ubuntu 22 (you must have switched kernels):\nsudo apt-get install debhelper build-essential dh-make meson pkg-config libprocps-dev libcap-ng-dev # I think it can either be pkg-conf or pkg-config\ngit clone https://github.com/insilications/uksmd-clr\nRename the directory to be something compatible with below steps, like uksmd-1 before you cd into it.\nFollow steps from here\ndh_make --createorig\ndh_auto_configure --buildsystem=meson\ndpkg-buildpackage -rfakeroot -us -uc -b\nThe debian package will be build in the directory above the source directory.\nInstall your debian package!\nIf you want the uksmdstats command for monitoring purposes, you can only get it from the cachyos github (or make your own, it’s just a shell script).\nsudo curl https://raw.githubusercontent.com/CachyOS/uksmd/master/uksmdstats -o /usr/bin/uksmdstats\n\n\nSwitching Kernels\nstatus: complete\ncurl 'https://liquorix.net/install-liquorix.sh' | sudo bash from the liqourix kernel website\nI checked if liqourix has the necessary features, and yes it does.\n\nSetting the Default Kernel\nstatus: researching\nReddit post where I ask how to set default kernel in grub\nOn that reddit post, I talk about some flawed solutions I have found. Mainly they don’t seem to be truly persistent, not surviving kernel updates, updates of grub, or installation of new kernels. This endeavor is pretty risky, because a broken grub means we will have no choice but to delete our aws and start over. I need a 100% solution.\nAfter grilling chatgpt through 3 wrong answers, which chatgpt presented with the absolute confidence that an AI has, it finally presented me a solution that seems like it doesn’t have a risk of breaking the AWS system we are working on.\n\nI need to make some adjustments, but I should be able to select for the term “liqourix” while removing the term “recovery” to select the correct kernel (but not the recovery kernel) with complete consistency even through kernel updates, grub updates, or installation of new kernels.\nI searched around for how to do this, but I eventually gave up and asked chatgpt again, getting this:\n\nMy 20_linux_xen is not the same as what chatgpt wants, so I asked again, and it gave me this code to put in /etc/default/grub:\n# Set the default menu entry based on the title of the menu entry\n# that contains the word \"liqourix\" while ignoring the term \"recovery\"\nGRUB_DEFAULT=\"$(grep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}')\"\ngrep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}'\nI will test this in a virtual machine.\n\n\n\nWeaker alternative: ksm_preload\nstatus: won’t be used\ngit clone https://github.com/binfess/ksm_preload\ncmake .\nmake\nsudo make install\nI added LD_PRELOAD=/usr/local/share/ksm_preload/libksm_preload.so to the file /etc/environment\nI haven’t tested the above, but I saw very minimal space saved, only about 0.11 **megabytes* saved, on my desktop. Tests on my sample server are similarly discouraging:\n\nThe above was with 2 kasm sessions open. Nearly useless. In addition to that, uksmd seems to make this completely obsolete."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#zram",
    "href": "projects/setting-up-kasm/index.html#zram",
    "title": "Kasmweb setup",
    "section": "zram",
    "text": "zram\nstatus: immplemented on my personal systems, but not on the ubuntu vm yet.\nTo install zram, sudo apt install systemd-zram-generator\nThen, you can configure zram by editing /etc/systemd/zram-generator.conf\nThis works, but apparently, things in a swap file aren’t deduplicated by uksmd. Rather, zram handles it’s own deduplication, with the compression algorithms. Becuase it must hash pages, this can potentially lead to more cpu usage.\nBecause I don’t know about this, I made yet another reddit post asking about this topic."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#zswap",
    "href": "projects/setting-up-kasm/index.html#zswap",
    "title": "Kasmweb setup",
    "section": "zswap",
    "text": "zswap\nStatus: Partially done, but dropped in favor of zswap\n\nzswap is disabled by default on my ubuntu virtual machine. Odd that both are disabled by default.\nParameters for zswap can be found in /sys/module/zswap/parameters/\nTo set parameters at boot, use kernel boot paremeters, like zswap.enabled=1 zswap.compressor=lz4 zswap.max_pool_percent=20 zswap.zpool=z3fold\nI will need to tinker to see what is the most optimized zswap setup\nzswap has it’s own memory deduplication feature, which is enabled by default on both my ubuntu vm and the aws ubuntu server:\nSee above, my comments about zswap’s memory deduplication feature."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html",
    "href": "projects/openstack-on-nixos/index.html",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Hyperconverged infrastrucucture, is when multiple aspects of computing can all be managed from the same platform. This is usually done with virtualization, like virtualized servers, or virtualized storage.\nPromox Virtual Environment is one of the most popular examples of this for homelabbers, or people who manage their own servers for personal use. It offers a web based interface to configure virtual machines, virtualized storage, and clustering. When researching what software to use to manage my server, I considered proxmox.\nProxmox is based on debian linux, and is very tightly integrated into that ecosystem. It is nearly impossible to run proxmox on any other linux distro, and I disliked this inflexibility.\nOpenstack is an open source, public and private cloud solution, containing hyperconverged infrastructure, and more. It’s used when people don’t want to rely on external cloud solutions, like Amazon Web Services (AWS). For example, a university may decide that it is cheaper to manage and maintain their own cloud than to rely on AWS.\nOpenstack is massive, consisting of multiple components that must be installed and configured independently of eachother, yet set up to work with eachother. Becuase of this, openstack is usually deployed as configuration as code. The two most popular solutions, from my research, openstack-ansible, and kolla-ansible, work by deploying containerized, preconfigured installs of openstack, that connect to the bare metal portions of the system through standardized API’s like libvirt, which are easy to configure on the base system using ansible. On the other hand, because openstack is much more complex, it is easy to simply make a preconfigured container image, and distribute it out for people to use.\n\n\n\nNixos is an operating system that uses the nix package manager to install packages, but also the nix language for configuration. Because of this, it is a form of configuration as code.\nFor example, I used nix’s ability to create a shell environment to create a shell environment with quarto on linux\nI asked, and searched around, looking for if there was a Nixos way to set up something like proxmox or openstack.\nI first asked, but then I realized that I could search github for the nix programming language, and keywords I desired. I did so, and I found someone’s lxdware configurations\n\n\nShow someone else’s configs for lxdware on nixos\n\n{ config, pkgs, lib, ... }: {\n  systemd.services.docker-create-network-lxdware = {\n    enable = true;\n    description = \"Create lxdware docker network\";\n    path = [ pkgs.docker ];\n    serviceConfig = {\n      Type = \"oneshot\";\n      RemainAfterExit = \"yes\";\n      ExecStart = pkgs.writeScript \"docker-create-network-lxdware\" ''\n        #! ${pkgs.runtimeShell} -e\n        ${pkgs.docker}/bin/docker network create lxdware || true\n      '';\n    };\n    after = [ \"network-online.target\" ];\n    wantedBy = [ \"multi-user.target\" ];\n  };\n\n  virtualisation.oci-containers.containers.\"lxdware\" = {\n    autoStart = true;\n    image = \"docker.io/lxdware/dashboard:latest\";\n    volumes = [ \"/services/lxdware/lxdware:/var/lxdware\" ];\n    dependsOn = [ \"create-network-lxdware\" ];\n    extraOptions = [\n      # networks\n      \"--network=lxdware\"\n      # labels\n      \"--label\"\n      \"traefik.enable=true\"\n      \"--label\"\n      \"traefik.docker.network=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.rule=Host(`lxd.local.bspwr.com`)\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.entrypoints=websecure\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls=true\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls.certresolver=letsencrypt\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.service=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.middlewares=local-allowlist@file, default@file\"\n      \"--label\"\n      \"traefik.http.services.lxdware.loadbalancer.server.port=80\"\n    ];\n  };\n}\n\nLxdware is a web based frontend for LXD, a type of hyperconverged infrastructure. LXD is a daemon, or background process, for managing containers (ran via LXC), virtual machines, and to an extent, virtualized storage. It appealed to me, when I was searching for a hyperconverged infrastructure solution for my home lab.\n\nIt’s feature rich, and mature. However, I dislike the particular implementation used in the above configuration. They ran lxdware, in a docker container. This works, and probably works well, but this isn’t a very nixos way of doing things. Nix offers reproducibility, so docker isn’t needed, and is generally frowned upon because it brings some disadvantages. I wanted to configure lxdware using nix myself.\nI later looked at openstack, because I wanted to see if there was an ideal way to configure this with nixos. Nix makes it so easy to configure so many other services, just a few lines of nix code in the configuration.nix file to set up webservers, or other services.\nIn fact, people even discussed this in a thread posted on the Nixos discourse forums\n\nIs there anyone actively working on being able to run an OpenStack cloud using NixOS? Shouldn’t “we” be able to do what the Kayobe project does but without the Ansible stuff? https://docs.openstack.org/kayobe/latest/configuration/reference/kolla-ansible.html 9\n\nOne person replied, saying that they had done a bit, but work had “rotten since”. And indeed, when I searched around github, I found an old project, in a github repo titled nixstack\nThis person, or group of people, had packaged openstack to be easy to enable. But as I read through the code, I realized that the app is from the era of python2, so this code is really old."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#what-is-hyperconverged-infrastrucuture",
    "href": "projects/openstack-on-nixos/index.html#what-is-hyperconverged-infrastrucuture",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Hyperconverged infrastrucucture, is when multiple aspects of computing can all be managed from the same platform. This is usually done with virtualization, like virtualized servers, or virtualized storage.\nPromox Virtual Environment is one of the most popular examples of this for homelabbers, or people who manage their own servers for personal use. It offers a web based interface to configure virtual machines, virtualized storage, and clustering. When researching what software to use to manage my server, I considered proxmox.\nProxmox is based on debian linux, and is very tightly integrated into that ecosystem. It is nearly impossible to run proxmox on any other linux distro, and I disliked this inflexibility.\nOpenstack is an open source, public and private cloud solution, containing hyperconverged infrastructure, and more. It’s used when people don’t want to rely on external cloud solutions, like Amazon Web Services (AWS). For example, a university may decide that it is cheaper to manage and maintain their own cloud than to rely on AWS.\nOpenstack is massive, consisting of multiple components that must be installed and configured independently of eachother, yet set up to work with eachother. Becuase of this, openstack is usually deployed as configuration as code. The two most popular solutions, from my research, openstack-ansible, and kolla-ansible, work by deploying containerized, preconfigured installs of openstack, that connect to the bare metal portions of the system through standardized API’s like libvirt, which are easy to configure on the base system using ansible. On the other hand, because openstack is much more complex, it is easy to simply make a preconfigured container image, and distribute it out for people to use."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#what-is-nixos",
    "href": "projects/openstack-on-nixos/index.html#what-is-nixos",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Nixos is an operating system that uses the nix package manager to install packages, but also the nix language for configuration. Because of this, it is a form of configuration as code.\nFor example, I used nix’s ability to create a shell environment to create a shell environment with quarto on linux\nI asked, and searched around, looking for if there was a Nixos way to set up something like proxmox or openstack.\nI first asked, but then I realized that I could search github for the nix programming language, and keywords I desired. I did so, and I found someone’s lxdware configurations\n\n\nShow someone else’s configs for lxdware on nixos\n\n{ config, pkgs, lib, ... }: {\n  systemd.services.docker-create-network-lxdware = {\n    enable = true;\n    description = \"Create lxdware docker network\";\n    path = [ pkgs.docker ];\n    serviceConfig = {\n      Type = \"oneshot\";\n      RemainAfterExit = \"yes\";\n      ExecStart = pkgs.writeScript \"docker-create-network-lxdware\" ''\n        #! ${pkgs.runtimeShell} -e\n        ${pkgs.docker}/bin/docker network create lxdware || true\n      '';\n    };\n    after = [ \"network-online.target\" ];\n    wantedBy = [ \"multi-user.target\" ];\n  };\n\n  virtualisation.oci-containers.containers.\"lxdware\" = {\n    autoStart = true;\n    image = \"docker.io/lxdware/dashboard:latest\";\n    volumes = [ \"/services/lxdware/lxdware:/var/lxdware\" ];\n    dependsOn = [ \"create-network-lxdware\" ];\n    extraOptions = [\n      # networks\n      \"--network=lxdware\"\n      # labels\n      \"--label\"\n      \"traefik.enable=true\"\n      \"--label\"\n      \"traefik.docker.network=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.rule=Host(`lxd.local.bspwr.com`)\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.entrypoints=websecure\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls=true\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls.certresolver=letsencrypt\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.service=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.middlewares=local-allowlist@file, default@file\"\n      \"--label\"\n      \"traefik.http.services.lxdware.loadbalancer.server.port=80\"\n    ];\n  };\n}\n\nLxdware is a web based frontend for LXD, a type of hyperconverged infrastructure. LXD is a daemon, or background process, for managing containers (ran via LXC), virtual machines, and to an extent, virtualized storage. It appealed to me, when I was searching for a hyperconverged infrastructure solution for my home lab.\n\nIt’s feature rich, and mature. However, I dislike the particular implementation used in the above configuration. They ran lxdware, in a docker container. This works, and probably works well, but this isn’t a very nixos way of doing things. Nix offers reproducibility, so docker isn’t needed, and is generally frowned upon because it brings some disadvantages. I wanted to configure lxdware using nix myself.\nI later looked at openstack, because I wanted to see if there was an ideal way to configure this with nixos. Nix makes it so easy to configure so many other services, just a few lines of nix code in the configuration.nix file to set up webservers, or other services.\nIn fact, people even discussed this in a thread posted on the Nixos discourse forums\n\nIs there anyone actively working on being able to run an OpenStack cloud using NixOS? Shouldn’t “we” be able to do what the Kayobe project does but without the Ansible stuff? https://docs.openstack.org/kayobe/latest/configuration/reference/kolla-ansible.html 9\n\nOne person replied, saying that they had done a bit, but work had “rotten since”. And indeed, when I searched around github, I found an old project, in a github repo titled nixstack\nThis person, or group of people, had packaged openstack to be easy to enable. But as I read through the code, I realized that the app is from the era of python2, so this code is really old."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#pip2nix",
    "href": "projects/openstack-on-nixos/index.html#pip2nix",
    "title": "Packaging Openstack on Nixos",
    "section": "Pip2nix",
    "text": "Pip2nix\nI install and setup pip2nix and attempt to use it. It first complains about python36 being too old, but after I switch to the python39 version of pip2nix:\n[nix-shell:~/vscode/keystone]$ pip2nix generate .\nProcessing /home/moonpie/vscode/keystone\n... # lots of extraneous output\npip2nix generate seems to take the same inputs as pip, so to install a python package from the current working directory, you could usually do pip install ., but here I do a pip2nix generate . instead. This generates a python-packages.nix file with all the python packages, including openstack keystone.\nI played around with trying to install openstack keystone in a nix-shell environment.\n{\n        pkgs ? import &lt;nixpkgs&gt; {}\n} : \nwith import ./python-packages.nix {inherit pkgs;};\nlet\n        keystone-nix = callPackage keystone;\nin\npkgs.mkShell {\n        packages = [ keystone-nix ];\n}\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       error: function 'anonymous lambda' called without required argument 'fetchurl'\n\n       at /home/moonpie/vscode/keystone/python-packages.nix:4:1:\n\n            3|\n            4| { pkgs, fetchurl, fetchgit, fetchhg }:\n             | ^\n            5|\n\nI remember getting a similar error when I was trying to package quarto, and I was told to use callPackage. I don’t think I am using callPackage correctly here.\nI tried tinkering with some code that I got from searching github\n{\n    pkgs ? import &lt;nixpkgs&gt; {}\n} : \nwith pkgs;\nlet\n  python = python3;\n  pythonPackages = python.pkgs;\n\n  # generated with `pip2nix generate -r requirements.txt`\n  generatePipPackages = import ./python-packages.nix {\n    inherit pkgs;\n    inherit (pkgs) fetchurl fetchgit fetchhg;\n  };\n\n  pipPackages =  generatePipPackages pipPackages pythonPackages;\nin\npkgs.mkShell {\n    packages = with pipPackages; [ keystone ];\n}\nBut this errors:\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: attribute 'setuptools' missing\n\n       at /home/moonpie/vscode/keystone/python-packages.nix:106:7:\n\n          105|     propagatedBuildInputs = [\n          106|       self.\"setuptools\"\n             |       ^\n          107|       self.\"six\"\n\nI don’t really know why this errors. o this is good advice. I just graduated, and although I have an internship I will need to do this anix-shell error: … in the left operand of the update (//) operator\n     at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/lib/fixed-points.nix:69:64:\n\n       68|   #\n       69|   extends = f: rattrs: self: let super = rattrs self; in super // f self super;\n         |                                                                ^\n       70|\n\n   … in the left operand of the update (//) operator\n\n     at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/lib/fixed-points.nix:69:64:\n\n       68|   #\n       69|   extends = f: rattrs: self: let super = rattrs self; in super // f self super;\n         |                                                                ^\n       70|\n\n   (stack trace truncated; use '--show-trace' to show the full trace)\n\n   error: function 'anonymous lambda' called with unexpected argument 'self'\n\n   at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/development/interpreters/python/passthrufun.nix:37:6:\n\n       36|     # - applies overrides from `packageOverrides` and `pythonPackagesOverlays`.\n       37|     ({ pkgs, stdenv, python, overrides }: let\n         |      ^\n       38|       pythonPackagesFun = import ./python-packages-base.nix {\n\n&lt;/details&gt;\n\nI literally changed nothing. Except for adding setuptools to the start of the requirements.txt, but I still got a differing error than the one I got yesterday. \n\n\nI recloned the keystone repo (reverting back to the default requirements.txt file), and regenerated the nix files using pip2nix, however I still get the same error. Weird.\n\n2 days later, I still get the same error. So I decided to try another tool. Although I am aware that I could, in theory package every python package manually, I have opted not to do that when programattic solutions exist. Manual packaging is a last resort.\n\n## Mach-nix\n\n### Flying Blind\n\nSo I tried another tool, [mach-nix](https://github.com/DavHau/mach-nix), which is unmaintained, but worked when I tried it a little bit ago.\n\nFor some reason, trying to get the non-flake version of the package didn't work:\n\n&lt;details&gt;&lt;summary&gt;Show error&lt;/summary&gt;\n\n```{.default .code-overflow-wrap}\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell -p '(callPackage (fetchTarball https://github.com/DavHau/mach-nix/tarball/3.5.0) {}).mach-nix'\nthis derivation will be built:\n  /nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv\nbuilding '/nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv'...\nSourcing python-remove-tests-dir-hook\nSourcing python-catch-conflicts-hook.sh\n...\n...\n/nix/store/74l4x6m97bvry7ccxv51h952ayvg2j46-stdenv-linux/setup: line 1596: pop_var_context: head of shell_variables not a function context\nerror: builder for '/nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv' failed with exit code 1;\n       last 10 log lines:\n       &gt;   File \"/nix/store/pv3psrncam37dc8n1v2q80jfvnw601ln-python3.9-setuptools-67.4.0/lib/python3.9/site-packages/setuptools/_distutils/cmd.py\", line 305, in get_finalized_command\n       &gt;     cmd_obj.ensure_fio this is good advice. I just graduated, and although I have an internship I will need to do this ations()\n       &gt;   File \"/nix/store/pv3psrncam37dc8n1v2q80jfvnw601ln-python3.9-setuptools-67.4.0/lib/python3.9/site-packages/setuptools/command/egg_info.py\", line 220, in finalize_options\n       &gt;     parsed_version = packaging.version.Version(self.egg_version)\n       &gt;   File \"/nix/store/pv3psrncam37dc8n1v2q80jfvnw601ln-python3.9-setuptools-67.4.0/lib/python3.9/site-packages/setuptools/_vendor/packaging/version.py\", line 197, in __init__\n       &gt;     raise InvalidVersion(f\"Invalid version: '{version}'\")\n       &gt; setuptools.extern.packaging.version.InvalidVersion: Invalid version: 'master'\n       &gt; /nix/store/74l4x6m97bvry7ccxv51h952ayvg2j46-stdenv-linux/setup: line 1596: pop_var_context: head of shell_variables not a function context\n       For full logs, run 'nix-store -l /nix/store/71qql4471m8m5js7l1rwnd5m1aizd8m8-mach-nix-master.drv'.\n\nSo I had to use the flake version of mach-nix\n\nmoonpie@localhost:~/vscode/keystone&gt; nix shell github:DavHau/mach-nix\nmoonpie@localhost:~/vscode/keystone&gt; mach-nix env ./env -r requirements.txt\npath is '/nix/store/ahaz90hy6lins0a56mdivrd2fjj3rcb5-554d2d8aa25b6e583575459c297ec23750adb6cb'\n\nInitialized python environment in:                ./env\nTo change python requirements, modify the file:   ./env/requirements.txt\n\nTo activate the environment, execute:             nix-shell ./env\nHowever, when I actually tried to activate the environment, it simply stalled forever, until I forcefully exited the program using Control + C. After which, it would spit out an error:\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell ./env\n^Cerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/5n402azp0s9vza4rziv4z5y88v2cv1mq-nixpkgs/pkgs/stdenv/generic/make-derivation.nix:205:7\n\n       … while evaluating attribute 'buildInputs' of derivation 'nix-shell'\n\n         at /nix/store/5n402azp0s9vza4rziv4z5y88v2cv1mq-nixpkgs/pkgs/stdenv/generic/make-derivation.nix:247:7:\n\n          246|       depsHostHost                = lib.elemAt (lib.elemAt dependencies 1) 0;\n          247|       buildInputs                 = lib.elemAt (lib.elemAt dependencies 1) 1;\n             |       ^\n          248|       depsTargetTarget            = lib.elemAt (lib.elemAt dependencies 2) 0;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: download of 'https://github.com/DavHau/pypi-deps-db/tarball/e00b22ead9d3534ba1c448e1af3076af6b234acf' was interrupted\nI could report an issue, but I decided to simply try the new project that the developer is working on instead, dream2nix.\n\nA kind user helped me.\nA user on matrix responded to me asking for help with dream2nix\nThey said:\n\nHave you tried to use mach-nix pinned to rev = “65266b5cc867fec2cb6a25409dd7cd12251f6107”; and versionPy ? “39” to use the req.txt file from keystone 23.0.0 (works for me)\n\nSo I created a small shell.nix to that simply calls my derivation:\n\n\nshell.nix\n\n{\n    pkgs ? import &lt;nixpkgs&gt; {},\n    \n} :\nlet \n    okeystone = pkgs.callPackage ./keystone.nix {}; # okeystone becuase packages with the name keystone already exists.\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ okeystone ];\n    }\n\nAnd my derivation\n\n\nShow derivation\n\n\n\nkeystone.nix\n\n{\n    nixpkgs ? import &lt;nipxkgs&gt;,\n    #fetchFromGithub,\n    fetchgit,\n    python3\n} : \n\nlet\n    mach-nix = import (fetchgit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {};\nin\n\nmach-nix.buildPythonApplication {\n    pname =  \"keystone\";\n    version = \"placeholder\";\n    src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n    };\n}\n\n\nAnd I get the same error as above, after I forcefully quit the stalled program.\nI realized a little bit later that I forgot to declare versionPy ? \"39\".\nLater, I finally made some progress.\n\n\nShow\n\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    fetchgit,\n    versionPy ? \"39\"\n} : \n\nlet\n    #python = pkgs.\"python${versionPy}Full\";\n    #pythonPackages = pkgs.\"python${versionPy}Packages\";\n    mach-nix = import (fetchGit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    #sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {\n    python = \"python39\";\n    pythonPackages = \"python39Packages\";\n\n    pypiDataRev = \"e9571cac25d2f509e44fec9dc94a3703a40126ff\";\n    pypiDataSha256 = \"1rbb0yx5kjn0j6lk0ml163227swji8abvq0krynqyi759ixirxd5\";\n    };\nin\n\nmach-nix.buildPythonApplication {\n    pname =  \"keystone\";\n    version = \"placeholder\";\n\n    src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };\n    \n}\n\n\nThis doesn’t fail instantly, but rather does some stuff before giving me a differing, but easy to understand error.\n\n\nShow error\n\nmoonpie@localhost:~/vscode/moonpiedumplings.github.io/projects/openstack-on-nixos/mach-nix&gt; nix-shell\nbuilding '/nix/store/lwpd31yfdr3jcpi8f1ik21dgyhp063iw-python3-3.9.9-env.drv'...\ncreated 223 symlinks in user environment\nbuilding '/nix/store/iag09dlfh5z4l4f5zq87f38wpwqhdg5h-python3-3.9.9-env-patched.drv'...\nFile /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/distutils/core.py is read-only; trying to patch anyway\npatching file /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/distutils/core.py\n/nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/site-packages/setuptools/__init__.py\nFile /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/site-packages/setuptools/__init__.py is read-only; trying to patch anyway\npatching file /nix/store/jzlvmms4bxkqscd2akcfhwwdj2668sjp-python3-3.9.9-env-patched/lib/python3.9/site-packages/setuptools/__init__.py\nHunk #1 succeeded at 239 (offset -12 lines).\nbuilding '/nix/store/zj70xbgg1gm8kils98insy3xzjzmphqx-package-requirements.drv'...\nunpacking sources\nunpacking source archive /nix/store/22jqj6c8na5k547qjrhlpjpagzq3wvhc-keystone-040e6d0\nsource root is keystone-040e6d0\ninstalling\nextracting dependencies\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: Automatic requirements extraction failed for keystone:placeholder.\n       Please manually specify 'requirements'\n\nClear, and easy to understand. For whatever reason, mach-nix is unable to get requirements.txt from keystone, and it wants me to manually specify them.\nI will see if there is a way to get around this.\nSo I found a way to get around this:\nmach-nix.buildPythonApplication rec {\n    pname =  \"keystone\";\n    version = \"placeholder\";\n\n    src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };\n    requirements =  builtins.readFile \"${src}/requirements.txt\";\n    \n}\nBut of course, this still errors.\nAfter a lot of compiling and building, I get this:\n\n\nShow error\n\nERROR:root:Error parsing\nTraceback (most recent call last):\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/core.py\", line 111, in pbr\n    attrs = util.cfg_to_args(path, dist.script_args)\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/util.py\", line 272, in cfg_to_args\n    pbr.hooks.setup_hook(config)\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/__init__.py\", line 25, in setup_hook\n    metadata_config.run()\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/base.py\", line 27, in run\n    self.hook()\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/metadata.py\", line 25, in hook\n    self.config['version'] = packaging.get_version(\n  File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/packaging.py\", line 874, in get_version\n    raise Exception(\"Versioning for this project requires either an sdist\"\nException: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\nerror in setup command: Error parsing /build/keystone-040e6d0/setup.cfg: Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\nerror: builder for '/nix/store/gyj2647z1nk1g0dq5wh8jbf1lhv91b9x-python3.9-keystone-placeholder.drv' failed with exit code 1;\n       last 10 log lines:\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/__init__.py\", line 25, in setup_hook\n       &gt;     metadata_config.run()\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/base.py\", line 27, in run\n       &gt;     self.hook()\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/hooks/metadata.py\", line 25, in hook\n       &gt;     self.config['version'] = packaging.get_version(\n       &gt;   File \"/nix/store/bksvs9irxl40bnm126sdnhxam0rzidby-python3.9-pbr-5.11.0/lib/python3.9/site-packages/pbr/packaging.py\", line 874, in get_version\n       &gt;     raise Exception(\"Versioning for this project requires either an sdist\"\n       &gt; Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n       &gt; error in setup command: Error parsing /build/keystone-040e6d0/setup.cfg: Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n       For full logs, run 'nix-store -l /nix/store/gyj2647z1nk1g0dq5wh8jbf1lhv91b9x-python3.9-keystone-placeholder.drv'.\n\nAt least the error is explicit. I will look into this more later.\nI found something that worked. Looking at the old nixstack code, there is an attribute that can be set:\nPBR_VERSION = \"${version}\";\nand this works. My current keystone.nix:\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    fetchgit,\n    fetchPypi,\n    #fetchFromGitHub,\n    versionPy ? \"39\",\n} : \n\nlet\n    #python = pkgs.\"python${versionPy}Full\";\n    #pythonPackages = pkgs.\"python${versionPy}Packages\";\n    mach-nix = import (fetchgit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {\n    python = \"python39\";\n    pythonPackages = \"python39Packages\";\n\n    pypiDataRev = \"e9571cac25d2f509e44fec9dc94a3703a40126ff\";\n    pypiDataSha256 = \"1rbb0yx5kjn0j6lk0ml163227swji8abvq0krynqyi759ixirxd5\";\n    };\nin\n\nmach-nix.buildPythonPackage rec {\n    pname =  \"keystone\";\n    version = \"23.0.0\";\n    PBR_VERSION = \"${version}\";\n    src = builtins.fetchGit {\n        url = \"https://github.com/openstack/keystone/\";\n        ref = \"refs/tags/23.0.0\";\n        rev = \"c08d97672dcd40f8d927f91e59049053cfe3b5e4\";\n    };\n    requirements =  builtins.readFile \"${src}/requirements.txt\";\n    \n}\n\nBut I don’t really want to use mach-nix, as it is unmaintained. I will see if there is a way to set this up with dream2nix.\nFor those trying to help me, here is a live version, updated every git push, of the files I am working on, because I don’t usually update my blog as I work on things, only after.\n\n\nShow\n\n\n\nshell.nix\n\n{\n    pkgs ? import &lt;nixpkgs&gt; {},\n    \n} :\nlet \n    okeystone = pkgs.callPackage ./keystone.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ okeystone ];\n    }\n\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    fetchgit,\n    fetchPypi,\n    #fetchFromGitHub,\n    versionPy ? \"39\",\n} : \n\nlet\n    #python = pkgs.\"python${versionPy}Full\";\n    #pythonPackages = pkgs.\"python${versionPy}Packages\";\n    mach-nix = import (fetchgit {\n    url = \"https://github.com/DavHau/mach-nix\";\n    rev = \"65266b5cc867fec2cb6a25409dd7cd12251f6107\";\n    sha256 = \"sha256-1OBBlBzZ894or8eHZjyADOMnGH89pPUKYGVVS5rwW/0=\";\n    }) {\n    python = \"python39\";\n    pythonPackages = \"python39Packages\";\n\n    pypiDataRev = \"e9571cac25d2f509e44fec9dc94a3703a40126ff\";\n    pypiDataSha256 = \"1rbb0yx5kjn0j6lk0ml163227swji8abvq0krynqyi759ixirxd5\";\n    };\nin\n\nmach-nix.buildPythonPackage rec {\n    pname =  \"keystone\";\n    version = \"23.0.0\";\n    PBR_VERSION = \"${version}\";\n\n    /*src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };*/\n    /*src = fetchTarball {\n        url = \"https://github.com/openstack/keystone/archive/eff960e124e2f28922067800547e23f1931d3c4a.tar.gz\";\n        sha256 = \"\";\n    };*/\n    src = builtins.fetchGit {\n        url = \"https://github.com/openstack/keystone/\";\n        ref = \"refs/tags/23.0.0\";\n        rev = \"c08d97672dcd40f8d927f91e59049053cfe3b5e4\";\n        #sha256 = \"sha256-JYP29APY27BpX9GSyayW/y7rskdn8zW5mVsjdBXjCus=\";\n    };\n    /*src = fetchPypi {\n        inherit pname version;\n        sha256 = \"sha256-t0ravo9+H2nYcoGkvoxn5YxHOTf68vSon+VTJFn6INY=\";\n    };*/\n   /*src = fetchFromGitHub {\n        owner = \"openstack\";\n        repo = \"keystone\";\n        tag = \"23.0.0\";\n   };*/\n   /*src = fetchTarball {\n        url = \"https://github.com/openstack/keystone/archive/refs/tags/23.0.0.tar.gz\";\n        sha256 = \"\";\n    };*/\n    #src = /home/moonpie/vscode/keystone; # currently in correct tag\n\n    requirements =  builtins.readFile \"${src}/requirements.txt\";\n    \n}"
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#dream2nix",
    "href": "projects/openstack-on-nixos/index.html#dream2nix",
    "title": "Packaging Openstack on Nixos",
    "section": "Dream2nix",
    "text": "Dream2nix\nI decided to follow the their python project instructions.\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix run .#resolveImpure\nwarning: Git tree '/home/moonpie/vscode/keystone' is dirty\nResolving:: Name: main; Subsystem: python; relPath: \n/tmp/tmp.rB9TAH7neG ~/vscode/keystone\nreading setup requirements from pyproject.toml\nError: [Errno 2] No such file or directory: './source/pyproject.toml'\ninstall setup dependencies from extraSetupDeps\n\n[notice] A new release of pip available: 22.2.2 -&gt; 23.1.2\n[notice] To update, run: /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/bin/python3.10 -m pip install --upgrade pip\ndownload setup dependencies from pyproject.toml\n\n[notice] A new release of pip available: 22.2.2 -&gt; 23.1.2\n[notice] To update, run: /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/bin/python3.10 -m pip install --upgrade pip\ndownload files according to requirements\nrealpath: ./install/lib/python3.10/site-packages: No such file or directory\nProcessing ./source\n  Preparing metadata (setup.py) ... error\n  error: subprocess-exited-with-error\n  \n  × python setup.py egg_info did not run successfully.\n  │ exit code: 1\n  ╰─&gt; [19 lines of output]\n      /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/lib/python3.10/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.\n        warnings.warn(\n      WARNING: The wheel package is not available.\n      Error parsing\n      Traceback (most recent call last):\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/core.py\", line 105, in pbr\n          attrs = util.cfg_to_args(path, dist.script_args)\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/util.py\", line 272, in cfg_to_args\n          pbr.hooks.setup_hook(config)\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/hooks/__init__.py\", line 25, in setup_hook\n          metadata_config.run()\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/hooks/base.py\", line 27, in run\n          self.hook()\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/hooks/metadata.py\", line 25, in hook\n          self.config['version'] = packaging.get_version(\n        File \"/tmp/tmp.rB9TAH7neG/source/.eggs/pbr-5.11.1-py3.10.egg/pbr/packaging.py\", line 874, in get_version\n          raise Exception(\"Versioning for this project requires either an sdist\"\n      Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n      error in setup command: Error parsing /tmp/tmp.rB9TAH7neG/source/setup.cfg: Exception: Versioning for this project requires either an sdist tarball, or access to an upstream git repository. It's also possible that there is a mismatch between the package name in setup.cfg and the argument given to pbr.version.VersionInfo. Project name keystone was given, but was not able to be found.\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\nerror: metadata-generation-failed\n\n× Encountered error while generating package metadata.\n╰─&gt; See above for output.\n\nnote: This is an issue with the package mentioned above, not pip.\nhint: See above for details.\n\n[notice] A new release of pip available: 22.2.2 -&gt; 23.1.2\n[notice] To update, run: /nix/store/w3lza2zk3i6xi1x5rh5zbwc9zlfsymc0-python3-3.10.7-env/bin/python3.10 -m pip install --upgrade pip\n\nAnd an error. Interestingly, this is the exact same error that I got with mach-nix. This means that the error proabbly has the same solution, and if I find it, I’d like to stick to dream2nix is it is actively maintained, unlike mach-nix.\nSo I started doing some research. The first thing was something another user on matrix sent to me, a stackoverflow post\nThis post had several interesting ramifications. I was recommended to attempt to set the version of keystone using an environment variable, but I noticed that PBR, the tool used to build/install keystone, should be able to get teh current version from git tags, without that hassle. So I did more research as to why this wasn’t working.\nI found a keystone specific bug, on launchpad, a bug reporting site. Someone not using nix encountered my exact error. Linked below was a fix, created and merged into other openstack repos, but not the keystone one. Apparently, git was changed in such a way that it cannot do any operations on a repo not owned by the current user, in order to fix a security vulnerability. This may not bug normal users, but with nix, the git repo is downloaded into the nix store, where is is owned by the root user. Then, the nixbld user’s operate on the store paths and files. This means that nix is probably encountering this specific git behavior. I am guessing my next step is do do something that makes it okay for git to operate on git repo’s not owned by the current user.\nYou should be able to adjust git configs to add “safe” directories, that is, declare directories that are ok to operate on not by the current user.\nOne stackoverflow answer recommended this:\ngit config --global --add safe.directory *\nBut this edits the git configuration files for that user. There might be a way to edit just for that repository, since git stores some configs in the repo itself, but I doubt that, as that creates a very easy workaround for the fix designed to squash the security issue.\nThere is the git config --system option, but if I ran that inside of a nix chroot, where would it store it?\nMaybe I could use a modified version of git with nix?\nAnother thing to consider is, for the dream2nix approach, does it store a copy of the repository in the nix store, or does it work directly on the repository I run it in?\nPreviously, I had run this all on my newer laptop, which has a multi user install, meaning the /nix directory is owned by root, and the nixbld* user’s do the operations. However, I decided to try it on my other laptop, which has a single user nix install. This means that the nix store is owned by my user moonpie, and the operations are also done by my user, moonpie. However, I encountered the same error.\nOkay, I don’t need to mess with git, I just need to set the PBR_VERSION environment variable. How can I set environment variables with dream2nix?\nOkay, so on the matrix thread where I am being helped, I received this as a reply when I asked how to set up environment variables in dream2nix:\n\nBasically, dream2nix doesn’t have this feature yet. I don’t know where I am going to go from here. I think I should simply wait until dream2nix is more mature, and put this project on hold, although I might see if there is a way to hack it, although my understanding of the nix langauge is still lacking."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#poetry2nix",
    "href": "projects/openstack-on-nixos/index.html#poetry2nix",
    "title": "Packaging Openstack on Nixos",
    "section": "Poetry2nix",
    "text": "Poetry2nix\nPoetry is a python dependency manager similar to nix. poetry2nix uses poetry’s specifications to do things like download package, package applications, or create shell environments using nix.\nFirst I needed to convert the requirements.txt to poetry.\nI used the dephell tool to do so. Specifically, dephell deps convert\ndephell deps convert --from pip --to poetry\nAfter that, I used poetry2nix to create a shell environment, to test if the dependencies were downloaded.\n\n\nshell.nix\n\n{ \npkgs ? import &lt;nixpkgs&gt; {},\npoetry2nix ? import &lt;poetry2nix&gt; {}\n} :\n\npoetry2nix.mkPoetryEnv {\n    projectDir = ./.;\n}\n\nAfter that, using pip list to list the python packages that pip can see confirms that this works to download all dependencies.\nBut I don’t really like doing this, because dephell is depreceated. So I’m going to look into some other converion tools, tools to convert the pip file format/requirements.txt to the poetry file format/pyproject.toml.\n\n\n\nProject\nNotes\n\n\n\n\ndephell\nWorks, but is archive/unmaintained, and complains about a piece being deprecated when ran…\n\n\nstanza\nLast commit was a year ago\n\n\npoeareq\nmaintained by one person, last commit a year ago. Present in pypi\n\n\ntigerhawk’s gist\njust a github gist, a user complimented in a comment one month ago. But the script was last updated in 2019\n\n\nvarious shell one liners\nprovided by stackoverflow. They work, but are hacky, and apparently have edge cases they don’t cover.\n\n\npip-poetry-migration\nuntested/unresearched\n\n\npoetry-import\nuntested/unresearched. In pypi.\n\n\npoetrify\nlast updated 2 years ago. Says that requirements.txt is still in trial in description.\n\n\npoetry import plugin\npoetry plugin, so it may be easier to install/automate. Last updated 9 months ago.\n\n\nmicropipenv\nI don’t know if it can actually do what I want. last updated 2 months ago. Maintained by Red Hat.\n\n\n\nHere is a live version of what I am working on for poetry, updated every git push.\n\n\nShow\n\n\n\nshell.nix\n\n{\n    pkgs ? import &lt;nixpkgs&gt; {},\n    \n} :\nlet \n    okeystone = pkgs.callPackage ./keystone.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ okeystone ];\n    }\n\n\n\nkeystone.nix\n\n{\n    pkgs ? import &lt;nipxkgs&gt; {}, \n    #pkgs ? (import fetchtarball {\"https://github.com/nixos/nixpkgs/archive/f10cdcf31dd2a436edbf7f0ad82c44b911804bc8.tar.gz\"}),\n    #poetry2nix ? (import fetchtarball {\"https://github.com/nix-community/poetry2nix/archive/215afa14f7077ca0610882d6d18ac3454f48fa65.tar.gz\"}),\n    fetchgit,\n} : \n\nlet\n    /*poetryConverter = src: pkgs.runCommand \"${pkg.name}-nixgl-wrapper\" {} ''\n    ${pkgs.poetry}/bin/poetry self add poetry-plugin-import\n    mkdir $out\n    ln -s ${pkg}/* $out\n    rm $out/bin\n    mkdir $out/bin\n  '';*/\n    poetry2nix = import (builtins.fetchTarball {\n    url = \"https://github.com/nix-community/poetry2nix/archive/215afa14f7077ca0610882d6d18ac3454f48fa65.tar.gz\";\n    sha256 = \"0k0blf48ln6bcj7c76cjjcdx41l1ygscpczi2k2d2vqv14a5bzan\";\n  }) {};\nin\npoetry2nix.mkPoetryApplication {\n    /*src = fetchgit {\n        url = \"https://github.com/openstack/keystone/\";\n        rev = \"040e6d09b1e7e6817c81209c2b089d318715bef6\";\n        sha256 = \"sha256-qQgGh0WwEDSYQC1PDnSDp3RUiWoFjV5SCjw0SiUlJtk=\";\n    };\n    */\n    src = \"$HOME/vscode/keystone\"; # Not gonna post anything online yet, espcially in such an alpha state. \n}"
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#packaging-them-myself",
    "href": "projects/openstack-on-nixos/index.html#packaging-them-myself",
    "title": "Packaging Openstack on Nixos",
    "section": "Packaging them myself",
    "text": "Packaging them myself\nOkay, I did find something else interesting. When I was looking at one of the openstack dependencies, packaged in nixpkgs, I found something interesting in the meta section:\nmeta = with lib; {\n    description = \"Oslo test framework\";\n    homepage = \"https://github.com/openstack/oslotest\";\n    license = licenses.asl20;\n    maintainers = teams.openstack.members;\n  };\nFor maintainers, nix has “teams,”” or groups of people dedicated to packaging a group of packages. And apparently, they seem to have an openstack team:\nSure enough, in the nixpkgs list of teams, they can be found:\nopenstack = {\n    members = [\n      emilytrau\n      SuperSandro2000\n    ];\n    scope = \"Maintain the ecosystem around OpenStack\";\n    shortName = \"OpenStack\";\n  };\nThese people maintain the ecosystem around openstack, but not openstack itself. Perhaps all openstack dependencies are already packaged?\nI would probably have to do overrides on some of the depedencies to ensure that the versions are correct, since sometimes newer packages have breaking changes, but if stuff is already packaged, that would be great.\nHere is the list of the packages from the requirements.txt. I will link each package to it’s equivalent in nixpkgs.\n\n\nShow list\n\npbr!=2.1.0,&gt;=2.0.0 # Apache-2.0 nixpkgs\nWebOb&gt;=1.7.1 # MIT nixpkgs\nFlask!=0.11,&gt;=1.0.2 # BSD nixpkgs\nFlask-RESTful&gt;=0.3.5 # BSD nixpkgs\ncryptography&gt;=2.7 # BSD/Apache-2.0 nixpkgs\nSQLAlchemy&gt;=1.4.0 # MIT nixpkgs\nsqlalchemy-migrate&gt;=0.13.0 # Apache-2.0 nixpkgs\nstevedore&gt;=1.20.0 # Apache-2.0 nixpkgs\npasslib&gt;=1.7.0 # BSD nixpkgs\npython-keystoneclient&gt;=3.8.0 # Apache-2.0 nixpkgs\nkeystonemiddleware&gt;=7.0.0 # Apache-2.0 NOT FOUND\nbcrypt&gt;=3.1.3 # Apache-2.0 nixpkgs\nscrypt&gt;=0.8.0 # BSD nixpkgs\noslo.cache&gt;=1.26.0 # Apache-2.0 NOT FOUND\noslo.config&gt;=6.8.0 # Apache-2.0 nixpkgs\noslo.context&gt;=2.22.0 # Apache-2.0 nixpkgs\noslo.messaging&gt;=5.29.0 # Apache-2.0 NOT FOUND\noslo.db&gt;=6.0.0 # Apache-2.0 nixpkgs\noslo.i18n&gt;=3.15.3 # Apache-2.0 nixpkgs\noslo.log&gt;=3.44.0 # Apache-2.0 nixpkgs\noslo.middleware&gt;=3.31.0 # Apache-2.0 NOT FOUND\noslo.policy&gt;=3.10.0 # Apache-2.0 NOT FOUND\noslo.serialization!=2.19.1,&gt;=2.18.0 # Apache-2.0 nixpkgs\noslo.upgradecheck&gt;=1.3.0 # Apache-2.0 NOT FOUND\noslo.utils&gt;=3.33.0 # Apache-2.0 nixpkgs\noauthlib&gt;=0.6.2 # BSD nixpkgs\npysaml2&gt;=5.0.0 nixpkgs\nPyJWT&gt;=1.6.1 # MIT nixpkgs\ndogpile.cache&gt;=1.0.2 # BSD nixpkgs\njsonschema&gt;=3.2.0 # MIT nixpkgs\npycadf!=2.0.0,&gt;=1.1.0 # Apache-2.0 NOT FOUND\nmsgpack&gt;=0.5.0 # Apache-2.0 nixpkgs\nosprofiler&gt;=1.4.0 # Apache-2.0 NOT FOUND\npytz&gt;=2013.6 # MIT nipxkgs\n\nHowever, I needed a way to check the compatibility of a python package, to a version of python. Thankfully, I found an easyish way to do so.\npip install --python-version 311 --no-deps --target test-pkg packagename==missing\nBecause openstack keystone is actually packaged in pypi repos, downloaded by pip, this works:\nmoonpie@localhost:~/vscode/keystone&gt; pip install --python-version 311 --no-deps --target test-pkg keystone==missing\nERROR: Could not find a version that satisfies the requirement keystone==missing (from versions: 12.0.2, 12.0.3, 13.0.2, 13.0.3, 13.0.4, 14.0.0, 14.0.1, 14.1.0, 14.2.0, 15.0.0.0rc1, 15.0.0.0rc2, 15.0.0, 15.0.1, 16.0.0.0rc1, 16.0.0.0rc2, 16.0.0, 16.0.1, 16.0.2, 17.0.0.0rc1, 17.0.0.0rc2, 17.0.0, 17.0.1, 18.0.0.0rc1, 18.0.0, 18.1.0, 19.0.0.0rc1, 19.0.0.0rc2, 19.0.0, 19.0.1, 20.0.0.0rc1, 20.0.0, 20.0.1, 21.0.0.0rc1, 21.0.0, 22.0.0.0rc1, 22.0.0, 23.0.0.0rc1, 23.0.0)\nERROR: No matching distribution found for keystone==missing\nSo openstack supports python 3.11, which is good. Nix doesn’t have many of the above libraries packaged for python 3.8 and below."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 1, 2023\n\n\nBuilding my own Server\n\n\n\n\n\n\n\nJul 14, 2023\n\n\nAutomating my server config, first nix, then ansible\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nPackaging Openstack on Nixos\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nPackaging quarto using nix\n\n\n\n\n\n\n\nMay 2, 2023\n\n\nCompiling KasmVNC on NixOS\n\n\n\n\n\n\n\nJan 26, 2023\n\n\nKasmweb setup\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/build-server/index.html",
    "href": "projects/build-server/index.html",
    "title": "Building my own Server",
    "section": "",
    "text": "I wanted a machine for experimenting with devops and deep learning. That means plenty of ram, cpu, and a modest gpu.\nI was very careful with my selection, and here is what I ended up with:\n\nDell Precision Tower Server 7910, with 2X intel xeon E5 2687 v4. Came with 32 GB of ram.\nMore memory, when added, I will get a total of 128 GB of ECC memory.\nNvidia rtx 4070 GPU, 12 GB vram.\n\n\n\n\nHere is the inside of my server:\n\nAnyway, I need to figure out where to put the GPU. The computer has several PCIe slots, and I want the fastest one.\nUp top:\n\nAnd below:\n\nI need to figure out what each of these pcie ports is. What do the numbers and color mean?\nTo catalog them:\n\nThree black PCIe3x16 slots (all 75W)\nOne blue PCIe3x16 75W + ext 225 w\nOne black PCIe2x4 25 W\nOne tan PCI slot\n\nI think it is safe to assume that the color is related to wattage, not PCIe protocol. Since the computer comes with the necessary power plug that the nvidia gpu wants, it is safe to assume that any of the PCIe3x16 are optimal, although I will try to place the gpu in the best spot for cooling.\nI found a forum post (wayback archive) where someone asked this exact question.\nAn unsourced answer replied that the blue pcie slot was the primary gpu slot, so since I only have one gpu, that is where I put it.\nI found the manual for my system online, but it doesn’t seem to label each pci port in images.\nI also did research into some youtube videos.\nGeneral review\nMemory upgrading process\nThe important thing to note about the memory upgrading process is that the memory shroud (cover) does not interfere with other processes.\n\n\n\nHowever, I am trying to add the gpu, so I can get monitor output, so I can run testing suites to ensure the computer is in proper working order.\nHowever, the gpu does not seem to have screw slots for holding it in place. Rather than that, the entire thing rests on a single screw, as shown below.\n\nBut the GPU, is inserted:\n\nAfter plugging the gpu power in:\n\nI can turn the machine on and get monitor output:\n\nNext step is to run the system diagnostics on the memory I currently have. Thankfully, I have an extra keyboard I can connect it to. It’s a small bluetooth one, closer to a remote than something meant for typing, but it will do nicely.\nAnyway, I also did more research, attempting to find more manuals and whatnot.\nLinked on this page I found a proper manual which gave visual instructions on how to do things like remove the memory shroud.\nToo bad it still doesn’t detail how to install the gpu. I just know I’m not doing it right, the servers I worked with at cirrascale had a screw that would actually hold the gpu in place. It doesn’t make sense to simply have the gpu be resting. I may simply give up, and find some nuts and attatch them to the back of the screws, locking the gpu into place.\nLater, I found out that it was very simple, and I’m frustrated I missed it. I simply had to reverse the direction of the screw, and screw it in from the other side, like so:\n\n\n\n\nNow that I had the GPU installed I could get video output and see what the BIOS is saying. I installed two hard drives, and got an error: “Alert Hard drive not found”.\nI decided to pause on the hard drives, and run the built in memory test. The memory, and cpu tests ran without error, however, I got an error about not being able to find the hard drives\n\nJust to make sure that the issue was a hardware issue, and not the BIOS merely complaining about not seeing a bootable device, I booted into a live USB and ran lsblk:\n\nI messed around with moving around the hard drive positions, unplugging and plugging in cables, no dice. This issue is definitely at the hardware level, however, and there are several causes:\n\nBad cable. I only have a single cable, an SAS connector on both sides.\nNo power to the hard drives. I will test to see if they light up later.\nMotherboard not working. Maybe the motherboard plug isn’t working, but I tried both of them…\n\nIt’s definitely not defective hard drives, as this happens with both of the hard drives I am testing with, both of which are new. However, they rely on the same cable and same power supply (they attatch to a seperate board which has a cable connecting that to the motherboard and another connecting it to the power supply)\nUpon doing some further research, I found a related youtube video. According to this video, the port I am trying to attatch this to is controlled by the RAID controller, so RAID must be enabled, although I can use RAID 0 for RAID without any of the special RAID features or complications.\nI enabled the RAID controller, and now the BIOS can see the hard drives:\n\nHowever, when I actually inspect these drives via a live usb, they don’t appear to be mountable properly.\n\nIt sees the hard drives as 0 byte, empty drives, which I can’t do anything.\nI need to get into the raid controller bios.\nI found a relevant manual which says that when booting, pressing Control + I to enter the raid configuration screen.\nI am wondering if there is an option to bypass raid entirely, as I have no need for any of it’s features, and striped will limit me to the size of the smallest disk.\nAlright, since I skimmed the manual, I missed something. There are actually three different ways to get to the raid controller interface, depending on what raid controller you have installed. I tested all methods, but the only one that worked was entering the boot options menu, and then going to device configuration.\nThen, I was met with a screen like this:\n\nI browsed around a little bit, and although it could see the physical drives, there was no option to create a volume or do anything with the disks. I think this is because I have two different disks, an nvme SSD, and a sata HDD.\nThis is frustrating. I (might) need to buy another disk, or buy parts to make the disks work via a SATA connection.\nI did more research and found a relevant support article (wayback machine) detailing how to use the raid controller and it seems I missed something. It’s seems that creating volume is not in the drives section, but rather in controller configuration."
  },
  {
    "objectID": "projects/build-server/index.html#selection",
    "href": "projects/build-server/index.html#selection",
    "title": "Building my own Server",
    "section": "",
    "text": "I wanted a machine for experimenting with devops and deep learning. That means plenty of ram, cpu, and a modest gpu.\nI was very careful with my selection, and here is what I ended up with:\n\nDell Precision Tower Server 7910, with 2X intel xeon E5 2687 v4. Came with 32 GB of ram.\nMore memory, when added, I will get a total of 128 GB of ECC memory.\nNvidia rtx 4070 GPU, 12 GB vram."
  },
  {
    "objectID": "projects/build-server/index.html#building",
    "href": "projects/build-server/index.html#building",
    "title": "Building my own Server",
    "section": "",
    "text": "Here is the inside of my server:\n\nAnyway, I need to figure out where to put the GPU. The computer has several PCIe slots, and I want the fastest one.\nUp top:\n\nAnd below:\n\nI need to figure out what each of these pcie ports is. What do the numbers and color mean?\nTo catalog them:\n\nThree black PCIe3x16 slots (all 75W)\nOne blue PCIe3x16 75W + ext 225 w\nOne black PCIe2x4 25 W\nOne tan PCI slot\n\nI think it is safe to assume that the color is related to wattage, not PCIe protocol. Since the computer comes with the necessary power plug that the nvidia gpu wants, it is safe to assume that any of the PCIe3x16 are optimal, although I will try to place the gpu in the best spot for cooling.\nI found a forum post (wayback archive) where someone asked this exact question.\nAn unsourced answer replied that the blue pcie slot was the primary gpu slot, so since I only have one gpu, that is where I put it.\nI found the manual for my system online, but it doesn’t seem to label each pci port in images.\nI also did research into some youtube videos.\nGeneral review\nMemory upgrading process\nThe important thing to note about the memory upgrading process is that the memory shroud (cover) does not interfere with other processes."
  },
  {
    "objectID": "projects/build-server/index.html#gpu",
    "href": "projects/build-server/index.html#gpu",
    "title": "Building my own Server",
    "section": "",
    "text": "However, I am trying to add the gpu, so I can get monitor output, so I can run testing suites to ensure the computer is in proper working order.\nHowever, the gpu does not seem to have screw slots for holding it in place. Rather than that, the entire thing rests on a single screw, as shown below.\n\nBut the GPU, is inserted:\n\nAfter plugging the gpu power in:\n\nI can turn the machine on and get monitor output:\n\nNext step is to run the system diagnostics on the memory I currently have. Thankfully, I have an extra keyboard I can connect it to. It’s a small bluetooth one, closer to a remote than something meant for typing, but it will do nicely.\nAnyway, I also did more research, attempting to find more manuals and whatnot.\nLinked on this page I found a proper manual which gave visual instructions on how to do things like remove the memory shroud.\nToo bad it still doesn’t detail how to install the gpu. I just know I’m not doing it right, the servers I worked with at cirrascale had a screw that would actually hold the gpu in place. It doesn’t make sense to simply have the gpu be resting. I may simply give up, and find some nuts and attatch them to the back of the screws, locking the gpu into place.\nLater, I found out that it was very simple, and I’m frustrated I missed it. I simply had to reverse the direction of the screw, and screw it in from the other side, like so:"
  },
  {
    "objectID": "projects/build-server/index.html#hard-drives",
    "href": "projects/build-server/index.html#hard-drives",
    "title": "Building my own Server",
    "section": "",
    "text": "Now that I had the GPU installed I could get video output and see what the BIOS is saying. I installed two hard drives, and got an error: “Alert Hard drive not found”.\nI decided to pause on the hard drives, and run the built in memory test. The memory, and cpu tests ran without error, however, I got an error about not being able to find the hard drives\n\nJust to make sure that the issue was a hardware issue, and not the BIOS merely complaining about not seeing a bootable device, I booted into a live USB and ran lsblk:\n\nI messed around with moving around the hard drive positions, unplugging and plugging in cables, no dice. This issue is definitely at the hardware level, however, and there are several causes:\n\nBad cable. I only have a single cable, an SAS connector on both sides.\nNo power to the hard drives. I will test to see if they light up later.\nMotherboard not working. Maybe the motherboard plug isn’t working, but I tried both of them…\n\nIt’s definitely not defective hard drives, as this happens with both of the hard drives I am testing with, both of which are new. However, they rely on the same cable and same power supply (they attatch to a seperate board which has a cable connecting that to the motherboard and another connecting it to the power supply)\nUpon doing some further research, I found a related youtube video. According to this video, the port I am trying to attatch this to is controlled by the RAID controller, so RAID must be enabled, although I can use RAID 0 for RAID without any of the special RAID features or complications.\nI enabled the RAID controller, and now the BIOS can see the hard drives:\n\nHowever, when I actually inspect these drives via a live usb, they don’t appear to be mountable properly.\n\nIt sees the hard drives as 0 byte, empty drives, which I can’t do anything.\nI need to get into the raid controller bios.\nI found a relevant manual which says that when booting, pressing Control + I to enter the raid configuration screen.\nI am wondering if there is an option to bypass raid entirely, as I have no need for any of it’s features, and striped will limit me to the size of the smallest disk.\nAlright, since I skimmed the manual, I missed something. There are actually three different ways to get to the raid controller interface, depending on what raid controller you have installed. I tested all methods, but the only one that worked was entering the boot options menu, and then going to device configuration.\nThen, I was met with a screen like this:\n\nI browsed around a little bit, and although it could see the physical drives, there was no option to create a volume or do anything with the disks. I think this is because I have two different disks, an nvme SSD, and a sata HDD.\nThis is frustrating. I (might) need to buy another disk, or buy parts to make the disks work via a SATA connection.\nI did more research and found a relevant support article (wayback machine) detailing how to use the raid controller and it seems I missed something. It’s seems that creating volume is not in the drives section, but rather in controller configuration."
  },
  {
    "objectID": "projects/build-server/index.html#operating-system",
    "href": "projects/build-server/index.html#operating-system",
    "title": "Building my own Server",
    "section": "Operating system",
    "text": "Operating system\nI was originally going to choose a RHEL compatible distro, but then RHEL made changes put the status of those in limbo.\nI am currently deciding between:\n\nUbuntu\nDebian\nRHEL (via a developer subscription)\nA RHEL rebuild\n\nRocky Linux\nAlma Linux\nOne of the academic distros, like scientific linux\n\nCentos Stream\n\nThe important thing is that it’s a stable release distro with automatic updates. I don’t want to have to do too much manual maintainence."
  },
  {
    "objectID": "projects/build-server/index.html#software-suite",
    "href": "projects/build-server/index.html#software-suite",
    "title": "Building my own Server",
    "section": "Software Suite",
    "text": "Software Suite\nI want an easy install, but I also want lots of features. Here are some things I have looked at:\n\nProxmox VE\nXen Orchestra\nOpenstack\nCanonicals LXD UI\nOvirt\nHarvester\nOpenVZ"
  },
  {
    "objectID": "projects/build-server/index.html#openstack",
    "href": "projects/build-server/index.html#openstack",
    "title": "Building my own Server",
    "section": "Openstack",
    "text": "Openstack\nCurrently, openstack appeals to me a lot. Although I originally wanted to do a bare metal install, I now realize that that is too time consuming and not realistic, so I am most likely going to use one of the automated methods of installation.\nKolla ansible\nThey have an easy deployment guide for an all in one node, perfect for my single server.\nI will definitely not use every service, but I do want to use openstack because of the sheer number of services it offers. Openstack offers every single feature of other virtualization platforms, at the cost of complexity. Here are the features that made me decide I needed that complexity.\n\nSkyline/Horizon\nOpenstack has a previous generation web ui, horizon, and a newer generation web ui, skyline. These web ui’s offer all the features of other web based virtualization platforms like proxmox, but they also let you configure the other things of openstack.\n\nAnd they have some special features, like giving you a visual layout of network topology.\n\n\n\nMulti tenancy.\nThe most important feature of openstack, in my opinion, is it’s multi tenant architechture. Unliek proxmox, which is designed for a single organization, openstack is designed in such a way that you can create extra users, which get their own allocation of resources.\nSo when I go to college, if anyone wants a VPS to play around in, I can just allocate them a few resources, and then they get their own web ui for playing with servers and networking.\nMany public cloud services are actually using openstack in the background for it’s public tenant architecture. Openstack’s dashboards can be rebranded to be your own company:\n\n\n\nBare metal nodes\nOpenstack saves a lot of trouble by immensely abstracting almost all the manual work that goes into putting together a modern cloud.\nIt takes it a step further, with it’s ability to treat physical, bare metal machines, as virtual machines, even automating the provisioning the same way you can do so for a virtual machine.\nThe docs make it sound complex, but it really isn’t all that. By leveraging the nova component of openstack, which abstracts the backend drivers of virtual machines (qemu-kvm, xen, or even hyper-v) can be used as backend drivers for nova.\nHowever, when combined with ironic openstack’s service to configure bare metal nodes, nova can also use bare metal as a driver for compute nodes. This integrates further with services like magnum…\n\n\nMagnum\nMagnum is openstack’s kubernetes-as-as-service. It provisions nodes using nova, with kubernetes clusters.\nNow here is where I get greedy. Do I need kubernetes? No. Will kubernetes even be useful on a single server setup? No. Do I want kubernetes? Yes.\nHere is a video demonstration, where someone uses the web ui to create a cluster using magnum.\n\nIn addition to that, because openstack magnum uses openstack heat, which provisions nodes from templates, it can be used to do things like auto install nvidia drivers and container runtime.\n\nThis video is a bit outdated, so heat and magnum are much more mature since then, and have only gained more features.\n\n\nAn api, and many automation features\nOpenstack is much more automatable than solutions like proxmox, becuase it is designed to be a cloud. Because of this, software like terraform, or perhaps custom software written to suit the needs of the users can interact with the openstack api\n\n\nOpenstack Zun\nOpenstack zun is the container service for openstack. It doesn’t run them in virtual machines, but instead directly on metal. It’s likely that when I want to run containerized services for actual usage, this is what I will be using instead of kubernetes since I will be using a single server, and thus won’t be able to get the benefits of kubernetes. The benefit I see form using containers is that because I have an nvidia desktop gpu, I won’t be able to use vgpu, a feature that lets you divide the gpu between virtual machines. However, containers have no such limitation."
  },
  {
    "objectID": "projects/build-server/index.html#installing-openstack",
    "href": "projects/build-server/index.html#installing-openstack",
    "title": "Building my own Server",
    "section": "Installing Openstack",
    "text": "Installing Openstack\nI’ve decided to use the kolla-ansible project to install openstack. It works by using ansible to deploy docker containers, which the openstack services run in.\nThey have a quickstart guide:\nhttps://docs.openstack.org/kolla-ansible/latest/user/quickstart.html\nAnd the ansible playbook can be found here:\nhttps://opendev.org/openstack/kolla-ansible\nAnd they provide a sample ansible inventory for the all in one node.\nI do not need all of those features. I pretty much just want virtualized compute, networking, containers, and kubernetes. I don’t need things like an S3 compatible block storage, a relational database, or an app store. Okay, but maybe I want them.\nI will do more research into what I want and what I don’t want, and edit the ansible playbook accordingly.\nHowever, this method of deployment seems to require two NIC’s (network interface cards). I think I have both, but just in case, I noted another method of deployment, openstack ansible (yeah the naming isn’t the greatest), which deploys openstack without using containers, it actaully installs and configures the services on the host operating system.\nThe openstack ansible All in one deployment, doesn’t seem to have the same requirement of two NIC’s."
  },
  {
    "objectID": "playground/interactive-python-tutorial/index.html",
    "href": "playground/interactive-python-tutorial/index.html",
    "title": "Experiments with Running python in the browser",
    "section": "",
    "text": "My goal is an in browser python editor I can embed into a blog.\nCode is ran client side, so don’t try to crash any server or anything like that.\n\n\n                        \n                    \nAwww, the input function doesn’t seem to work.\nShift + enter/return to evaluate code of the below.\n\nThis kinda works, but also has issues. Input fucntion still doesn’t work.\nShift + enter/return to evaluate below. Or double click.\n\n   import numpy as np\n   np.random.rand(5)\n\nThis is cool, but the code isn’t editable. It just resets itself for some reason.\nMaybe this will work?\n\n\n  Open the project Untitled Project in LiveCodes\n\n\n\n\n\n\n\nyeet = input(\"test\")\n\nprint(yeet)"
  },
  {
    "objectID": "playground/include-text-in-quarto/index.html",
    "href": "playground/include-text-in-quarto/index.html",
    "title": "Can I include text from other files in quarto?",
    "section": "",
    "text": "Just playing around with embeding quarto again.\n&lt;embed src=\"data/test.txt\"&gt;\n\n&lt;object type=\"text/plain\" data=\"file.txt\"&gt;&lt;/object&gt;\n\n\nDoesn’t look quite right.\n\n\nquarto include function\n\n\n{{&lt; include data/test.txt &gt;}}\n\n“Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.”\nThis works. But how can I get formatted text? I would like to include code from files outside the main file, for ease of editing and modularity.\n&lt;script src=\" https://cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js \"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/npm/prismjs@v1.x/plugins/autoloader/prism-autoloader.min.js\"&gt;&lt;/script&gt;\n\n&lt;link href=\" https://cdn.jsdelivr.net/npm/prismjs@1.29.0/themes/prism.min.css \" rel=\"stylesheet\"&gt;\n\n&lt;pre&gt;&lt;code class=\"language-python\"&gt;import numpy as np&lt;/code&gt;&lt;/pre&gt;\n\n\n\n\n\nimport numpy as np\nBit weird, but it works. Now how can I include external content from external files with this?\n\n\n\n\n\n\n\n\ndata/test.txt\n\n\n\nSince quarto can run python, and you can force the output of that to render to markdown with a few options:\n#| output: asis\n#| echo: true\n\nbacktick = \"`\"\na = \"\"\nwith open('data/python.py', 'r') as f:\n    file_contents = f.read()\n    a = file_contents\n\nprint(f'''\n\n{backtick * 3}{{.python filename=\"data/python.py\"}}\n{a}\n{backtick * 3}\n''')\nOf course, the proper quarto syntax is:\n```{python}\n#| options here\n# python code here\n```\n\n\ndata/python.py\n\n# A function to check if a number is prime\ndef is_prime(n):\n  # If n is less than 2, it is not prime\n  if n &lt; 2:\n    return False\n  # Loop from 2 to the square root of n\n  for i in range(2, int(n**0.5) + 1):\n    # If n is divisible by i, it is not prime\n    if n % i == 0:\n      return False\n  # If no divisor is found, it is prime\n  return True\n\n# Test the function with some numbers\nprint(is_prime(2)) # True\nprint(is_prime(3)) # True\nprint(is_prime(4)) # False\nprint(is_prime(5)) # True\nprint(is_prime(6)) # False\n\nIt works! Perhaps an awkward way of doing this, but it works. I can probably even get code from remote repos, since it’s all python. There are other ways to do this, but I didn’t want to do have to rely on any extensions or external dependencies."
  },
  {
    "objectID": "playground/crypto-scam/index.html",
    "href": "playground/crypto-scam/index.html",
    "title": "A very clever crypto scam",
    "section": "",
    "text": "So I was browsing around, and I found this public pastebin.\n\nklovenierm6@193.233.202.76 where the password is bDBShj\nIt seemed like a crypto wallet reset password? But why post this in a pastebin? It seemed suspicious, but I decided to log in.\n\nWow. $10000 worth of Bitcoin, right there for me to take. It seems to good to be true.\nSo, I spun up a wallet and attempted to deposit the money into it.\n\nWow. In order to withdraw the money first, to an unverified account, I first have to deposit around $2000 worth of Bitcoin into their account.\nObviously, this is a scam. It’s a fairly common pattern, “Here’s free money, but first, send me some money.” But this is such an interesting technological twist, I couldn’t not blog about it.\nBut I decided to experiment a little further.\n~ ❯ nmap -sV 193.233.202.76\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-06-11 12:46 PDT\nNmap scan report for vm.lan (193.233.202.76)\nHost is up (0.21s latency).\nNot shown: 995 closed tcp ports (conn-refused)\nPORT      STATE    SERVICE       VERSION\n22/tcp    open     ssh           OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n2222/tcp  open     ssh           OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n5900/tcp  filtered vnc\n5901/tcp  filtered vnc-1\n16992/tcp filtered amt-soap-http\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\nNmap is a port scanning utility that tells you what people are running on their server.\nSo I attempted to connect to their other services. I tried to connect to both vnc servers first, but for both, I got a server not found error. This is probably because those services aren’t up for public usage, as shown by the filtered state they return in the nmap scan. I suspect that they have a firewall that only allows for certain IP addresses or something like that.\nI also tried to connect to the ssh service on port 2222, but it kicked me out saying I needed a public key.\nOooh, a scan later in the day (around 4-5 hours later) is differerent.\n~ ❯ nmap -sV 193.233.202.76\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-06-11 22:00 PDT\nNmap scan report for vm.lan (193.233.202.76)\nHost is up (0.21s latency).\nNot shown: 996 closed tcp ports (conn-refused)\nPORT     STATE    SERVICE VERSION\n22/tcp   open     ssh     OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n2222/tcp open     ssh     OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n5900/tcp filtered vnc\n5901/tcp filtered vnc-1\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 32.93 seconds\nThere appear to have taken the amt-soap-http service offline. Good security protocol, getting rid of uneeded services, minimizing the attack surface, but do they really need those vnc servers?\nI decided to do a little more searching.\nI first searched google, and only the same pastebin came up, which has since been taken offline.\nI did find something interesting by searching on github:\n\nBut upon going through that repo, it was simply someone’s pastebin archiving repo. And in addition to that, the servers in those that I found, I have since been taken offline."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Jeffrey Fonseca",
    "section": "",
    "text": "Over the summer before my freshman year of high school, I took a course at palomar college, CSNT 110, hardware and OS fundamentals.\nI first learned about the internals of computers during a summer course I took at Palomar college online before my freshman year of highschool — CSNT 110/ OS and Hardware Fundamentals.\nDuring my freshman year, when my laptop starting slowing down, I switched to a linux based operating system, in an attempt to make it faster, using knowledge I had obtained from that course.\nSince then, I have been tinkering, playing with software, and doing personal projects. However, when I was taking AP Computer Science Principles, during my senior year of high school, my teacher criticized me for being a maverick. I didn’t bother with the group projects, working on my own projects, although I contributed back to the group often, but I didn’t document what I do.\nAs a response, I created this website. It isn’t fancy, because frontend/website design isn’t my preferred work. But for it does what I desire for it’s purposes, and since then, I’ve been documenting my work, to create a portfolio for myself.\nThe projects documents my progress in my personal projects.\nBlog are my writings and ramblings.\nPlayground is my shorter term experiments and testings.\nGuides are guides I’ve written to help other people. For example, I created the duckdns guide because I was tired of helping people through that myself."
  },
  {
    "objectID": "guides/tmux/index.html",
    "href": "guides/tmux/index.html",
    "title": "Tmux",
    "section": "",
    "text": "Tmux is what’s called a terminal multiplexer. Essentially, it lets you split one terminal window, into multiple similar to how a your desktop interface lets you use multiple windows at once. It has more features than that, but the multiplexing is what this article is going to focus on."
  },
  {
    "objectID": "guides/tmux/index.html#window-control",
    "href": "guides/tmux/index.html#window-control",
    "title": "Tmux",
    "section": "Window control",
    "text": "Window control\nTmux’s most basic and essential feature is creating more windows. First, input the prefix key (default is control + b), and then another key to tell tmux to do something:\n\n\n\nkey\naction\n\n\n\n\nc\ncreate new window\n\n\np\nprevious window\n\n\nn\nnext window\n\n\n0, 1, 2, …\ngo to specific numbered window\n\n\n&\nkill window"
  },
  {
    "objectID": "guides/tmux/index.html#copyscroll-mode",
    "href": "guides/tmux/index.html#copyscroll-mode",
    "title": "Tmux",
    "section": "Copy/Scroll mode",
    "text": "Copy/Scroll mode\nBecause you can’t scroll normally in tmux, you have to enter a special scroll mode with prefix + [. In this scroll mode, you can use q to quit, Control S to search down, and Control R to search upwards. After searching, n goes to the next item, and Shift+n goes to the previous one. q exits search."
  },
  {
    "objectID": "guides/nix-fun/index.html",
    "href": "guides/nix-fun/index.html",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "On a macos or linux system, run thsi command in your terminal (from the nix manual) curl -L https://nixos.org/nix/install | sh -s -- --daemon\nThis should work on all cases, but if you are using WSL/WSL2, and you don’t have systemd enabled, then you can do a single user, no daemon install.\ncurl -L https://nixos.org/nix/install | sh -s -- --no-daemon\nNow, nix is installed. You should be able to access the nix commands in your terminal. For example, the nix-shell command can be used to create a temporary shell environment with packages, environment variables, and more. For examples, see below.\n\n\nHome manager is a way to declaratively manage a user environment, including packages installed, configuration files, and environment variables. It uses nix as the langauge of configuration.\nTo install home manager, you can follow the steps from their docs\nTo use home-manager, you can edit the configuration file located at $HOME/.config/home-manager/home.nix For an example, see my my blog post about this\n\n\n\nFlakes are an “experimental” feature of nix. I put experimental in quotes because they are not enabled by default, and are marked as experimental, but they are so popular in the community and so well tested that they are basically just another feature.\nWithin home-manager’s home.nix:\n\n\nhome.nix\n\nnix.settings.experimental-features = [\"nix-command\" \"flakes\"];\n\nWithout home manager, edit the nix.conf file:\n\n\n$HOME/.config/nix/nix.conf\n\nexperimental-features = nix-command flakes"
  },
  {
    "objectID": "guides/nix-fun/index.html#installing-home-manager",
    "href": "guides/nix-fun/index.html#installing-home-manager",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "Home manager is a way to declaratively manage a user environment, including packages installed, configuration files, and environment variables. It uses nix as the langauge of configuration.\nTo install home manager, you can follow the steps from their docs\nTo use home-manager, you can edit the configuration file located at $HOME/.config/home-manager/home.nix For an example, see my my blog post about this"
  },
  {
    "objectID": "guides/nix-fun/index.html#enabling-flakes",
    "href": "guides/nix-fun/index.html#enabling-flakes",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "",
    "text": "Flakes are an “experimental” feature of nix. I put experimental in quotes because they are not enabled by default, and are marked as experimental, but they are so popular in the community and so well tested that they are basically just another feature.\nWithin home-manager’s home.nix:\n\n\nhome.nix\n\nnix.settings.experimental-features = [\"nix-command\" \"flakes\"];\n\nWithout home manager, edit the nix.conf file:\n\n\n$HOME/.config/nix/nix.conf\n\nexperimental-features = nix-command flakes"
  },
  {
    "objectID": "guides/nix-fun/index.html#connecting-your-github-account-to-git-from-the-terminal",
    "href": "guides/nix-fun/index.html#connecting-your-github-account-to-git-from-the-terminal",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "Connecting your github account to git from the terminal",
    "text": "Connecting your github account to git from the terminal\nFirst, make sure you have git installed, but you probably do already, if you are here.\nThen: nix-shell -p gh. This installs the github cli tool.\ngh auth login # logs you into github\ngh setup-git And with this, git is configured to use the github cli as a credential helper\ngit config --global credential.helper store with this, git stores the credentials permanently.\nNow, while still in the nix environment:\ngit push while in the local copy of a repo you have stored in github.\nCheck the /home/yourusername/.git-credentials file to make sure your git credentials are stored.\nAnd then you should be good to exit the nix environment. Because your credentials are saved, you can now run git push from the command line."
  },
  {
    "objectID": "guides/nix-fun/index.html#deleting-sensitive-detail-or-large-binary-files-from-a-git-repo",
    "href": "guides/nix-fun/index.html#deleting-sensitive-detail-or-large-binary-files-from-a-git-repo",
    "title": "Installing the nix package manager and how it’s useful",
    "section": "Deleting sensitive detail, or large binary files from a git repo",
    "text": "Deleting sensitive detail, or large binary files from a git repo\nGit tracks every change. So if you store something like an image, or a binary in a git repo, if you delete those files later, they will continue to eat space and be wasteful, even if you commit the deletions later on. Or, if you have sensitive data, like passwords or api keys in a repo, even if you delete them in later commits, they will still be present.\nTo alter every past commit, you can use special tools, which are very easy to install using nix.\nnix-shell -p git-filter-repo\nFrom here, you can use the git-filter-repo command to nuke files or folders:\nFirst, cd into your git repo.\ngit-filter-repo --invert-paths --paths path/or/file. git-filter-repo works by only taking anything that matches an expression, so by inverting that, it takes everything except what matches that path.\nAlternatively, the bfg-repo-cleaner tool can be used. I did not opt for this to clean out the images of my git repo, because it doesn’t seem to be able to delete entire paths, or even individual files, only matching filenames, or doing text replacement. However, apparently, it is much faster than git-filter-repo for large repos, due to a different implementation.\nTo replace sensitive data with bfg:\nnix-shell -p bfg-repo-cleaner\nbfg-repo-cleaner --replace-text passwords.txt\nWhere passwords.txt contains data you want to replace.\n\n\npasswords.txt\n\nsecretapikey ==&gt; ***REMOVED*** is the default if you don't have an arrow\nsecretpassword ==&gt; but you can replace it with anything\nglob:*baddata* ==&gt; glob matches work too\n\nTo push the changes to github, or your remote repository:\ngit push --force --all\nAnd then it should be done. This should be seamless, but if you encounter any hiccups, like I did with slow internet speeds causing it to fail, there are some commands you can run:\ngit gc --aggressive optimizes the repository\ngit config --global http.postBuffer 524288000 if you are on a slower network (high latency), this gives it more grace.\nBut for me, because I am currently on vacation and didn’t have access to the internet speeds I do at home, the only thing that worked was actually getting up and moving my laptop to an area with faster wifi.\nNow, I did this to delete unused images from my repo, however, if you are trying to clean sensitive data off of the internet, there are some extra steps you may want to take."
  },
  {
    "objectID": "guides/git/index.html",
    "href": "guides/git/index.html",
    "title": "Git",
    "section": "",
    "text": "Still writing"
  },
  {
    "objectID": "guides/git/index.html#get-your-code-git-clone",
    "href": "guides/git/index.html#get-your-code-git-clone",
    "title": "Git",
    "section": "Get your code: git clone",
    "text": "Get your code: git clone\nThe most basic, starting command.\ngit clone repositoryurl\nGit has it’s own protocol, but https works as well, and it’s easier to use.\nBecause of this, you can use the url of the website that hosts the code as your code url.\ngit clone https://github.com/moonpiedumpling/moonpiedumpling.github.io/\n↑ That’s the url of the repository for this website."
  },
  {
    "objectID": "guides/git/index.html#save-code-to-git",
    "href": "guides/git/index.html#save-code-to-git",
    "title": "Git",
    "section": "Save code to git",
    "text": "Save code to git\nCode stored in a git repo exists in four states: unsaved changes, unstaged changes, staged changes, and committed changes, gone through in that order.\nFirst, you open up a file and edit some code. Then you save your changes to the file. However, git doesn’t save your changes. The changes become unstaged changes, meaning git doesn’t even see them.\nTo stage changes:\ngit add file/folder\nSo to stage everything in the repository: git add .\nNow, git sees your changes, but it still hasn’t saved them. They are staged changes, meaning that git can see them in order to commit them.\ngit commit -m \"message here\" commits your changes with a commit message, which makes keeping track of what you did easier, especially when you need to revert changes.\nYou can commit everything in one go using git commit -am, which is more convinient if you don’t need to stage changes.\ngit status shows staged and unstaged changes."
  },
  {
    "objectID": "guides/git/index.html#interact-with-a-remote-repository",
    "href": "guides/git/index.html#interact-with-a-remote-repository",
    "title": "Git",
    "section": "Interact with a remote repository",
    "text": "Interact with a remote repository\nTo upload your changes to a remote repository: git push\nPretty simple. Although you can push to a specific repo.\ngit push remoterepourl\nAnd to bring changes from a remote repo down: git pull"
  },
  {
    "objectID": "guides/git/index.html#viewing-and-undoing-changes",
    "href": "guides/git/index.html#viewing-and-undoing-changes",
    "title": "Git",
    "section": "Viewing and undoing changes",
    "text": "Viewing and undoing changes\nIf you make a small change, you can just copy and paste stuff around to undo it. But if you are working on a large app, and a massive feature you intended to add didn’t work out, trying to just delete the code won’t really work out.\nThat’s where git comes in.\n\nTo view\ngit log shows the changes. q quits, and / searches. When searching, n to go to the next found string, and N (capital N) to go to the previous. It uses vim keybindings. Each change is noted by their SHA1 hash.\nBut this kind of sucks, so I recommend using a graphical interface to view changes. This is one function of “git forges”, websites like github, where they give you a graphical interface to view changes. However, there are locally installed applications with similar features, like gitg.\n\n\nTo revert to a change\ngit revert HEAD creates a new commit that undoes the previous commit.\ngit revert &lt;SHA1 HASH&gt; creates a new commit that reverts all the way back to the commit designated by the SHA1 hash of a commit.\ngit reset SHA1 has a similar feature, except it simply moves backwards in commits. If you make new commits, this results in a nonlinear search commit history. You’ll create a tree, or an octopus. Some people don’t like this, because it can be a bit of a pain to manage."
  },
  {
    "objectID": "guides/git/index.html#branches",
    "href": "guides/git/index.html#branches",
    "title": "Git",
    "section": "Branches",
    "text": "Branches\nBranches are one of the killer features of version control. Basically, each branch is it’s own copy of the codebase, which can be worked on without affecting other branches.\ngit checkout -b &lt;branch name&gt; creates a new branch.\ngit checkout &lt;branch name&gt; switches to a branch.\nThen, commit away. Switching to another branch and only the changes applied to that branch are present.\nIn the wild, branches are commonly used to manage versioning. An older version, but still maintained version of a program will be kept in another branch, so people can still work on it.\ngit checkout manipulates the repository state. To manipulate branches, you must use git branch:\ngit branch -d &lt;branch name&gt; deletes a branch."
  },
  {
    "objectID": "guides/cockpit-setup/index.html",
    "href": "guides/cockpit-setup/index.html",
    "title": "Setting up cockpit",
    "section": "",
    "text": "What is cockpit (and similar softwares)\nAmazon lets us have free servers via EC2. The typical way to manage servers is either by sshing in, or using the cloud shell that Amazon (and Oracle) give. However, there are alternative ways to manage servers. One extremely popular example is pteradactyl, a webpage based gui to manage game (usually minecraft). It lets you download game servers as docker containers, run them, stop them, and maybe manage some basic settings, All the things a casual who just wants video games may need. But when I created a free Oracle server, I wanted something more. By this point, I was an experienced linux user, and I wanted more advanced features. So I searched for a more advanced server management tool, like people use on real servers, and I found cockpit.\nCockpit comes with many benefits. The two things I really like however, are that it’s terminal is not laggy at all, unlike the amazon ec2 cloud terminal, and it also offers a gui to manage docker containers.\n\n\nThe installation process\nThe installation process is simple:\nsudo apt install cockpit\nTo start the server, run:\nsudo systemctl enable --now cockpit\nThis sets the cockpit server to start on boot, and it starts it now.\nHowever, the firewall must open ports to allow the cockpit server through. This opens the default ports for the cockpit server. It should be noted that not every version fo linux uses ufw as a firewall, some use other firewalls with different management commands.\nsudo ufw allow 9090\nAnother important thing is to set the password for the default “ubuntu” user account so that you can login to cockpit.\nsudo passwd ubuntu\nIt will ask for the new password twice, not showing what you are typing.\nReboot the computer for the server to start properly, however, this won’t work as the virtual private cloud must have its ports open. I had to do this when I set up cockpit on my Oracle server, so I knew the gist of the steps.\n\n\nOpening EC2’s VPC ports (Also necessary if you want to host servers on ports other than 22, 80, or 443)\nFirst, go to your EC2 vps, where you would normally click connect from, and click on the link under vpc:\n\nThis should bring you up to a screen like this:\n\nClick the security tab, bringing you to a screen like this:\n\nAnd then click on security groups, bringing you to a screen like this:\n\nAnd then click on the “edit inbound rules”\nFinally, you should get something like this:\n\nAdd an item that matches what I have in the third row. That opens the port to allow cockpits server to escape. You may also need to use this page to open other ports if you are hosting servers on nonstandard ports."
  },
  {
    "objectID": "blog/switch-to-opensuse/index.html",
    "href": "blog/switch-to-opensuse/index.html",
    "title": "I installed opensuse on my laptop",
    "section": "",
    "text": "School had just ended. During the school year, I had been using a hybrid graphics (dual-gpu) laptop, that otherwise worked normally, but had horrible, terrible battery life. I had struggled to get 2 hours out of it, because the dedicated nvidia gpu would not get turned off properly.\nSo I decided to switch to my second laptop, which has been unused so far. But it has pure intel graphics, and the laptop has much better linux support, and consequently, a much better battery life.\nBefore setting it up, I had a few requiremnts.\nSecurity. Previously, I didn’t care about this, because I carried my laptop around with me at all times, but now I was going to be leaving my laptop unattended, maybe for extended periods of time. That meant I needed full disk encryption, and secure/trusted boot. However, I had limited time to set this up, so I neede to find a distro that did this the easiest way possible. I eventually settled on opensuse, which had an option to set up secure boot and encryption in the installer, under guided partitioning. After some hiccups, it installed just fine.\nAnd the other, is my tools and packages. I didn’t worry too much about this, because I had decided beforehand on using nix and home-manager to install packages not available in the repositories. Home manager is a tool that allows for declarative configuration of a user environmetn, including packages, configuration files, or environment variables, using the nix programming language. Notably, it can be useed on almost all linux distros.\nHere is my current home.nix, as of writing this, the file home-manager takes as an input.\n\n\nShow\n\n\n\nhome.nix\n\n{\n  pkgs ? import &lt;nixpkgs&gt; {},\n  config,\n  lib,\n  ... \n}:\nlet\n  nixgl = import &lt;nixgl&gt; {};\n  nixGlWrapper = import ./nixglwrapper.nix {inherit nixgl pkgs lib config;};\nin\nwith import ./quarto.nix {inherit pkgs config lib;};\nwith import ./nixglwrapper.nix {inherit pkgs config lib nixgl;};\n{\n  # Home Manager needs a bit of information about you and the paths it should\n  # manage.\n  home.username = \"moonpie\";\n  home.homeDirectory = \"/home/moonpie\";\n  targets.genericLinux.enable = true;\n\n  nixpkgs.config.allowUnfree = true;\n  nix.settings.experimental-features = [\"nix-command\" \"flakes\"];\n\n  # This value determines the Home Manager release that your configuration is\n  # compatible with. This helps avoid breakage when a new Home Manager release\n  # introduces backwards incompatible changes.\n  #\n  # You should not change this value, even if you update Home Manager. If you do\n  # want to update the value, then make sure to first check the Home Manager\n  # release notes.\n  nix.package = pkgs.nix;\n  home.stateVersion = \"23.11\"; # Please read the comment before changing.\n\n  # The home.packages option allows you to install Nix packages into your\n  # environment.\n  #fonts.fontconfig.enable = true;\n  xdg.mime.enable = true;\n  home.packages = [\n    #nixgl\n    nixgl.nixGLIntel\n    nixgl.nixVulkanIntel\n\n    (nixGLWrap pkgs.vscode)\n    (nixGLWrap pkgs.microsoft-edge)\n    (nixGLWrap pkgs.firefox)\n\n\n    pkgs.micro\n    pkgs.calibre\n    pkgs.languagetool\n    pkgs.git\n    pkgs.soundwireserver\n\n    quarto\n    pkgs.jupyter\n    pkgs.python3\n\n    pkgs.yt-dlp\n\n\n    pkgs.macchanger\n    pkgs.nmap\n    pkgs.wireshark\n\n    pkgs.gocryptfs\n    # # Adds the 'hello' command to your environment. It prints a friendly\n    # # \"Hello, world!\" when run.\n    # pkgs.hello\n\n    # # It is sometimes useful to fine-tune packages, for example, by applying\n    # # overrides. You can do that directly here, just don't forget the\n    # # parentheses. Maybe you want to install Nerd Fonts with a limited number of\n    # # fonts?\n    # (pkgs.nerdfonts.override { fonts = [ \"FantasqueSansMono\" ]; })\n\n    # # You can also create simple shell scripts directly inside your\n    # # configuration. For example, this adds a command 'my-hello' to your\n    # # environment:\n    # (pkgs.writeShellScriptBin \"my-hello\" ''\n    #   echo \"Hello, ${config.home.username}!\"\n    # '')\n  ];\n\n  # Home Manager is pretty good at managing dotfiles. The primary way to manage\n  # plain files is through 'home.file'.\n  home.file = {};\n  home.sessionVariables = {\n    # EDITOR = \"emacs\";\n  };\n\n  # Let Home Manager install and manage itself.\n  programs = {\n    home-manager.enable = true;\n    bash.enable = true;\n    gh.enable = true;\n  };\n}\n\n\nOf course, the home.nix file isn’t all there is too it. There are also some imports, which take info from files that aren’t home.nix.\nI have two imports, as of right now.\nUsing nix to run applications on non-nixos distros mostly works, but has some quirks. One quirk is that hardware accelerated graphics (opengl, vulkan) is lacking. In order to get around this, I use a program called nixgl. However, nixgl is essentially a wrapper, and it works by calling hte program you want to run as a command line argument. Someone automated that in nix, and I integrated that into my code.\n\n\nShow\n\n\n\nnixglwrapper.nix\n\n{ config, pkgs, lib, nixgl } :\n{\nnixGLWrap = pkg: pkgs.runCommand \"${pkg.name}-nixgl-wrapper\" {} ''\n    mkdir $out\n    ln -s ${pkg}/* $out\n    rm $out/bin\n    mkdir $out/bin\n    for bin in ${pkg}/bin/*; do\n     wrapped_bin=$out/bin/$(basename $bin)\n     echo \"exec ${lib.getExe nixgl.nixGLIntel} $bin \\\"\\$@\\\"\" &gt; $wrapped_bin\n     chmod +x $wrapped_bin\n    done\n  '';\n  nixVulkanWrap = pkg: pkgs.runCommand \"${pkg.name}-nixgl-wrapper\" {} ''\n    mkdir $out\n    ln -s ${pkg}/* $out\n    rm $out/bin\n    mkdir $out/bin\n    for bin in ${pkg}/bin/*; do\n     wrapped_bin=$out/bin/$(basename $bin)\n     echo \"exec ${lib.getExe nixgl.nixVulkanIntel} $bin \\\"\\$@\\\"\" &gt; $wrapped_bin\n     chmod +x $wrapped_bin\n    done\n  '';\n}\n\n\nAnd of course, finally my custom quarto package that I had made in another post\n\n\nShow\n\n\n\nquarto.nix\n\n{ pkgs, config, lib, ... } :\n\nlet \n    pandoc = null;\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\nin\n {\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        buildInputs = with pkgs; [ ];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\n}\n\n\nHowever, in order for running some programs with sudo to work, I had to edit opensuse’s default sudo configuration to keep environment variables, and not change the default path. This is an understandable thing to do on a multi user system, but on my single user system where I want to use some packages installed via nix with sudo, it is just annoying.\n\n\n/etc/sudoers\n\n...\n##\n## Defaults specification\n##\n## Prevent environment variables from influencing programs in an\n## unexpected or harmful way (CVE-2005-2959, CVE-2005-4158, CVE-2006-0151)\n#Defaults always_set_home\n## Use this PATH instead of the user's to find commands.\n#Defaults secure_path=\"/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/bin:/usr/local/sbin\"\nDefaults !env_reset\n## Change env_reset to !env_reset in previous line to keep all environment variables\n....\n\nI’ve now uploaded my home.nix to a github repo: https://github.com/moonpiedumplings/home-manager\nHere is the home.nix that is currently in the main branch of my github repo (this is dynamically rendered and updated every update of this blog)\n\n\nhome.nix\n\n{\n  pkgs ? import &lt;nixpkgs&gt; {},\n  config,\n  lib,\n  ... \n}:\nlet\n  nixgl = import &lt;nixgl&gt; {};\nin\nwith import ./quarto.nix {inherit pkgs config lib;};\nwith import ./nixglwrapper.nix {inherit pkgs config lib nixgl;};\n{\n  # Home Manager needs a bit of information about you and the paths it should\n  # manage.\n  home.username = \"moonpie\";\n  home.homeDirectory = \"/home/moonpie\";\n  targets.genericLinux.enable = true;\n\n  nixpkgs.config.allowUnfree = true;\n  nix = {\n    settings = {\n    experimental-features = [\"nix-command\" \"flakes\"];\n    };\n  };\n\n  # This value determines the Home Manager release that your configuration is\n  # compatible with. This helps avoid breakage when a new Home Manager release\n  # introduces backwards incompatible changes.\n  #\n  # You should not change this value, even if you update Home Manager. If you do\n  # want to update the value, then make sure to first check the Home Manager\n  # release notes.\n  nix.package = pkgs.nix;\n  home.stateVersion = \"23.11\"; # Please read the comment before changing.\n\n  # The home.packages option allows you to install Nix packages into your\n  # environment.\n  #fonts.fontconfig.enable = true;\n  xdg.mime.enable = true;\n  home.packages = [\n    #nixgl\n    nixgl.nixGLIntel\n    nixgl.nixVulkanIntel\n\n    #nixgl wrapped stuff\n    (nixGLWrap pkgs.vscode)\n    (nixGLWrap pkgs.microsoft-edge)\n    (nixGLWrap pkgs.firefox-bin)\n\n\n    #general tools and utilities\n    pkgs.micro\n    pkgs.calibre\n    pkgs.languagetool\n    pkgs.htop\n\n    #git tools\n    pkgs.git\n    pkgs.bfg-repo-cleaner\n    pkgs.git-filter-repo\n\n    #share sound with android devices.\n    pkgs.soundwireserver\n\n    # development enviroment stuff\n    quarto # see the imports above.\n    (pkgs.python311.withPackages(ps: with ps; [ jupyter ]))\n    pkgs.poetry\n\n    # general cli tools\n    pkgs.yt-dlp\n\n    #hacking\n    pkgs.macchanger\n    pkgs.nmap\n    pkgs.wireshark\n    pkgs.metasploit\n    pkgs.aircrack-ng\n    pkgs.mdk4\n    pkgs.airgeddon\n\n    #creativity\n    pkgs.manuskript\n\n    # storage and encryption\n    pkgs.rclone\n    pkgs.gocryptfs\n    pkgs.syncthing\n    # # It is sometimes useful to fine-tune packages, for example, by applying\n    # # overrides. You can do that directly here, just don't forget the\n    # # parentheses. Maybe you want to install Nerd Fonts with a limited number of\n    # # fonts?\n    # (pkgs.nerdfonts.override { fonts = [ \"FantasqueSansMono\" ]; })\n\n    # # You can also create simple shell scripts directly inside your\n    # # configuration. For example, this adds a command 'my-hello' to your\n    # # environment:\n    # (pkgs.writeShellScriptBin \"my-hello\" ''\n    #   echo \"Hello, ${config.home.username}!\"\n    # '')\n  ];\n\n  # Home Manager is pretty good at managing dotfiles. The primary way to manage\n  # plain files is through 'home.file'.\n  home.file = {};\n  home.sessionVariables = {\n    # EDITOR = \"emacs\";\n  };\n\n  # Let Home Manager install and manage itself.\n  programs = {\n    home-manager.enable = true;\n    bash.enable = true;\n    gh.enable = true;\n  };\n  services.syncthing.enable = true;\n}"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 3, 2023\n\n\nSwitching to opensuse tumbleweed, research into nixos\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nFirst job, an internship at cirrascale\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nIncreasing my security\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nI installed opensuse on my laptop\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\nWhy schools should be less aggressive with content blocking\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/blocking/index.html",
    "href": "blog/blocking/index.html",
    "title": "Why schools should be less aggressive with content blocking",
    "section": "",
    "text": "I love experimenting with, and managing internet servers. Over the years, I have learned real world skills from my personal projects, as shown by the role I play my the computer science classes here at del norte. I do things, like create guides or fix broken one’s.\nBut when I try to access my server on school wifi, I get met with the below image.\n\nNot managed. My server is not blocked because it contains dangerous or harmful content, it’s blocked because my school’s (and so many others) uses a whitelist system. A whitelist means that everything is blocked by default, and things must explicitly be allowed through the firewall.\nNow I have a way around this. I can use a VPN, or similar software to get around the content filtering restrictions In addition to that, I can simply work on my home wifi, where there are no such restrictions. For me, getting access to my project based learning environments are easy.\nBut for students, who only use chromebooks, they cannot get around these restrictions. Chromebooks, the managed devices distributed by our school and so many others, are completely locked down. The chromebook itself implements the content blocking software, rather than the WiFi.\nThis means that, if a student were working at home on a school chromebook that was their only device, they would be denied resources that I have access to, purely because I have my own laptop. Information, research material, learning material, troubleshooting guides, and things like my digital playgrounds are locked away from those students.\nIt gets worse. Because the restrictions on the chromebooks prevent them from installing software, they cannot install the necessary tools needed for our computer science classes here at Del Norte. And unlike so many other computer based courses at Poway USD, the CompSci class does not provide it’s own computers for use. I personally know someone who wanted to take computer science, but was forced to drop out because they could not obtain their own device.\nObviously, this is unfair. The point of chromebooks is to offer students who are unable to obtain their own device for whatever reason the ability to have a computer to work on so they can be on equal footing with those who can afford their own devices. But what’s the purpose if they don’t actually offer students the ability to participate in the same activities and classes that students with more resources — wealth — have?\nChromebooks should be, at the very least, usable for all classes. But the only just thing is that students with only school provided chromebooks are given access to the exact same resources as those who can afford to buy their own device.\nThe reason why schools can and do provide chromebooks and other internet services is that they are cheap — Cheaper than they are normally. In America, there exists a program called e-rate, which is designed to make technology more accessible to public institutions, by providing discounts or potentially free products and services. The government subsidizes the cost.\nSadly, this program does not seem to cover computers (see section about internet access), which is probably why schools opt for chromeboks, the cheapest, lowest end device. Although it should be noted, that Google and Apple do have their own deals for offering discounts for bulk purchases for schools and other public institutions.\nAs great as e-rate is (it’s why the WiFi on school and college campus is so fast, if you know how to utilize it), it comes with caveats and conditions.\nThe same way the federal government holds funding for certain services over the head of state and local governments, the federal government holds the eligibility for e-rate over the head of schools and libraries. They must meet a certain requirements. One of those conditions is following CIPA, the Children’s Internet Protection Act.\nIn the beginning of this article, I linked an image of what our school’s block screen looks like. Quoting below:\n\nThe site you have requested has been blocked because it does not comply with the filtering requirements as described by the Children’s Internet Protection Act (CIPA).\n\nSo, when you click on CIPA, it takes you to the law itself. The law is very short, very vague, and the really important parts can fit comfortably into this article:\n\nSchools must implement a policy addressing these things. It does not say exactly what policies they must implement, or does it define materials harmful to minors, or even define “hacking”.\nBecause of the way the law is vaguely worded, schools are obligated to implement as aggressively as a content blocking policy as they can, including the complete prevention of installing software on chromebooks. Because the truth of the matter is, if someone can install VSCode (Programming application we use at Del Norte), then they can potentially install censorship circumvention software.\nIt’s very clear what is happening here. The school cannot meet both obligations, one to prevent students from accessing resources, and another to make resources accessible to students. Becuase not attempting to block everything they can leads to losses on the e-rate discounts, Del Norte breaks it’s promise to it’s students to provide resources to them. Because to this institution, and many more, money is more important than the education of their students.\nNow, it’s not like I am personally not doing anything about this. In this blog, I have another post, about creating a system that will let students access. In addition to simply setting this system up, I am optimizing it, to make it cheaper and more accessible to students, especially those with less resources.\nOf course, as amazing as what I am deploying is, allowing students to get access to a fully featured linux desktop from their browser, it is also flawed — it also acts as a censorship circumvention software. Inside the system they have access to, users are given unfiltered content. Because of this, there is a possibility that schools would be obligated to block this as well.\nUltimately, my software is cool, equipping at least our computer science class the ability to be done on nothing but chromebooks, it is but a band-aid for the real problem — the laws that force schools further limit access to digital resources for students who already are lacking resources.\nIt does them little good. Off the top of my head, I know of several different ways to get around these content blocker, even on chromebooks. These blocking measures certainly do have an effect for general purpose use, but against a dedicated student, they are ultimately ineffective.\nFor some context, CIPA went into effect in 2001. The first iPhone came out in 2005. The law’s intentions are nice, but it pretty clearly wasn’t created with the foreknowledge that every student (who could afford one) would have a device in their pocket capable of circumventing pretty much all of the content blocking restrictions.\nBecause of this, despite the law being written to “protect” everybody, it only actually affects one group of people. Those who can only rely on school provided devices.\nThis is a form of institutional oppression of those who are in a lower socioeconomic class. Even though chromebooks are easily capable of installing the necessary software for computer science courses, laws in place force administrators to prevent students from doing so.\nThis is unjust. The restrictions on these laws should be lessened to enable students who already have a lesser access to such resources an equal access. If parents can be trusted to monitor their kid’s internet usage on a personal device, like a phone or macbook, why can’t they be trusted to do the same with a school chromebook?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/cirrascale-intern/index.html",
    "href": "blog/cirrascale-intern/index.html",
    "title": "First job, an internship at cirrascale",
    "section": "",
    "text": "Cirrascale is a San Diego company that offers managed cloud computing, targeted at AI. Before my vacation, I took a tour of their hosting center. It honestly had me awestruck, seeing rack upon rack of water cooled servers.\n2023-7-10\nFirst day. It was pretty nice. My skills with managing linux, containers, and virtual machines came in handy. I took initiative and stepped up when a task was offered, which was tough because I was so new. My heart was literally racing and I felt so nervous, but I didn’t take on more than I could handle and I made good progress on it in that same day. In addition to that, I collaborated with the other interns on things. Overall, it was a lot of fun, and the people were very friendly.\n2023-7-12\nIn the last two days I feel like I have done a lot, despite being so new. I helped another intern install Ubuntu linux, and I am now working on another project, creating a proxmox image with packer to deploy using maas.\n2023-8-1\nSince then, I have completed created an image, although I haven’t tested yet. In addition to that, I got to post all my code on github:\nhttps://github.com/moonpiedumplings/proxmox-maas"
  },
  {
    "objectID": "blog/more-security/index.html",
    "href": "blog/more-security/index.html",
    "title": "Increasing my security",
    "section": "",
    "text": "Previously, I’ve been very lax with security. But as I go into the world, and will be leaving my devices unattended, and as what is on my computers becomes more valuable than game site logins, I now need to put actual effort into securing my devices."
  },
  {
    "objectID": "blog/more-security/index.html#cleaning",
    "href": "blog/more-security/index.html#cleaning",
    "title": "Increasing my security",
    "section": "Cleaning",
    "text": "Cleaning\nI needed to clean the data off of the USB’s. However due to USB’s unique method of data storage, simply deleting the data isn’t enough. I decided to overwrite the data once for a more secure erase. However, it wasn’t practical to overwrite whole files, so I have to find the sensitive data, so that I can delete it. This is much faster than overwriting everything.\nfind . -type f -regex '.*mozilla.*\\.\\(json\\|db\\|sqlite\\)$'\nThis uses a regex to find anything ending in .json, .db, or .sqlite, that is also has a parent or grandfather directory with the name mozilla in it.\nfind . -type f -regex ''.*mozilla.*\\.\\(json\\|db\\|sqlite\\)$' -exec &lt;securedeletecommand&gt; {} \\;\nWith this, I can run my secure delete command on my USB’s. I think I am going to use srm a command line tool that is compatible with the rm file remover standard util.\nsrm -rfsv is the command I will use.\nfind . -type f -regex '.*mozilla.*\\.\\(json\\|db\\|sqlite\\)$' -exec srm -rfsv {} \\;\nAnd this will clean my external storage quite nicely, while not touching other databases or json files that don’t have sensitive data."
  },
  {
    "objectID": "blog/more-security/index.html#virtual-fido-device",
    "href": "blog/more-security/index.html#virtual-fido-device",
    "title": "Increasing my security",
    "section": "Virtual FIDO Device",
    "text": "Virtual FIDO Device\nI may also consider a virtual Fido2 device, to emulate a yubikey, if I don’t really want to keep them plugged in. Here are two projects I found:\nhttps://github.com/bulwarkid/virtual-fido\nhttps://github.com/psanford/tpm-fido\nhttps://github.com/danstiner/rust-u2f"
  },
  {
    "objectID": "blog/switch-tumbleweed/index.html",
    "href": "blog/switch-tumbleweed/index.html",
    "title": "Switching to opensuse tumbleweed, research into nixos",
    "section": "",
    "text": "I recently switched from opensuse leap to opensuse tumbleweed. The upgrade process was almost completely seamless. I simply followed the official documentation from opensuse.\nHowever, after I upgraded to tumbleweed, secure boot wasn’t working. I decided to do a fresh reinstall of tumbleweed… and secure boot still didnt’ work.\nHowever, other than that, I a very happy with opensuse tumbleweed. The repositories are large and kept up do date, and some packages which weren’t available or were more dated on opensuse leap such as gocryptfs, syncthing, or firefox, where available in the repositories. I did not have to use nix to install them.\nIn addition to that, the opi command line helper, lets you easily install third party packages from the open build service, or repositories like microsoft’s, so that you can get packages like microsoft edge or vscode, without having to use nix.\nHowever, secure boot not working is detrimental for me. I did a little bit of asking around, and another user complained about a similar struggle, claiming that they have never gotten opensuse tumbleweed secure boot working, and my struggle was futile.\nHmm, in my research, I found an open issue: https://bugzilla.opensuse.org/show_bug.cgi?id=1209985. It appears to be a bug in opensuse tumbleweed. That sucks.\nIt should be automatic, except it isn’t working. I am considering switching to nixos, so that I can have secure boot working, although I don’t really like the most popular implementatio of secure boot on nixos, lanzaboote, because it stores kernels in the esp system partition, which may run out of space because it is usually very small, and nixos often stores many older kernels.\nRelevant links:\nhttps://nixos.wiki/wiki/Secure_Boot\nhttps://nixos.wiki/wiki/Security#Filesystem_encryption\nreddit post where a commmenter posted instructions\nhttps://github.com/DeterminateSystems/bootspec-secureboot\nThis looks very appealing. Rather than using it with systemd-boot, I might be able to set up grub."
  },
  {
    "objectID": "guides/duckdns/index.html",
    "href": "guides/duckdns/index.html",
    "title": "How to get a subdomain from duckdns",
    "section": "",
    "text": "Why?\nPeople can’t register for freenom consistently, and it can take time to get a domain from the one’s we have as a class. Duckdns allows people to create their own free domain, extremely easily, and nearly instantly.\n\n\nRegistration and Setup\n\nThis is the registration page. I really like duckdns because you only need a github account to login, which we already have.\n\nAfter you login, you will see this. You can get your own subdomain, and then set your ip address manually.\nTo find the ip address of your server, run this command on your AWS server:\ncurl ifconfig.me\nIf the command curl is not found, install it using apt.\nThen, you can manually input your server’s ip address into the duckdns website.\nIn your nginx confiuration file, make sure you set your nginx configuration file to be your domain name.\nserver {\n    listen 80;\n    listen [::]:80;\n    server_name [yoursubdomain].duckdns.org;\n\n    location / {\n        proxy_pass http://localhost:8087; # Make sure this matches the port your docker-compose.yml is set to\n        # Simple requests\n        if ($request_method ~* \"(GET|POST)\") {\n                add_header \"Access-Control-Allow-Origin\"  *;\n        }\n\n        # Preflight requests\n        if ($request_method = OPTIONS ) {\n                add_header \"Access-Control-Allow-Origin\"  *;\n                add_header \"Access-Control-Allow-Methods\" \"GET, POST, OPTIONS, HEAD\";\n                add_header \"Access-Control-Allow-Headers\" \"Authorization, Origin, X-Requested-With, Content-Type, Accept\";\n                return 200;\n        }\n    }\n}"
  },
  {
    "objectID": "guides/index.html",
    "href": "guides/index.html",
    "title": "Guides",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 7, 2023\n\n\nTmux\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nGit\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nUrestricted Wifi/Hotspot as long as you have one unlimited device\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nInstalling the nix package manager and how it’s useful\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\nNginx proxy manager\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nHow to get a subdomain from duckdns\n\n\n\n\n\n\n\nSep 30, 2022\n\n\nSetting up cockpit\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/npm/index.html",
    "href": "guides/npm/index.html",
    "title": "Nginx proxy manager",
    "section": "",
    "text": "What is NPM and why do I want to use it?\nNginx proxy manager (npm, but not the node one) is a web based frontend for nginx that automatically also configures letsencrypt, similar to how certbot does it. It makes nginx much easier to use. Rather than writing config files, you can just click around, which is much easier. For a high school computer science class, I think NPM is better, because it doesn’t have the complexity of npm (less potential for accidental failurs), but still teaches people about ports mapping, encryption, and other necessary skills.\nIn addition to that, with npm, even if someone does create a bad config, only their server goes down. With npm, a bad nginx config leads to the whole server going down. That is… not optimal.\n\n\nInstallation and Setup\nInstallation is simpleish.\nFirst, create a docker network for usage with our docker containers (step from here). Because these are our school projects, I will call that network nighthawks\ndocker network create nighthawks\nCreate a folder called npm, and put a docker-compose.yml in it (basic compose file from here):\nversion: '3'\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      - '80:80'\n      - '81:81'\n      - '443:443'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\ndocker-compose up -d and you’re good to go. It should be noted that you need to have ports 443 and 80 unused by anything else, like Nginx proper. So if you are running nginx, stop it first before you up NPM.\nNPM does need to have port 81 accessible.\nYou can either use a reverse proxy, or open up the port to be accessible from the internet.\nIf you want the port to be accessible from the internet, you might have a firewall of some kind, so just open that. And if you are using one of the big cloud providers (aws, azure, oracle), then you also might have to configure security control groups, as that acts as an extra firewall for those server types. See my cockpit guide on how to do this with AWS.\nIf you want to do a reverse proxy, just use npm to do it: Use proxy post to connect http://npm_app_1:81 to a domain name.\nNow, to configure npm, just access the web interface at https://[domainname/ip]:81\n\n\nUsage\nYou may notice above, in the section about using a reverse to expose npm, I use the docker container name, rather than a port. That’s the amazing part of npm. As long as your docker containers are on the same network, all you need is a hostname and the used port. You don’t even need to expose ports in your docker-compose.yml\nSo rather than the docker-compose.yml we use in our deployment guide\nWe can use:\nversion: '3'\nservices:\n      web:\n              image: flask_port_v1\n              build: .\n              #ports: # ports section not needed\n                     # - \"8086:8080\"\n              volumes:\n                      - persistent_volume:/app/volumes\nvolumes:\npersistent_volume:\n  driver: local\n  driver_opts:\n    o: bind\n    type: none\n    device: /home/ubuntu/flask_portfolio/volumes\n    # replace just flask_portfolio\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\nAnd then you can simply expose http://flask_port_v1_web_1:8080 to the world!"
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html",
    "href": "guides/unrestricted-tethering/index.html",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "",
    "text": "So I was on a plane ride recently, and the plane gave free, unlimited wifi, to one device, as long as you were using a T-Mobile data plan.\nI used my phone for a bit, but then I wanted to use my computer.\nOkay, so I’m writing this later, after I had written the guide. Apparently, the plan for the boat was 4 devices per person, not total. And even though I had wifi, when I tested USB tethering, it just worked. Either my phone does the proxying automatically or something, because it shouldn’t have worked according to reports of people I’ve talked to online.\nBut after looking at some configs on my computer, It does look like my phone creates a virtual network which my computer connects to, and proxies all traffic."
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html#why",
    "href": "guides/unrestricted-tethering/index.html#why",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "",
    "text": "So I was on a plane ride recently, and the plane gave free, unlimited wifi, to one device, as long as you were using a T-Mobile data plan.\nI used my phone for a bit, but then I wanted to use my computer.\nOkay, so I’m writing this later, after I had written the guide. Apparently, the plan for the boat was 4 devices per person, not total. And even though I had wifi, when I tested USB tethering, it just worked. Either my phone does the proxying automatically or something, because it shouldn’t have worked according to reports of people I’ve talked to online.\nBut after looking at some configs on my computer, It does look like my phone creates a virtual network which my computer connects to, and proxies all traffic."
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html#how",
    "href": "guides/unrestricted-tethering/index.html#how",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "How",
    "text": "How\nThere is probably an equivalent set of steps you can take to get this setup using iSH, the iPhone terminal emulator, and iPhone USB tethering, as this process is universal, but this guide is written for Android specifically, as that is what I have.\nFirstly, tether your phone using USB tethering. Simply plug your android device into your computer, and then in the settings, you can usually find the usb tethering option in the tethering and mobile hotspot options, wireless and networks, or usb preferences sections of settings. Test if this works. I should have tested, instead of trying to do everything in this guide first, because the security of United airlines Wifi seems to be less than that of the cell carrier of that one reddit user who said that simply USB tethering didn’t allow them to get around their carrier hotspot data cap for other devices.\nInstall termux. Termux is a terminal emulator for android, which gives access to many linux utilities. Yyou can get it from F-droid, or the Github Releases. The version in the Google Play store is outdated, and not recommended.\nIn termux, first update the system:\npkg update\nThen, install the necessary package. We are going to be using a Socks5 Proxy for this.\npkg install microsocks\nNow run the command ip a You should get an output similar to this\nmoonpie@localhost:~/vscode/moonpiedumplings.github.io&gt; ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: wlan0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether f6:41:0a:25:44:c5 brd ff:ff:ff:ff:ff:ff permaddr 0c:dd:24:ca:bb:f1\n    altname wlo1\n    altname wlp0s20f3\n3: usb0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000\n    link/ether 2a:56:99:ae:d0:6f brd ff:ff:ff:ff:ff:ff\n    altname enp0s20f0u1\n    inet 192.168.67.6/24 brd 192.168.67.255 scope global dynamic noprefixroute usb0\n       valid_lft 3573sec preferred_lft 3573sec\n    inet6 fe80::96de:a3e6:3915:6283/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\nAlthough this is ran from my computer which is currently tethered using this method, so your output won’t be the same. You will lack a usb0 part, and you wlan0 will have an inet value. Note this value, as this is the IP address of your phone on the wifi you are connected to.\nNow, for security purposes, it’s important to ensure that the proxy requires authentication, otherwise anyone can connect to it.\nmicrosocks -u username -p password -1 # This command requires a username and password to authenticate once, then whitelists, the connecting ip, so that it doesn’t have to authenticate.\nTo authenticate the proxy to put your device on the whitelist, you can use any method to connect to a socks5, authenticated proxy, such as curl --socks5 user:password@phoneip:1080 google.com, on your computer or connecting device.\nThen, you can connect to the proxy using other means. For example, firefox has a built in option to connect to socks5 proxies, although it does not support authentication.\n\nWith this, the traffic from your browser will go through your phone, enabling you to get around the one-device restriction.\nAlternatively, on linux, there is usually a proxy options with this setting, to enable you to set a socks5 proxy system wide.\nHere is this option in my KDE system settings.\n\nIf you have no way to authenticate a socks5 proxy, then you can run an unauthenticated proxy. Simply run microsocks, and connect the same way.\n\n\n\n\n\n\nWarning\n\n\n\nThis method works by creating an unauthenticated socks5 proxy server. Although not apparent to the average user, anyone with scanning utilities on the same network can see this server, and connect to it.\nIf they connect, then no private data of yours will be put in jepoardy.\nHowever, if they do connect they will be able to eat up your bandwidth, potentially slowing your connection down, or they could do illegal things and you could be held liable for them, as it is your connection.\nI’m going to see if there is some way to bind this only to the tethered device.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen authenticating to a socks5 server, the passwords are sent over the network in plaintext. Although if you connect over USB tethering, no network should be exposed to the wireless network, if you are connecting directly over the wireless network, then people would be able to sniff your password and connect to your socks5 server.\nI might research if there is a way to limit the connections in advance, but it might not be possible with such a small and lightweight server application.\n\n\nAirplane wifi, and later on when I get to it, boat wifi, are both really slow. Don’t expect much speed from this setup when you are already limited. But I suspect this works great for those who live in areas without internet, but are on cellular data plans that let them created limited hotspots or do limited tethering."
  },
  {
    "objectID": "playground/cruise-ship-computers/index.html",
    "href": "playground/cruise-ship-computers/index.html",
    "title": "The cruise ship I am on has very locked down computers",
    "section": "",
    "text": "So I finally found the internet cafe on the cruise ship. I didn’t think they had one.\nThe computers look something like this:\n\nThey have a keyboard, mouse and screen, but the rest of the machine was locked away, physically, behind the desk. I couldn’t get to it, and the cabinets were locked.\nThe only reason I know it’s windows is because all devices on this ship, including point of sale devices appear to be using windows. However, there is no taskbar, or windows UI visible, just a chrome browser in an incognito tab session, and a big blue “End Internet Session” button in the top right. When presses, this button completely resets the session.\nSo obviously, I decided to do some snooping around.\nCan I get open task manager from the cntrl alt delete interface? :\n\nNo, I can’t. But I can do other things.\n\nControl + O brings up the file open dialog, which lets me see the immediate user filesystem. Using this, I figured out the currently logged in user. Then, using chrome’s file:// dialog, I decided to do some browsing.\n\nI went through all the directories of the currently logged in user, but I didn’t find anything of note. It seems like the reset of the seession deletes any downloaded files, or something like that.\nHowever, I quickly discovered something more interesting:\n\nI could get to the C:/Windows directory, where the explorer.exe file was. Explorer.exe is the program that launches the windows UI.\nBut when I tried to click on it, chrome just downloaded it instead. But this was promising.\nSo I tried accessing the C drive through the “open file” interface that pops up when you hit Control + O.\n\nHowever, when I tried to access the C drive, I got an access denied. But if I put in the absolute path like so:\n\nThen it worked:\n\nI right clicked on explorer.exe, and selected “open”. And I got the windows UI to launch.\n\nFrom here, I had broken out of the small box that I was locked in. But I decided to snoop around more.\n\nAww, command prompt was disabled. Too bad.\n\nBut powershell wasn’t.\n\nWindows defender was active, but it was complaining about virus definitons being outdated.\nThe settings app wouldn’t launch, it would close instantly. And attempting to launch control panel made erred as well, complaining about being disabled by the administrator.\nSo, I decided to use powershell to do further snooping around:\n\nWow, that is an old version of windows. It looks like they haven’t updated this system from when they first installed it in 2020 (It’s currently 6/24/2023 as I write this).\n\nThe whoami command tells a little bit about users and permissions. Looking at permissions, I notice something. The user I am logged in as is denied all permissions except one, bypass traversal checking. Bypass traversal checking allows users to visit lower level directories they haven’t been explicitly granted permission to go to, even if they are denied to higher level directories.\nIn this case, even though I have been denied access to the C drive, this singular permission enabled me to get into C:/Windows, and launch the explorer.exe program. This singular persmission created a security hole for me to crawl to.\nNow, there are probably ways I can exploit this further. Since these machines are connected to the network, even if they aren’t connected to the internet, I could use an http server on another network connected device to send an payload to be executed on these devices, targeting a vulnerability not patched in this older version of windows. Or, since it is an older version of windows, there may be a easy exploit, something I could simply type into powershell.\nI ended up not going any further. Ironically, I was pretty busy this vacation, between my other personal projects, and all the traveling."
  },
  {
    "objectID": "playground/dorking/index.html",
    "href": "playground/dorking/index.html",
    "title": "Dorking around",
    "section": "",
    "text": "So I recently saw Maia Crimew’s (in?)famous blog post, How to completely own an airline in 3 easy steps. In this, she describes how she managed to get ahold of confedential data from major airlines, using little technological knowlege, or actual hacking.\nSh used the zoomeye internet search engine, somewhat an analogue of google, except rather than searching all websites, it searches all internet connected devices — including insecure ones.\nI was inspired by her, to do my own “dorking,” or search engine based hacking, trying to find vulnerable public services. She searched for public jenkins servers, which is a build and deployment system, that seems to be able to leak secrets if not configured correctly.\nI decided to search for something even easier. I searched for xterm, a browser based terminal. It took some tinkering with the terms, but eventually I found some publicly exposed servers. People had left an xterm session running, sometimes even with root privileges enabled.\nI decided to search further. I searched guacamole, a web based connector to remote desktop protocol sessions to see if anyone had left any exposed."
  },
  {
    "objectID": "playground/index.html",
    "href": "playground/index.html",
    "title": "Playground",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 6, 2023\n\n\nCan I write my resume in python?\n\n\n\n\n\n\n\nJun 21, 2023\n\n\nThe cruise ship I am on has very locked down computers\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nDorking around\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nA very clever crypto scam\n\n\n\n\n\n\n\nJun 11, 2023\n\n\nCan I include text from other files in quarto?\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nExperiments with Running python in the browser\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "playground/resume/index.html",
    "href": "playground/resume/index.html",
    "title": "Can I write my resume in python?",
    "section": "",
    "text": "Note\n\n\n\nI’ve changed the python blocks to not auto execute, because the github actions environment does not seem to be able to render them.\n\n\nI am trying to generate a two column resume, but nothign I have tried has been able to output to both pdf and html format. So I am now trying to generate a resume using jupyter and pythons graph generating capabilities.\nI found an example online, but it has syntax errors when I try to run it, and I also don’t know what the code is liscensed under, so I think I will try to write my own.\nFirst, I need to select a graphics library. They used matplotlib, but I will search for an easier way to do it first.\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np\n\ndef plot_func(freq):\n    x = np.linspace(0, 2*np.pi)\n    y = np.sin(x * freq)\n    plt.plot(x, y)\n\ninteract(plot_func, freq = widgets.FloatSlider(value=7.5,\n                                               min=1,\n                                               max=5.0,\n                                               step=0.5))\nI found another guide to write a resume with html, and then convert it to pdf with python, but I don’t really want that.\n#| column: body-outset\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\nimport matplotlib.pyplot as plt\n\n# Configuring the graph\n%matplotlib inline\nplt.figure(figsize=(10,6))\nplt.axis('off')\n\n\n\n# Test text\nTest = \"test\"\nplt.annotate(Test, (.02,.98), weight='regular', fontsize=8, alpha=.75)\n\n# Border lines\n#plt.axvline(x=.99, color='#000000', alpha=0.5, linewidth=300)\nbqplot instead:\nimport bqplot.pyplot as plt\nimport numpy as np\n\nfig = plt.figure(title=\"Sine\")\n\n# create data vectors\nx = np.linspace(-10, 10, 200)\ny = np.sin(x)\n\n# create line mark\nline = plt.plot(x, y)\n\n# renders the figure in the output cell (with toolbar for panzoom, save etc.)\nplt.show()\nThis apparently requires some libraries:\nplayground/resume/bqplot.js (404: Not Found)\n  /playground/resume/@jupyter-widgets/1551f4f60c37af51121f.woff2 (404: Not Found)\n  /playground/resume/@jupyter-widgets/eeccf4f66002c6f2ba24.woff (404: Not Found)\n  /playground/resume/@jupyter-widgets/be9ee23c0c6390141475.ttf (404: Not Found)\n  /playground/resume/@jupyter-widgets/html-manager/dist/1551f4f60c37af51121f.woff2 (404: Not Found)\n  /playground/resume/@jupyter-widgets/html-manager/dist/eeccf4f66002c6f2ba24.woff (404: Not Found)\n  /playground/resume/@jupyter-widgets/html-manager/dist/be9ee23c0c6390141475.ttf (404: Not Found)"
  },
  {
    "objectID": "projects/compiling-kasmvnc-on-nixos/index.html",
    "href": "projects/compiling-kasmvnc-on-nixos/index.html",
    "title": "Compiling KasmVNC on NixOS",
    "section": "",
    "text": "This is a living document, for the time being. Rather than a complete blogpost, this is a tracker of my progress, as well as a quick reference I can come back to.\n\nWhat is this?\nKasmweb is a software to create remote desktops, that exist within docker containers, and allow users to access them, all from a browser based GUI.\nKasmvnc is the VNC server part of kasm web, a custom fork of previous existing features, enhanced with more performance, and most importantly, a web native setup. Incompatible with existing VNC protocols, the only way to access Kasmvnc is through the http/https serive it offers, the web based VNC ui.\nKasmVNC is an amazing piece of software, but development for it is not truly, fully public. This is present in their build system. Their build system is a series of bash scripts, that call docker containers, which call more bash scripts, to finally compile the software, and package it, all in one.\nAs part of the scripts they have to compile kasm, lots of static linking happens. They do this because not all distros package the most updated, performant versions of the libraries that kasmvnc uses.\nBut Nixos, the distro I want to package KasmVNC for, does. In addition to that, it is completely incompatible with the hacked together build system that kasm uses. In order to package KasmVNC for Nixos, I must reverse engineer their build system, and bit by bit, port it to NixOS.\n\n\nNixos Build System\nHow the nixos build system works. I will do later, but here I will document, step by step, how nixos builds a package.\n\n\nKasmVNC’s Build System\nReverse engineering KasmVNC’s build system.\nThe below is what I neeed for the compiliation commands to work. I don’t know where Kasm runs these commands, or similar equivalents, this is just what I’ve figured out.\nPerhaps I need to only run make in the KasmVNC/unix directory?\n\ngit clone https://github.com/TigerVNC/tigervnc\n\ncd tigervnc\n\ngit clone https://github.com/kasmtech/KasmVNC\n\ncd KasmVNC\n\ncp ..vncserver .vncserver # this sets up build environment. I will still need to check if every single one of these things are necessary, but this works for now\nThe build script can be found here\nBut from this build script, I don’t think all of it is necessary. Below, I will extract what commands are actually needed, from all the fluff, cruft, and hacks.\n\ncmake -D CMAKE_BUILD_TYPE=RelWithDebInfo . -DBUILD_VIEWER:BOOL=OFF \\\n  -DENABLE_GNUTLS:BOOL=OFF\n\nmake\nBuilds end up in KasmVNC/unix/\nExcept the preliminary builds don’t work. They error:\n~/vscode/tigervnc/KasmVNC/unix master ?1 ❯ ./vncserver \n\nCan't locate List/MoreUtils.pm in @INC (you may need to install the List::MoreUtils module) (@INC contains: /usr/lib/perl5/5.36/site_perl /usr/share/perl5/site_perl /usr/lib/perl5/5.36/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5/5.36/core_perl /usr/share/perl5/core_perl) at ./vncserver line 38.\nBEGIN failed--compilation aborted at ./vncserver line 38.\nThe above is probably because a perl library is missing. After attempting to install the missing library using pacman -S perl-list-moreutils I get a different error.\n\n~/vscode/tigervnc/KasmVNC/unix master ?1 ❯ ./vncserver \n\nCan't locate KasmVNC/CliOption.pm in @INC (you may need to install the KasmVNC::CliOption module) (@INC contains: /usr/lib/perl5/5.36/site_perl /usr/share/perl5/site_perl /usr/lib/perl5/5.36/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5/5.36/core_perl /usr/share/perl5/core_perl) at ./vncserver line 42.\nBEGIN failed--compilation aborted at ./vncserver line 42.\nObviously, this won’t work. I must figure out why KasmVNC pacakges perl packages, where it puts them by default, and how to package them for Nix.\nAlright, the vncserver command appears to be in the git repo, and rather than being a binary, it is a perl wrapper script to start an xvnc server. from the script:\nuse KasmVNC::CliOption;\nuse KasmVNC::ConfigKey;\nuse KasmVNC::PatternValidator;\nuse KasmVNC::EnumValidator;\nuse KasmVNC::Config;\nuse KasmVNC::Users;\nuse KasmVNC::TextOption;\nuse KasmVNC::TextUI;\nuse KasmVNC::Utils;\nuse KasmVNC::Logger;\nThese perl modules/libraries can be found in KasmVNC/unix/KasmVNC\nSo that is what is necessary for the vncserver script to run. But is this script really necessary? Based on the names of the perl libraries, this script might not be adding any core functionalities to kasmvnc, only things like additional command line options, or loggers.\nI need to find where this script runs kasmvnc, and also where the actual kasmvnc binary is.\nI think their fork of xvnc is located in KasmVNC/unix/xserver/hw/vnc/xvnc.c.\nBut I get compilation errors:\n\n[nix-shell:~/vscode/tigervnc/KasmVNC]$ make \n[  3%] Built target os\n[ 13%] Built target rdr\n[ 30%] Built target network\n[ 31%] Built target Xregion\n[ 78%] Built target rfb\n[ 80%] Built target tx\n[ 81%] Built target unixcommon\n[ 81%] Linking CXX executable vncconfig\n/nix/store/178vvank67pg2ckr5ic5gmdkm3ri72f3-binutils-2.39/bin/ld: cannot find -lturbojpeg: No such file or directory\ncollect2: error: ld returned 1 exit status\nmake[2]: *** [unix/vncconfig/CMakeFiles/vncconfig.dir/build.make:157: unix/vncconfig/vncconfig] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:610: unix/vncconfig/CMakeFiles/vncconfig.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\nI don’t know why this happens. For those who don’t know, make pretty much calls a set of scripts, called Makefiles. I will need to find where in these scripts, the error commands are run.\nWeirdly, I can’t reproduce outside of the build script:\n\n~/vscode/tigervnc/KasmVNC master ?1 ❯ ld -lsdsakdfj\nld: cannot find -lsdsakdfj: No such file or directory\n~/vscode/tigervnc/KasmVNC master ?1 ❯ ld -lturbojpeg\nld: warning: cannot find entry symbol _start; not setting start address\n~/vscode/tigervnc/KasmVNC master ?2 ❯ ld -ljpeg     \nld: warning: cannot find entry symbol _start; not setting start address\n~/vscode/tigervnc/KasmVNC master ?2 ❯ ld -ljpeg-turbo\nld: cannot find -ljpeg-turbo: No such file or directory\n~/vscode/tigervnc/KasmVNC master ?1 ❯ \nI suspect the make scripts have some hacks that change working directory, or otherwise hide my installation of libjpeg.\nWhen commenting out the part of the KasmVNC/Cmakelists.txt file that appears to be related to libjpeg, the error when I run make changes.\n[ 95%] Building CXX object unix/vncconfig/CMakeFiles/vncconfig.dir/vncconfig.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/Surface.cxx:23:10: fatal error: FL/Fl_RGB_Image.H: No such file or directory\n   23 | #include &lt;FL/Fl_RGB_Image.H&gt;\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake[2]: *** [tests/CMakeFiles/fbperf.dir/build.make:104: tests/CMakeFiles/fbperf.dir/__/vncviewer/Surface.cxx.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/Surface_X11.cxx:26:10: fatal error: FL/Fl_RGB_Image.H: No such file or directory\n   26 | #include &lt;FL/Fl_RGB_Image.H&gt;\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake[2]: *** [tests/CMakeFiles/fbperf.dir/build.make:118: tests/CMakeFiles/fbperf.dir/__/vncviewer/Surface_X11.cxx.o] Error 1\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:31:10: fatal error: FL/Fl.H: No such file or directory\n   31 | #include &lt;FL/Fl.H&gt;\n      |          ^~~~~~~~~\nThis is probably missing libraries.\nAfter installing fltk (pacman -S fltk), this error goes away, and I get a new, even harder to comprehend error.\n\n[ 97%] Building CXX object tests/CMakeFiles/decperf.dir/decperf.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx: In constructor ‘PlatformPixelBuffer::PlatformPixelBuffer(int, int)’:\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:64:29: error: ‘uint8_t’ was not declared in this scope\n   64 |   setBuffer(width, height, (uint8_t*)xim-&gt;data,\n      |                             ^~~~~~~\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:38:1: note: ‘uint8_t’ is defined in header ‘&lt;cstdint&gt;’; did you forget to ‘#include &lt;cstdint&gt;’?\n   37 | #include \"PlatformPixelBuffer.h\"\n  +++ |+#include &lt;cstdint&gt;\n   38 | \n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:64:37: error: expected primary-expression before ‘)’ token\n   64 |   setBuffer(width, height, (uint8_t*)xim-&gt;data,\n      |                                     ^\nI suspect I have the wrong version of the tigervnc source code/libraries. I will need to investigate where Kasm’s build system downloads these vncviewer libraries from.\nAfter editing the errored file, to include the library:\nWithin KasmVNC/vncviewer/PlatformPixelBuffer.cxx:\n\n#include &lt;cstdint&gt;\n\nI get a different error.\n\n[ 94%] Building CXX object tests/CMakeFiles/fbperf.dir/__/vncviewer/PlatformPixelBuffer.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx: In constructor ‘PlatformPixelBuffer::PlatformPixelBuffer(int, int)’:\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:66:3: error: ‘setBuffer’ was not declared in this scope; did you mean ‘setbuffer’?\n   66 |   setBuffer(width, height, (uint8_t*)xim-&gt;d﻿tion. \nI think it’s likely that I have the wrong version of tigervnc or something. I will try to see how Kasm’s build scripts set this up.\nOkay, I will attempt to create a visual graph, documenting each step of KasmVNC’s build system, to build an ubuntu package. I will have hyperlinks to each of the scripts/dockerfiles, or other pieces of the github repo. Currently still working on figuring out how to use Graphviz.\n\nEntrybuild-tarballBuildWWWdockerfile.ubuntu.build\n\n\n\n\n\n\n\n   \n\nEntry\n\n  Entry.  Starts out at KasmVNC/builder     \n\nBuildPackage\n\n  I will run the ‘builder/build-package ubuntu bionic’ command.   This isn’t the only command available, but for an example.     \n\nEntry-&gt;BuildPackage\n\n    \n\nBuildTarball\n\n  build-tarball ubuntu bionic     \n\nBuildPackage-&gt;BuildTarball\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildTarball\n\n  \n\nwww\n\n  if some condition, then   build and run dockerfile.www.build     \n\ndockerbuild\n\n  Build and run the appropiate Dockerfile,   which in this case dockerfile.ubuntu_bionic.build     \n\nwww-&gt;dockerbuild\n\n    \n\nincomplete\n\n ???, currrently in progress.   \n\nBuildTarball\n\n  build-tarball     \n\nBuildTarball-&gt;www\n\n    \n\ndockerbuild-&gt;incomplete\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildWWW\n\n  \n\nbuildwwwsh\n\n  ENTRYPOINT [ “/src/build_www.sh” ]     \n\nCOPY kasmweb/ /src/www/\n\n COPY kasmweb/ /src/www/   \n\nCOPY builder/build_www.sh /src/\n\n COPY builder/build_www.sh /src/   \n\nCOPY kasmweb/ /src/www/-&gt;COPY builder/build_www.sh /src/\n\n    \n\nWORKDIR /src/www\n\n WORKDIR /src/www   \n\nCOPY builder/build_www.sh /src/-&gt;WORKDIR /src/www\n\n    \n\nRUN npm install\n\n RUN npm install   \n\nWORKDIR /src/www-&gt;RUN npm install\n\n    \n\nRUN npm install-&gt;buildwwwsh\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildonUbuntu\n\n  \n\nEntry\n\n Entry here. This entire phase happens in a docker container   \n\ndevpendencies\n\n Install some dependencies. Notably, tightvncserver   \n\nEntry-&gt;devpendencies\n\n    \n\nMakeinstalls\n\n  build and install webp and libjpeg turbo     \n\ndevpendencies-&gt;Makeinstalls\n\n    \n\nInstall some more libs\n\n Install some more libs   \n\nMakeinstalls-&gt;Install some more libs\n\n    \n\nbuildsh\n\n  build.sh  This is the command the built docker container runs     \n\nInstall some more libs-&gt;buildsh\n\n   \n\n\n\n\n\n\n\n\n\n\n\nNote about dependencies install\n\n\n\n\n\nFor some reason they use multiple build phases for the same step of installing packages. In addition to that, they don’t clean the apt cache between build stages.\n\n\n\n\n\n\n\n\n\nNote about webp and libjpeg-turbo\n\n\n\n\n\nThey make and install — in the build dockerfile. I will end up skipping this step, as nix packages these, but why do they do that?\n\n\n\n\n\n\nI will expand on this, and organize it further. But based on these beginnings, kasmvnc appears to be a perl script that either starts a sepreate webserver or serves a webserver (if there is a perl native way to do this?), which appears to be based on novnc, while it also starts the vnc server, which is based on tigervnc.\nHowever, I am still confused about one thing: Where does it download the vnc server source code.\n\n\nThe easy way\nAfter I packaged quarto, I realized that I can actually package the kasmvnc binary using nix. I have decided to do this for now.\nHere is first attempt for this. Now, kasmvnc’s packaging system is weird, in that they do not offer a binary tarball for their packages. So, I have decided to convert their alpine package into something I can use, because based on a cursory look into all the packages, it seems to be the easiest to package for Nix/Nixos.\n\n\nFirst try!\n\n{\n    stdenv,\n    lib,\n    fetchurl,\n    makeWrapper,\n} :\n\nstdenv.mkDerivation rec {\n  pname = \"kasmvnc\";\n  version = \"1.1.0\";\n  src = fetchurl {\n    url = \"https://github.com/kasmtech/KasmVNC/releases/download/v${version}/kasmvnc.alpine_317_x86_64.tgz\";\n    sha256 = \"sha256-j/3PUwBd8XygBKYfFdAwN15cwxDPf3vbEwbLy1laxSU=\";\n  };\n\n  nativeBuildInputs = [\n  ];\n\n  patches = [\n  ];\n\n  postPatch = ''\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n  '';\n\n  installPhase = ''\n      runHook preInstall\n\n      mkdir -p $out/bin $out/share $out/man $out/etc $out/lib\n\n      echo here\n      ls\n      ls local/bin\n\n      mv local/etc/* $out/etc\n      mv local/share/* $out/share\n      mv local/man/* $out/man\n      mv local/lib/* $out/lib\n      mv local/bin/* $out/bin\n\n      runHook preInstall\n  '';\n\n  meta = with lib; {\n    description = \"Kasmvnc\";\n    longDescription = ''\n        Long description here\n    '';\n    homepage = \"\";\n    changelog = \"https://github.com/kasmtech/KasmVNC/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ moonpiedumplings ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nThis is my first attempt at a derivation. I’ve converted this one from the quarto derivation I built, which is also on my blog.\nI’m installing it using a simple shell.nix, that uses the nix callPackage function to build the kasmvnc nix package, and make it available in my current shell.\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n    kasmvnc = pkgs.callPackage ./kasmvnctest.nix {};\nin\n    pkgs.mkShell {\n        packages = [ kasmvnc ];\n    }\nTo run kasmvnc, I run the vncserver command.\nHowever, I get an error."
  },
  {
    "objectID": "projects/nixos-vps/index.html",
    "href": "projects/nixos-vps/index.html",
    "title": "Automating my server config, first nix, then ansible",
    "section": "",
    "text": "I want to automate my install with nixos"
  },
  {
    "objectID": "projects/nixos-vps/index.html#deployment-attempts",
    "href": "projects/nixos-vps/index.html#deployment-attempts",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Deployment attempts",
    "text": "Deployment attempts\nHowever, I now want to deploy this. I am currently using webdock.io as my VPS provider, which doesn’t officially support nixos. So I want to convert an existing ubuntu install in place to a nixos installation, without physical access to the machine.\nI have tried multiple tools and they have not worked, but I am now trying nixos-anywhere for this.\nHere is my config as of 7/14/2023\n\n\nShow .nix file\n\n\n\nflake.nix\n\n\n\nflake.nix\n\n{\n  inputs.nixpkgs.url = github:NixOS/nixpkgs;\n  inputs.disko.url = github:nix-community/disko;\n  inputs.disko.inputs.nixpkgs.follows = \"nixpkgs\";\n\n  outputs = { self, nixpkgs, disko, ... }@attrs: {\n    nixosConfigurations.hetzner-cloud = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n      specialArgs = attrs;\n      modules = [\n        ({modulesPath, ... }: {\n          imports = [\n            (modulesPath + \"/installer/scan/not-detected.nix\")\n            #(modulesPath + \"/profiles/qemu-guest.nix\") #not a qemu vm\n            # try to fit the lxd-vm config in here\n            #https://github.com/mrkuz/nixos/blob/c468d9772b7a84ab8e38cc4047bc83a3a443d18f/modules/nixos/virtualization/lxd-vm.nix#L4\n            disko.nixosModules.disko\n          ];\n          disko.devices = import ./disk-config.nix {\n            lib = nixpkgs.lib;\n          };\n          boot.loader.efi.efiSysMountPoint = \"/boot/efi\";\n          boot.loader.grub = {\n            devices = [ \"nodev\" ];\n            efiSupport = true;\n            #efiInstallAsRemovable = true;\n          };\n          networking = {\n            usePredictableInterfaceNames = false;\n            interfaces = {\n              eth0 = {\n                useDHCP = false;\n                ipv4.addresses = [{ address = \"93.92.112.130\"; prefixLength = 24; }];\n              };\n            };\n            defaultGateway = {\n                interface = \"eth0\";\n                address = \"93.92.112.1\";\n            };\n          };\n          services = {\n            openssh = {\n                enable = true;\n                settings = {\n                    PasswordAuthentication = false;\n                    #PermitRootLogin = \"prohibit-password\"; # this is the default\n                };\n                openFirewall = true;\n            };\n            cockpit = {\n                enable = true;\n                openFirewall = true;\n            };\n          };\n\n          users.users.moonpie = {\n            isNormalUser = true;\n            extraGroups = [ \"wheel\" ]; # Enable ‘sudo’ for the user.\n            #packages = with pkgs; [];\n            initialHashedPassword = \"$y$j9T$ZGDLrUl6VP4AiusK96/tx0$1Xb1z61RhXYR8lDlFmJkdS8zyzTnaHL6ArNQBBrEAm0\"; # may replace this with a proper secret management scheme later\n            openssh.authorizedKeys.keys = [ \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDEQDNqf12xeaYzyBFrOM2R99ciY9i0fPMdb4J64XpU3Tjv7Z5WrYx+LSLCVolzKktTfaSgaIDN8+vswixtjaAXkGW2glvTD0IwaO0N4IQloov3cLZa84I7lkj5jIkZiXJ2zHJZ8bQmayUrSj2yBwYJ61QLs+R0hpEZHfRarBU9vphykACdM6wxSj0YVkFnGlKBxZOZipW6OoKjEkFOHOSH6DYrX3V/TqALYC62iH6jEiLIycxey1vfwkywfsP9V9GlGYHutdwgAgeaN3xUnL8+X6vkQ8cbC2jEuVopodsAAArFYZLVdfAcNc17WYq5y+FX3schGpTo89SZ4Uh9gd4b45h9Hq7h6p7hBF8UCkyqSKnFiPjDJxv5yuY+rYeZ9aJSeCJUYrb1xyOreWnJkhDuYff/1NCewWL8sfuD9IC9BXWBwhxoA/OUfV9KvDBZmYoThlh86ZCQ+uqCR1DIKa1YhPMlT6gzUY01yoMj+B93RpUBUW5LqLDVCL7Qujh/0ns= moonpie@cachyos-x8664\" ];\n          };\n          users.users.root.openssh.authorizedKeys.keys = [\n            # change this to your ssh key\n            \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCg+VqYIph1uiO243+jqdKkSOiw4hAyNWwVeIgzu7+KStY2Dji/Dzn1gu5LGj71jj/dW2Q8FpAC4WXWX5alqUJpK58HwN/d86BpnNPAxjDBOtYN2Znf3Y4108OKhEyaaKpiPSpADW5b/FW+Ki+ftMxRs9cUWuysTxoYDeX9BkTtpar5ChYaoMEkD//tUbqLx9wVFIQ4YdbVajgupUW3S2LRqCAgGBf0eTMYoZbNJHjSNlje7m9UJQOqvXJtiGdoYAqYQdHZfkCLKC1qBw6bl0ZHVkETTKr6tC89ZaZlKfZfGZqgCvyW0VzwYHwRmcOBndZgdOkEHQS/VIYmp91v1G58KMfuSBEKyUJoRVjo6lvbPHIsrGC1vNKLRiRYKGfo1lJ/qFIiq5NNfvmoYZMy+4A6jMohesTdA4yP7nwyz1o9jWmDIHeGJxZCfdYJyQ/IslesR3ACjUYAporCIk3U71f1qB7QOJAErF7+3Q6ZdOHNPPu7sURf2zMn/Q6mWktTxxU= moonpie@localhost.localdomain\"\n          ];\n        })\n      ];\n    };\n  };\n}\n\n\n\n\ndisk-config.nix\n\n\n\ndisk-config.nix\n\n{ disks ? [ \"/dev/sda\" ], ... }: {\n    disk = {\n      sda = {\n        device = builtins.elemAt disks 0;\n        type = \"disk\";\n        content = {\n          type = \"gpt\";\n          partitions = {\n            ESP = {\n              type = \"EF00\";\n              size = \"100M\";\n              content = {\n                type = \"filesystem\";\n                format = \"vfat\";\n                mountpoint = \"/boot/efi\";\n              };\n            };\n            root = {\n              size = \"100%\";\n              content = {\n                type = \"filesystem\";\n                format = \"ext4\";\n                mountpoint = \"/\";\n              };\n            };\n          };\n        };\n      };\n    };\n}\n\n\n\nIn the direcotry where this is, I run the nixos-anywhere command:\nnix run github:numtide/nixos-anywhere -- --flake .#hetzner-cloud moonpie@ip -i nixos-vps\nMy ssh identity file is named nixos-vps.\nBut this config doesn’t work. Although my effort to set up the grub bootloader seemed to have prevailed, and the terminal output said it had succeeded, I cannot access the device. I think it is an issue with network connection.\nFor those who may be attempting to help me, or look at my work, here is a copy of the files I am working with, updated live on every push to the github repo for this blog.\n\n\nShow files\n\n\n\nflake.nix\n\n{\n  inputs.nixpkgs.url = github:NixOS/nixpkgs;\n  inputs.disko.url = github:nix-community/disko;\n  inputs.disko.inputs.nixpkgs.follows = \"nixpkgs\";\n\n  outputs = { self, nixpkgs, disko, ... }@attrs: {\n    nixosConfigurations.hetzner-cloud = nixpkgs.lib.nixosSystem {\n      system = \"x86_64-linux\";\n      specialArgs = attrs;\n      modules = [\n        ({modulesPath, ... }: {\n          imports = [\n            (modulesPath + \"/installer/scan/not-detected.nix\")\n            #(modulesPath + \"/profiles/qemu-guest.nix\") #not a qemu vm\n            # try to fit the lxd-vm config in here\n            #https://github.com/mrkuz/nixos/blob/c468d9772b7a84ab8e38cc4047bc83a3a443d18f/modules/nixos/virtualization/lxd-vm.nix#L4\n            disko.nixosModules.disko\n          ];\n          disko.devices = import ./disk-config.nix {\n            lib = nixpkgs.lib;\n          };\n          boot.loader.efi.efiSysMountPoint = \"/boot/efi\";\n          boot.loader.grub = {\n            devices = [ \"nodev\" ];\n            efiSupport = true;\n            #efiInstallAsRemovable = true;\n          };\n          networking = {\n            usePredictableInterfaceNames = false;\n            interfaces = {\n              eth0 = {\n                useDHCP = false;\n                ipv4.addresses = [{ address = \"93.92.112.130\"; prefixLength = 24; }];\n              };\n            };\n            defaultGateway = {\n                interface = \"eth0\";\n                address = \"93.92.112.1\";\n            };\n          };\n          services = {\n            openssh = {\n                enable = true;\n                settings = {\n                    PasswordAuthentication = false;\n                    #PermitRootLogin = \"prohibit-password\"; # this is the default\n                };\n                openFirewall = true;\n            };\n            cockpit = {\n                enable = true;\n                openFirewall = true;\n            };\n          };\n\n          users.users.moonpie = {\n            isNormalUser = true;\n            extraGroups = [ \"wheel\" ]; # Enable ‘sudo’ for the user.\n            #packages = with pkgs; [];\n            initialHashedPassword = \"$y$j9T$ZGDLrUl6VP4AiusK96/tx0$1Xb1z61RhXYR8lDlFmJkdS8zyzTnaHL6ArNQBBrEAm0\"; # may replace this with a proper secret management scheme later\n            openssh.authorizedKeys.keys = [ \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDEQDNqf12xeaYzyBFrOM2R99ciY9i0fPMdb4J64XpU3Tjv7Z5WrYx+LSLCVolzKktTfaSgaIDN8+vswixtjaAXkGW2glvTD0IwaO0N4IQloov3cLZa84I7lkj5jIkZiXJ2zHJZ8bQmayUrSj2yBwYJ61QLs+R0hpEZHfRarBU9vphykACdM6wxSj0YVkFnGlKBxZOZipW6OoKjEkFOHOSH6DYrX3V/TqALYC62iH6jEiLIycxey1vfwkywfsP9V9GlGYHutdwgAgeaN3xUnL8+X6vkQ8cbC2jEuVopodsAAArFYZLVdfAcNc17WYq5y+FX3schGpTo89SZ4Uh9gd4b45h9Hq7h6p7hBF8UCkyqSKnFiPjDJxv5yuY+rYeZ9aJSeCJUYrb1xyOreWnJkhDuYff/1NCewWL8sfuD9IC9BXWBwhxoA/OUfV9KvDBZmYoThlh86ZCQ+uqCR1DIKa1YhPMlT6gzUY01yoMj+B93RpUBUW5LqLDVCL7Qujh/0ns= moonpie@cachyos-x8664\" ];\n          };\n          users.users.root.openssh.authorizedKeys.keys = [\n            # change this to your ssh key\n            \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQCg+VqYIph1uiO243+jqdKkSOiw4hAyNWwVeIgzu7+KStY2Dji/Dzn1gu5LGj71jj/dW2Q8FpAC4WXWX5alqUJpK58HwN/d86BpnNPAxjDBOtYN2Znf3Y4108OKhEyaaKpiPSpADW5b/FW+Ki+ftMxRs9cUWuysTxoYDeX9BkTtpar5ChYaoMEkD//tUbqLx9wVFIQ4YdbVajgupUW3S2LRqCAgGBf0eTMYoZbNJHjSNlje7m9UJQOqvXJtiGdoYAqYQdHZfkCLKC1qBw6bl0ZHVkETTKr6tC89ZaZlKfZfGZqgCvyW0VzwYHwRmcOBndZgdOkEHQS/VIYmp91v1G58KMfuSBEKyUJoRVjo6lvbPHIsrGC1vNKLRiRYKGfo1lJ/qFIiq5NNfvmoYZMy+4A6jMohesTdA4yP7nwyz1o9jWmDIHeGJxZCfdYJyQ/IslesR3ACjUYAporCIk3U71f1qB7QOJAErF7+3Q6ZdOHNPPu7sURf2zMn/Q6mWktTxxU= moonpie@localhost.localdomain\"\n          ];\n        })\n      ];\n    };\n  };\n}\n\n\n\ndisk-config.nix\n\n{ disks ? [ \"/dev/sda\" ], ... }: {\n    disk = {\n      sda = {\n        device = builtins.elemAt disks 0;\n        type = \"disk\";\n        content = {\n          type = \"gpt\";\n          partitions = {\n            ESP = {\n              type = \"EF00\";\n              size = \"100M\";\n              content = {\n                type = \"filesystem\";\n                format = \"vfat\";\n                mountpoint = \"/boot/efi\";\n              };\n            };\n            root = {\n              size = \"100%\";\n              content = {\n                type = \"filesystem\";\n                format = \"ext4\";\n                mountpoint = \"/\";\n              };\n            };\n          };\n        };\n      };\n    };\n}"
  },
  {
    "objectID": "projects/nixos-vps/index.html#scaffolding-and-beginnings",
    "href": "projects/nixos-vps/index.html#scaffolding-and-beginnings",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Scaffolding and Beginnings",
    "text": "Scaffolding and Beginnings\nAn example docker-compose, from the official ansible documentation:\ntasks:\n    - name: Remove flask project\n      community.docker.docker_compose:\n        project_src: flask\n        state: absent\n\n    - name: Start flask project with inline definition\n      community.docker.docker_compose:\n        pull: true # Not default. Will pull images upon every rerun.\n        \n        # rebuilds if a change to the dockerfile contents are detected. If a rebuild, then will attempt to pull a newer version fo the image, but not otherwise\n        build: always \n\n        state: present # default, but to be noted. \n\n        #Docker compose goes here. But can I have multiple projects?\n        project_name: flask\n        definition:\n            db:\n              image: postgres\n            web:\n              build: \"{{ playbook_dir }}/flask\"\n              command: \"python manage.py runserver 0.0.0.0:8000\"\n              volumes:\n                - \"{{ playbook_dir }}/flask:/code\"\n              ports:\n                - \"8000:8000\"\n              links:\n                - db\nAnsible also offers an image for managing direct parts of docker, like images.\n- name: Pull an image\n  community.docker.docker_image:\n    name: pacur/centos-7\n    source: pull\n    # Select platform for pulling. If not specified, will pull whatever docker prefers.\n    pull:\n      platform: amd64\n- name: Build image and with build args\n  community.docker.docker_image:\n    name: myimage\n    build:\n      path: /path/to/build/dir\n      args: # key value args \n        log_volume: /var/log/myapp\n        listen_port: 8080\n        file: Dockerfile_name\n    source: build\nAnsible also seems to support managing git repos, which I can use to automate. I’ve decided to write an example using the features that I would utilize.\n- name: Git checkout\n  ansible.builtin.git:\n    repo: 'https://foosball.example.com'\n    dest: /home/moonpie\n    version: release-0.22 # can be branch, tag, or sha1 hashshum of the repo.\nThis will enable me to write docker-compose’s, Dockerfiles, or other things and put them in a git repo, which I can then clone and use later.\nIn addition to that, I also need a way to keep the system updated, for security purposes. Because I am using ubuntu, I will use the ansible apt module.\n- name: update the system to latest distro packages\n  ansible.builtin.apt: \n    update-cache: yes # equivlaent of apt-get update\n    upgrade: safe # conservative, safe upprade.full/dist upgrades all packages to latest, but I will have to research the difference between the two. \n    autoclean: yes # cleans not installed packages from the cache\n    autoremove: yes # delete uneeded dependencies\n    clean: yes # deletes all packages from the cache\n\n  \n- name: update the distribution itself? Still working on this one\n  ansible.builtin.apt:\n  # Hmmm, the format for apt repos is currently changing.\n\n- name: manage apt_repo #since apt key is deprecated, this an an alternative around it.\n  block:\n    - name: somerepo |no apt key\n      ansible.builtin.get_url:\n        url: https://download.example.com/linux/ubuntu/gpg\n        dest: /etc/apt/keyrings/somerepo.asc\n\n    - name: somerepo | apt source\n      ansible.builtin.apt_repository:\n        repo: \"deb [arch=amd64 signed-by=/etc/apt/keyrings/myrepo.asc] https://download.example.com/linux/ubuntu {{ ansible_distribution_release }} stable\"\n        state: present\n\n- name: The newer deb822 format is better # however, this isn't used by my ubuntu install. \n  deb822_repository:\n    name: example\n    types: deb\n    uris: https://download.example.com/linux/ubuntu\n    suites: '{{ ansible_distribution_release }}'\n    components: stable\n    architectures: amd64\n    signed_by: https://download.example.com/linux/ubuntu/gpg\n\n\n\n\n- name: Reboot the system\n  reboot: # However, when I tested this for my current project at my internship, it didn't work. The ssh did not reconnect."
  },
  {
    "objectID": "projects/nixos-vps/index.html#ansible-inventory",
    "href": "projects/nixos-vps/index.html#ansible-inventory",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Ansible Inventory",
    "text": "Ansible Inventory\nAnsible has multiple ways to configure ssh keys. One way is to explicitly specify a ssh private key file in your inventory file:\nall:\n  hosts:\n    your_remote_host1:\n      ansible_user: your_username1\n      ansible_password: your_password1\n      ansible_ssh_private_key_file: /path/to/your_private_key1\n    your_remote_host2:\n      ansible_user: your_username2\n      # and so on\n\n  vars:\n    ansible_ssh_common_args: '-o StrictHostKeyChecking=no' # Disables auto reject of unkown hosts.\nAnother way is to specify the private key in your ~/.config/ssh/config file.\nHost your_remote_host1\n  HostName your_remote_host1.example.com\n  User your_username\n  IdentityFile /path/to/your_private_key\n\nHost your_remote_host2\n  HostName your_remote_host2.example.com\n  User your_username\n  IdentityFile /path/to/another_private_key\nYou can also specify the private key on the command line, with the –private-key=/path/to option.\nI am searching for the most CI/CD friendly way to do this. Tbh, it may be lazy, but for a single machine, I may simply ssh into the machine, git clone the repo, and use ansible’s local mode, which runs an ansible playbook on the local machine.\nHere is my current ansible inventory. Because I am only configuring one host, it is extremely simple.\n\n\ninventory.yml\n\n---\nall:\n  hosts:\n   office:\n\nAnd here is my ssh config file, censored of course.\n\n\n~/.ssh/config\n\nHost office\n        HostName {server ip or hostname goes here}\n        port 22\n        user moonpie\n        IdentityFile /home/moonpie/.ssh/office-vps\n\nNow, I can test if my hosts are up with a simple ansible command.\n[nix-shell:~/vscode/ansible-vps-config]$ ansible all -i inventory.yml -m ping\noffice | SUCCESS =&gt; {\n    \"ansible_facts\": {\n        \"discovered_interpreter_python\": \"/usr/bin/python3\"\n    },\n    \"changed\": false,\n    \"ping\": \"pong\"\n}\nThe “ping: pong” part means that it discoverd my server, and it is ready for me to configure it if I so wish."
  },
  {
    "objectID": "projects/nixos-vps/index.html#dry-runs-and-testing",
    "href": "projects/nixos-vps/index.html#dry-runs-and-testing",
    "title": "Automating my server config, first nix, then ansible",
    "section": "Dry runs and Testing",
    "text": "Dry runs and Testing\nNow, after some tinkering with syntax, I’ve gotten ansible to do a dry run without complaining:\n[nix-shell:~/vscode/ansible-vps-config]$ ansible-playbook --check -i inventory.yml main.yml --ask-become-pass\nBECOME password: \n\nPLAY [all] *********************************************************************************************************\n\nTASK [Gathering Facts] *********************************************************************************************\nok: [office]\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/meshcentral.yml for office\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nok: [office]\n\nTASK [docker-compose : npm] ****************************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/npm.yml for office\n\nTASK [docker-compose : npm] ****************************************************************************************\nchanged: [office]\n\nTASK [sys-maintain : update the system to latest distro packages] **************************************************\nchanged: [office]\n\nPLAY RECAP *********************************************************************************************************\noffice                     : ok=6    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \nThe --ask-become-pass asks me for the sudo password on my server, in case my user doesn’t have permission to do passwordless sudo.\nSince this is a dry run, it simply tells me what will be changed, but doesn’t actually change it. For some reason, it didn’t update one of the docker containers although it did update the other. I discoverd this was because I was missing something in my compose file.\n---\n- name: npm\n  community.docker.docker_compose:\n    pull: true\n    build: true \n    project_name: npm\n    definition:\n      version: '3'\n      services:\n        app:\n          image: 'jc21/nginx-proxy-manager'\n          restart: unless-stopped\n          ports:\n            - '80:80'\n            - '81:81'\n            - '443:443'\n          # - '53:53'\n          volumes: \n            - /home/{{ ansible_user_id }}/npm/data:/data\n            - /home/{{ ansible_user_id }}/npm/letsencrypt:/etc/letsencrypt\n      networks:\n        default:\n          external: true\n          name: mine\nFor the image portion, I needed image: 'jc21/nginx-proxy-manager:latest'. this will ensure that every time I rerun the ansible playbook, docker will attempt to update the container versions.\nNow, I can run just the docker-compose part of my using a the tags feature.\n[nix-shell:~/vscode/ansible-vps-config]$ ansible-playbook --check -i inventory.yml main.yml --ask-become-pass --tags docker-compose\nBECOME password: \n\nPLAY [all] *********************************************************************************************************\n\nTASK [Gathering Facts] *********************************************************************************************\nok: [office]\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/meshcentral.yml for office\n\nTASK [docker-compose : Meshcentral] ********************************************************************************\nchanged: [office]\n\nTASK [docker-compose : npm] ****************************************************************************************\nincluded: /home/moonpie/vscode/ansible-vps-config/roles/docker-compose/tasks/compose-files/npm.yml for office\n\nTASK [docker-compose : npm] ****************************************************************************************\nchanged: [office]\n\nPLAY RECAP *********************************************************************************************************\noffice                     : ok=5    changed=2    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0"
  },
  {
    "objectID": "projects/quarto-via-nix/index.html",
    "href": "projects/quarto-via-nix/index.html",
    "title": "Packaging quarto using nix",
    "section": "",
    "text": "Skip to the conclusion if you want to know how to use the efforts of my hard work, although it only works on x6_64 linux.\n\nWhat is quarto?\nTo explain quarto, I first have to explain jupyter notebooks, and quarto’s predecessor, fastpages.\nFastpages is a blogging platform built on jekyll, a static site generator. Static sites are websites that do not connect to a backend server, the user’s browser does all the rendering. Static site generators do all the hard work of creating these, by converting a very human readable format, like markdown, to pretty looking html, which is what browsers render.\nFastpages adds onto the features of jekyll, by adding support for jupyter notebooks. Jupyter is a technology that allows users to combine code, multimedia, and text into a single document, for any purposes that one might use it. It’s usually popular for data science, as code is used to generate diagrams, but I really like it for testing snippets of code, as you can have multiple pieces of code in one document, and then run and debug them independently of eachother. Fastpagess can convert all of this, even automatically running code and creating any interactive elements, and then putting it all up on the internet as a static site.\nFastpages is also deprecated. On the github page, which is archived, it recommends you switch to quarto.\nQuarto adds more features on top of fastpages, while also removing some features. Fastpages is primarily designed for blogging, whereas quarto also has support for generating books, pdf’s, and websites.\nHowever, quarto lacks some features, as it uses it’s own static site generator, rather than jekyll. The biggest and most noticable one, is the Liquid template language that my computer science teacher uses to dynamically render his schedule page.\nBut for my purposes, quarto works fine. This blogpost you are reading, was generated using quarto.\n\n\nWhat is nix?\nNix is multiple things. Nix is a linux distribution, an package repository, a package manager, a programming language, and a configuration as code system.\nRight now, I am trying to use it as a package manager — specifically, to give myself the quarto tool.\nI’ve selected nix because it focuses on reproducible builds, across Mac, and Linux and x86_64, and arm64. This enables a multitude of devices to get packages with an identical configuration to me.\n\n\nUsing Nix\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno ];\n    }\nThis is a sample shell.nix file. If you run the nix-shell command line tool while in the same working directory, or using the filename as an argument, it will use this bit of nix code to create an shell environment for you.\nNix is a functional programming langauge. Unlike a language like python or java, where everything is an object, in nix, everything is a function. The colon : declares the arguments for a function. The ? declares a default argument for the variable, pkgs, to be used in the function. This is important, because without this declarion, the program does now know where to get packages from.\nThe above shell.nix works great. However, it installs an older version of quarto, 1.2, as only an older version of quarto is packaged in the nixpkgs repository. I want the newest version, 1.3.\nHere is the code used to create the quarto package, called a derivation:\n\n\nShow derivation\n\n\n{ stdenv\n, lib\n, pandoc\n, esbuild\n, deno\n, fetchurl\n, nodePackages\n, rWrapper\n, rPackages\n, extraRPackages ? []\n, makeWrapper\n, python3\n, extraPythonPackages ? ps: with ps; []\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"quarto\";\n  version = \"1.2.475\";\n  src = fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-oyKjDlTKt2fIzirOqgNRrpuM7buNCG5mmgIztPa28rY=\";\n  };\n\n  nativeBuildInputs = [\n    makeWrapper\n  ];\n\n  patches = [\n    ./fix-deno-path.patch\n  ];\n\n  postPatch = ''\n    # Compat for Deno &gt;=1.26\n    substituteInPlace bin/quarto.js \\\n      --replace 'Deno.setRaw(stdin.rid, ' 'Deno.stdin.setRaw(' \\\n      --replace 'Deno.setRaw(Deno.stdin.rid, ' 'Deno.stdin.setRaw('\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n    wrapProgram $out/bin/quarto \\\n      --prefix PATH : ${lib.makeBinPath [ deno ]} \\\n      --prefix QUARTO_PANDOC : ${pandoc}/bin/pandoc \\\n      --prefix QUARTO_ESBUILD : ${esbuild}/bin/esbuild \\\n      --prefix QUARTO_DART_SASS : ${nodePackages.sass}/bin/sass \\\n      --prefix QUARTO_R : ${rWrapper.override { packages = [ rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n      --prefix QUARTO_PYTHON : ${python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n  '';\n\n  installPhase = ''\n      runHook preInstall\n\n      mkdir -p $out/bin $out/share\n\n      rm -r bin/tools\n\n      mv bin/* $out/bin\n      mv share/* $out/share\n\n      runHook preInstall\n  '';\n\n  meta = with lib; {\n    description = \"Open-source scientific and technical publishing system built on Pandoc\";\n    longDescription = ''\n        Quarto is an open-source scientific and technical publishing system built on Pandoc.\n        Quarto documents are authored using markdown, an easy to write plain text format.\n    '';\n    homepage = \"https://quarto.org/\";\n    changelog = \"https://github.com/quarto-dev/quarto-cli/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ mrtarantoga ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nI don’t want to bore you with details, but in short, it downloads an older version of quarto than the newest.\n\n\nPackaging quarto\nNix is very poorly documented. The recommended way of getting help with nix is to ask for help on the discord. So that is what I did. The first thing I asked was how to get a newer version of quarto:\nI started out by asking how to update the version of the quarto package. I started out to do so on my own, by cloning the nixpkgs github repo, and attempting to build nixpkgs, but I couldn’t figure out how to build it at first, which is when I asked.\n\nWhich didn’t work, because I did not want to download the whole nixpkgs. I wanted to store the nix derivation to build. The answer: use an ovveride:\nI was then told to use the overrideAttrs function, which overrides specific attributes, essentially variables, of the derivation, another type of function, used to build the program.\nMy first attempt was not too good.\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\nA simple shell.nix that replaced the version attribute of quarto. I shared this excitedly, thinking I had figured this out on my own, only to be told that this change was purely cosmetic, and the new version of quarto wasn’t actually installed. And they were right.\n[nix-shell:~/vscode/quartotest]$ which quarto\n/nix/store/9qy0kpll3r755c6i0717405dilhffdrd-quarto-1.3.361/bin/quarto\nIt looks right, until you check deeper:\n[nix-shell:~/vscode/quartotest]$ quarto --version\n1.2.475\nSo a deeper override was needed. I needed to override the src attribute, which determines where to download the files used to package the application.\n\n\nSecond attempt!\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n            src = fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-VEYUEI4xzQPXlyTbCThAW2npBCZNPDJ5x2cWnkNz7RE=\";\n  };\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nBut this errored as well.\n~/vscode/quartotest master !4 ?1 ❯ nix-shell                                                                                                                         2m 56s\nerror: undefined variable 'fetchurl'\n\n       at /home/moonpie/vscode/quartotest/shell.nix:5:19:\n\n            4|             version = \"1.3.361\";\n            5|             src = fetchurl {\n             |                   ^\n            6|     url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-lin\nI was confused? Why did this error? I had copied exactly what was in the derivation used to build the package?\nLater, I figured out why. When a package is built, the dependencies are declared in the beginning of the package:\n{ \n  Dependencies_Here\n} : stdenv.mkDerivation.restofpackage\nIn nix, every single thing is a function. When creation a function in nix, the curly brackets before the function declare the arguments that the function will take:\nfunction = {arg1, arg2} : functionhere\nBut when creating a package, this syntax plays another role. The arguments of the function act as a dependency list, by declaring what packages are necessary to build the derivation. This prevents the build from being tainted by anything that is not explicitly declared. However, because overrides are not the same as derivations, they act differently.\nBut to get around this error when using the override function:\n\n\nSuccess!\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n            src = pkgs.fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n  };\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nAnd it worked:\n[nix-shell:/tmp/test]$ quarto --version\n1.3.361\nExcept it didn’t. When I actually tried to render my project:\n[nix-shell:~/vscode/quartotest]$ quarto render\n[1/4] about.qmd\nCould not find data file templates/styles.citations.html\nSince this file couldn’t be found on my system, I tried to find it on the internet.\nAnd find it I did, in the data-files section of the information about the pandoc 3.1 package\ndata-files:\n                 -- templates\n                 data/templates/styles.html\n                 data/templates/styles.citations.html\nFirst, I checked what version of the pandoc that Nix had in their repositories. They only had 2.1.9, which was too old for the version of quarto I had.\nBut just in case, I asked on the github discussions page for quarto. And yes, quarto 1.3, the version I wanted, did require pandoc 3.0, which nix did not have packaged.\nExcept it did, although the package wasn’t in the dependencies. So I first tried to install it independently, using the nix-shell -p package tool\n\n\nExcept I got an error\n\n~/vscode/test ❯ nix-shell -p haskellPackages.pandoc_3_1_2\nthis derivation will be built:\n  /nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv\nbuilding '/nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv'...\nsetupCompilerEnvironmentPhase\nBuild with /nix/store/4wjl91hrizxghwqy18a1337gq2y9mh40-ghc-9.2.7.\nunpacking sources\nunpacking source archive /nix/store/7ncxphrr3nff9jb3j4w9ksl6ggznqhm6-pandoc-3.1.2.tar.gz\nsource root is pandoc-3.1.2\nsetting SOURCE_DATE_EPOCH to timestamp 1000000000 of file pandoc-3.1.2/xml-light/Text/Pandoc/XML/Light/Types.hs\npatching sources\ncompileBuildDriverPhase\nsetupCompileFlags: -package-db=/build/tmp.yaRUozaznX/setup-package.conf.d -j16 +RTS -A64M -RTS -threaded -rtsopts\n[1 of 1] Compiling Main             ( /nix/store/4mdp8nhyfddh7bllbi7xszz7k9955n79-Setup.hs, /build/tmp.yaRUozaznX/Main.o )\nLinking Setup ...\nconfiguring\nconfigureFlags: --verbose --prefix=/nix/store/7983f3r6gpgvf17dn1k2c05wma708xdn-pandoc-3.1.2 --libdir=$prefix/lib/$compiler --libsubdir=$abi/$libname --datadir=/nix/store/zdc55i48g6hpbwckiwk6s6iraf30hh99-pandoc-3.1.2-data/share/ghc-9.2.7 --with-gcc=gcc --package-db=/build/tmp.yaRUozaznX/package.conf.d --ghc-options=-j16 +RTS -A64M -RTS --disable-split-objs --enable-library-profiling --profiling-detail=exported-functions --disable-profiling --enable-shared --disable-coverage --enable-static --disable-executable-dynamic --enable-tests --disable-benchmarks --enable-library-vanilla --disable-library-for-ghci --ghc-option=-split-sections -f-trypandoc --extra-lib-dirs=/nix/store/4g9phbpakh51bbw2n391vipz9r5z56kw-ncurses-6.4/lib --extra-lib-dirs=/nix/store/mnq0hqsqivdbaqzmzc287l0z9zw8dp15-libffi-3.4.4/lib --extra-lib-dirs=/nix/store/0ssnwyy41aynhav7jr4dz1y01lfzi86f-gmp-with-cxx-6.2.1/lib\nUsing Parsec parser\nConfiguring pandoc-3.1.2...\nSetup: Encountered missing or private dependencies:\ndoctemplates &gt;=0.11 && &lt;0.12,\ngridtables &gt;=0.1 && &lt;0.2,\njira-wiki-markup &gt;=1.5.1 && &lt;1.6,\nmime-types &gt;=0.1.1 && &lt;0.2,\npandoc-types &gt;=1.23 && &lt;1.24,\ntexmath &gt;=0.12.7 && &lt;0.13\n\nerror: builder for '/nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv' failed with exit code 1\n\nExcept all the dependencies that the error message wanted, existed in nixpkgs.\nHere’s doctemplates\nHere’s gridtables\nAnd so on. It was like, even though pandoc required these packages, it couldn’t see them. They all had the format packagename_version, as opposed to simply packagename, which would be an older package.\nSo I asked on discord, again.\n\nThis one user, NobbZ, helps people so much that people joke that he is the documentation.\nI tried their solution, and it didn’t work. Apparently, this solution was designed for the newer feature of nix, flakes, which I wasn’t using.\nBut with some adjustment, I managed to figure out how to use the override feature on my own, with the same solution that NobbZ sent me seconds later:\n\n\nTrying to get override working\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n   /* pandoc = pkgs.haskellPackages.callCabal2nix \"pandoc\" (fetchTarball {\n        url = \"https://github.com/jgm/pandoc/archive/refs/tags/3.1.2.tar.gz\";\n        sha256 = \"1h928w4ghbxg5whq7d9nkrfll2abvmbkc45adfgv35rfhcpkiiv9\";\n    }) {};*/\n    doctemplates = pkgs.haskellPackages.doctemplates_0_11;\n    gridtables = pkgs.haskellPackages.gridtables_0_1_0_0;\n    jira-wiki-markup = pkgs.haskellPackages.jira-wiki-markup_1_5_1;\n    mime-types = pkgs.haskellPackages.mime-types_0_1_1_0;\n    pandoc-types = pkgs.haskellPackages.pandoc-types_1_23;\n    texmath = pkgs.haskellPackages.texmath_0_12_7_1;\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.override {inherit doctemplates gridtables jira-wiki-markup mime-types pandoc-types texmath;};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nExcept this errors:\nWarning:\n    This package indirectly depends on multiple versions of the same package. This is very likely to cause a compile failure.\n      package http-client (http-client-0.7.13.1-52kzOBAMbxmJrzoQZgatPf) requires mime-types-0.1.0.9-Gdz1G1mhqziCfo3C8KZHz7\n      package pandoc (pandoc-3.1.2) requires mime-types-0.1.1.0-4FUch8wD40c6kQtGdyJOSM\n      package texmath (texmath-0.12.7.1-BbrGid5okuSI4hfeGBAcF8) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package commonmark-pandoc (commonmark-pandoc-0.2.1.3-OwUzhyyJ0cDzxfYXzbAci) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package citeproc (citeproc-0.8.1-LP74PTBZCEoHiNCfXfUYdM) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package pandoc (pandoc-3.1.2) requires pandoc-types-1.23-AC7tSm0fcRIGMZsmro9kaK\n      package pandoc (pandoc-3.1.2) requires pandoc-types-1.23-AC7tSm0fcRIGMZsmro9kaK\n** abort because of serious configure-time warning from Cabal\nerror: builder for '/nix/store/ibawyigbdn9bs1gs9hc0mgzqraqfxhy0-pandoc-3.1.2.drv' failed with exit code 1\nEssentially, a dependency error. Doctemplates also required a package that wasn’t under the default of packagename, but rather packagename_version.\nAnother user proceeded to chime in with their solution:\n\n\nSee cdepillabout’s solution\n\nlet\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n    doctemplates = hprev.doctemplates_0_11;\n    gridtables = hprev.gridtables_0_1_0_0;\n    jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n    mime-types = hprev.mime-types_0_1_1_0;\n    pandoc-types = hprev.pandoc-types_1_23;\n    texmath = hprev.texmath_0_12_7_1;\n  });\nin\n  pkgs.mkShell {\n    packages = [pandoc];\n  }\n\nAnd this worked! Except it didn’t.\n[nix-shell:~/vscode/quartotest]$ pandoc\nbash: pandoc: command not found\n\n[nix-shell:~/vscode/quartotest]$ which pandoc\nwhich: no pandoc in (/nix/store/kbcrs84s1x8yd5bp1nq6q6ihda8nd2lp-bash-interactive-5.2-p15/bin:/nix/store/a9q4y7vw1fgs990bs5mpd3p50mc0iz27-python3-3.10.11/bin:/nix/store/nh8iz5l2zn5nbk19qxdw575a5fhfcajw-quarto-1.3.361/bin:/nix/store/ar2lzr4kr4pi1zgx3w8hl6fkny3bql53-python3.10-notebook-6.5.2/bin:/nix/store/ai5lxg5vzjsfk9zkyn65ndq81na2mm5c-python3.10-babel-2.12.1/bin:/nix/store/95cxzy2hpizr23343b8bskl4yacf4b3l-python3-3.10.11/bin:/nix/store/5ii8sm9yh01ny05bl1wjdv6pkdjb8bw0-python3.10-jupyter-core-5.2.0/bin:/nix/store/pkgr71n4dy7h9lp00paf6k3llfa95ig0-python3.10-Send2Trash-1.8.1b0/bin:/nix/store/x1kk4hlx0zl12igvr6v0pk2cq2720fbh-python3.10-jupyter_client-8.0.3/bin:/nix/store/9icvaw0dgk7258m564xlh513nz6xis1m-python3.10-nbformat-5.7.3/bin:/nix/store/6svh49hf9pq5hwavgyb642v5a0pjnn4a-python3.10-jsonschema-4.17.3/bin:/nix/store/15jn0r39wg0ripjzjfxj9arcv53qxck9-python3.10-nbclassic-0.5.2/bin:/nix/store/jjy30kw6pw2mq54ig6lrm84ds91a9snf-python3.10-ipython-8.11.0/bin:/nix/store\nApparently, in pandoc 3.0, the binary and the library have been split into two seperate packages. In nixpkgs, the library can be found in the haskellPackages.pandoc_3_1_2, and the binary can be found in haskellPackages.pandoc-cli.\nnix-shell -p haskellPackages.pandoc-cli\n\nSetup: Encountered missing or private dependencies:\ndoctemplates &gt;=0.11 && &lt;0.12, pandoc &gt;=3.0\n...\nerror: builder for '/nix/store/hzzqnffj08r9qc0xi3b4ydi7w91dn4m0-pandoc-server-0.1.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/arbq7vgw539xyd4l0y5x3jyhhra30v91-pandoc-cli-0.1.1.drv' failed to build\nThe pandoc-cli package is broken, for the exact same error that the pandoc library won’t compile for.\n\n\nSo I try my own override:\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n        doctemplates = hprev.doctemplates_0_11;\n        gridtables = hprev.gridtables_0_1_0_0;\n        jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n        mime-types = hprev.mime-types_0_1_1_0;\n        pandoc-types = hprev.pandoc-types_1_23;\n        texmath = hprev.texmath_0_12_7_1;\n      });\n    pandoc-cli = pkgs.haskellPackages.pandoc-cli.overrideScope (hfinal: hprev: {\n        hslua-core = hprev.hslua-core_2_3_1;\n        lua = hprev.lua_2_3_1;\n    });\nin\n    pkgs.mkShell {\n        packages = [ pkgs.python310Full quarto pkgs.jupyter pandoc pandoc-cli pkgs.deno ];\n    }\n\nBut this also errors:\n\n\nShow error\n\nUsing Parsec parser\nConfiguring tasty-hslua-1.0.2...\n\nSetup: Encountered missing or private dependencies:\nhslua-core &gt;=2.0 && &lt;2.3\n\nerror: builder for '/nix/store/nmhz8xnc24xw86q573v515n3q8m9l0y5-tasty-hslua-1.0.2.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/8q4ni7s2am50xbbbkcdjk85szyvq3jk8-hslua-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/hcysycpqkqv4kd3qmkwzyi7pkaqszqyy-hslua-marshalling-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/h4c32n03d305njipsiw4rzc8rq52l2bc-hslua-packaging-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/y655vw1bdq8a9j818k16y7228nlsf86y-hslua-cli-1.4.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/xkkp4cj1yfwjpczc6k7y08gxdqdbfh4n-pandoc-2.19.2.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/sc44mnc1ngxfxi7h3f6qrrvnvldla4w3-pandoc-cli-0.1.1.drv' failed to build\n~/vscode/quartotest master !4 ?9 ❯                                                                                        \n\nBut the hslua-core version I want was packaged in nixpkgs, similar to doctemplates, or gridtables. So I did a further override.\n\n\nShow/hide\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n        doctemplates = hprev.doctemplates_0_11;\n        gridtables = hprev.gridtables_0_1_0_0;\n        jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n        mime-types = hprev.mime-types_0_1_1_0;\n        pandoc-types = hprev.pandoc-types_1_23;\n        texmath = hprev.texmath_0_12_7_1;\n        tasty-hslua = hprev.tasty-hslua_1_1_0;\n        hslua-marshalling = hprev.hslua-marshalling_2_3_0;\n        hslua-aeson = hprev.hslua-aeson_2_3_0_1;\n        hslua = hprev.hslua_2_3_0;\n      });\n    pandoc-cli = pkgs.haskellPackages.pandoc-cli.overrideScope (hfinal: hprev: {\n        hslua-core = hprev.hslua-core_2_3_1;\n        lua = hprev.lua_2_3_1;\n        tasty-hslua = hprev.tasty-hslua_1_1_0;\n        hslua-marshalling = hprev.hslua-marshalling_2_3_0;\n        hslua-aeson = hprev.hslua-aeson_2_3_0_1;\n        hslua = hprev.hslua_2_3_0;\n    });\nin\n    pkgs.mkShell {\n        packages = [ pkgs.python310Full quarto pkgs.jupyter pandoc pandoc-cli pkgs.deno ];\n    }\n\nWhich still errors:\nUsing Parsec parser\nConfiguring hslua-typing-0.1.0...\n\nSetup: Encountered missing or private dependencies:\nhslua-core &gt;=2.3 && &lt;2.4, hslua-marshalling &gt;=2.3 && &lt;2.4\n\nerror: builder for '/nix/store/kbmfxjy0ycwwg6r6zsp9q9v1pfkmggnw-hslua-typing-0.1.0.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/nj4smnyrkaf52qx98r1wa0r1gdnjbwxk-hslua-2.3.0.drv' failed to build\nI quickly realized, that the updated set of haskell packages in nixpkgs, is broken all the way down. I found a relevant github issue. In this issue, somene had modified the derivation of the haskellPackages, to get pandoc-cli to work.\nI used their fork of nixpkgs to give myself pandoc-cli.\n~/vscode/quartotest master +3 !1 ❯ nix-shell -p haskellPackages.pandoc-cli -I nixpkgs=https://github.com/seam345/nixpkgs/archive/89e6e477c8357a087e863db562d2fa8d9fe5ba29.tar.gz\n[nix-shell:~/vscode/quartotest]$ pandoc --version\npandoc 3.1\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\nThis actually worked, and I got pandoc-cli with the 3.0 version of pandoc. However, I couldn’t get quarto to use pandoc-cli rather than the normal pandoc version, so quarto still wasn’t working.\nLater, cdepillabout chimed in again. Here is their solution:\n\n\nShow/hide\n\nlet\n  nixpkgs-src = fetchTarball {\n    # nixpkgs-unstable as of 2023-05-31\n    url = \"https://github.com/NixOS/nixpkgs/archive/58c85835512b0db938600b6fe13cc3e3dc4b364e.tar.gz\";\n    sha256 = \"0bkhaiaczj25s6hji2k9pm248jhfbiaqcfcsfk92bbi7kgzzzpif\";\n  };\n\n  my-overlay = final: prev: {\n\n    pandoc_1_3 =\n      let\n        inherit (final.haskell.lib.compose) disableCabalFlag markUnbroken;\n      in\n      final.lib.pipe\n        final.haskellPackages.pandoc-cli\n        [\n          markUnbroken\n          (disableCabalFlag \"lua\")\n          (p: p.overrideScope (hfinal: hprev: {\n            doctemplates = hprev.doctemplates_0_11;\n            gridtables = hprev.gridtables_0_1_0_0;\n            hslua-cli = null;\n            jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n            mime-types = hprev.mime-types_0_1_1_0;\n            pandoc = hprev.pandoc_3_1_2;\n            pandoc-lua-engine = null;\n            pandoc-server = markUnbroken hprev.pandoc-server;\n            pandoc-types = hprev.pandoc-types_1_23;\n            texmath = hprev.texmath_0_12_7_1;\n          }))\n        ];\n\n    quarto_1_3 =\n      let\n        quarto-version = \"1.3.361\";\n      in\n      (final.quarto.override { pandoc = final.pandoc_1_3; }).overrideAttrs (oldAttrs: {\n        version = quarto-version;\n        src = final.fetchurl {\n          url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${quarto-version}/quarto-${quarto-version}-linux-amd64.tar.gz\";\n          sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n      });\n  };\n\n  pkgs = import nixpkgs-src { overlays = [ my-overlay ]; };\n\nin\n\npkgs.quarto_1_3\n\nAlthough I modified it a bit, to be\n...\nin\npkgs.mkShell {\n  packages = [pkgs.quarto_1_3];\n}\nBecause it is the mkShell package that creates the shell environment.\nThis solution works, and gets me quarto 1.3, and also replaces the default dependency of quarto, on pandoc 2.1.9, with one on pandoc-cli.\n[nix-shell:~/vscode/quartotest]$ quarto pandoc --version\npandoc 3.1.2\nFeatures: +server -lua\nScripting engine: none\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\nHowever, it has a caveat. It compiles pandoc without lua support, as those packages where the ones that were broken in nixpkgs.\nBut apparently, quarto needs lua suppport.\n[nix-shell:~/vscode/quartotest]$ quarto render\n[1/9] about.qmd\nThis version of pandoc has been compiled without Lua support.\nSo yeah. That doesn’t work.\nBut meanwhile, the quarto team responded to a question I asked on their github discussion page. I had asked if quarto requires either the pandoc binary, or the pandoc library.\nThey replied, and said that a pandoc binary, is actually included inside the tarball and the packages they have created. As part of the build system, the package is fairly self reliant, not needing much in terms of external dependencies.\nSo that’s what I did.\nI modified the derivation so that it uses the built in pandoc, rather than replacing it with an external one.\n\n\nShow/hide\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        patches = [];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : ${pkgs.nodePackages.sass}/bin/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nAnd this works:\n[nix-shell:/tmp/test]$ quarto pandoc --version\npandoc 3.1.1\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\n\n[nix-shell:/tmp/test]$ quarto render\n[1/4] about.qmd\n[2/4] posts/post-with-code/index.qmd\n[3/4] posts/welcome/index.qmd\n[4/4] index.qmd\n\nOutput created: _site/index.html\n\n\n[nix-shell:/tmp/test]$ \nExcept not really:\n[nix-shell:/tmp/test]$ quarto check\n\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\nERROR: TypeError: Invalid Version: 1.62.1 compiled with dart2js 2.19.6\n\nTypeError: Invalid Version: 1.62.1 compiled with dart2js 2.19.6\n    at new SemVer (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:48564:19)\n    at Range.test (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:48974:23)\n    at satisfies (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:49191:18)\n    at checkVersion (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104009:14)\n    at checkVersions (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104035:5)\n    at async check (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:103989:13)\n    at async Command.fn (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104212:5)\n    at async Command.execute (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:8437:13)\n    at async quarto (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:127540:5)\n    at async file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:127558:9\nSo I did some experimenting. First, I replaced all the dependencies with the versions that came with the pandoc package, and then I didn’t get this error with the quarto check command, which checks the installation of quarto, python, pandoc, and R. I then removed the dependencies on by one, to see which one broke it. It ended up being dart-sass.\nSo I tried again:\n\n\nShow shell.nix\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        patches = [];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nAnd this works. The only thing that goes wrong is it gives me a warning when I use the preview function.\nWARNING: Specified QUARTO_PYTHON '/nix/store/xs35q9yb940cxsy1y0qcs84239zmd2jn-python3-3.10.11-env/bin/python3:/bin/python' does not exist.\nI’ve found no wayh to get rid of this warning, and since it is just a warning, I will ignore it. Here is my new shell.nix.\n\n\nShow shell.nix\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        buildInputs = with pkgs; [ python3 jupyter ];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3}/bin/python3\n        '';\n        installPhase = ''\n            echo \"this is the quarto python ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python\"\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nHowever, this is messy. Fitting an entire set of overrides into a single shell.nix file is definitely not the neatest way to do this. And there are some other flaws, like things that aren’t necessary as a dependency. Fortunately, there is a neater way.\nThe nix callPackage function allows for a nix function to call it’s own derivation. Rather than using an override, I can write my own derivation and use the callPackage fucnction to call upon it.\nHere is my derivation:\n\n\nShow derivation:\n\n{ stdenv\n, lib\n, esbuild\n, deno\n, fetchurl\n, nodePackages\n, rWrapper\n, rPackages\n, extraRPackages ? []\n, makeWrapper\n, python3\n, extraPythonPackages ? ps: with ps; []\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"quarto\";\n  version = \"1.3.361\";\n        src = fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n\n  nativeBuildInputs = [\n    makeWrapper\n  ];\n\n  patches = [\n    ./fix-deno-path.patch\n  ];\n\n  postPatch = ''\n    # Compat for Deno &gt;=1.26\n    substituteInPlace bin/quarto.js \\\n      --replace 'Deno.setRaw(stdin.rid, ' 'Deno.stdin.setRaw(' \\\n      --replace 'Deno.setRaw(Deno.stdin.rid, ' 'Deno.stdin.setRaw('\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${lib.makeBinPath [ deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${rWrapper.override { packages = [ rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${python3}/bin/python3\n    '';\n\n  installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n\n  meta = with lib; {\n    description = \"Open-source scientific and technical publishing system built on Pandoc\";\n    longDescription = ''\n        Quarto is an open-source scientific and technical publishing system built on Pandoc.\n        Quarto documents are authored using markdown, an easy to write plain text format.\n    '';\n    homepage = \"https://quarto.org/\";\n    changelog = \"https://github.com/quarto-dev/quarto-cli/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ mrtarantoga ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nAnd here is the shell.nix that summons this package:\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n    quarto = pkgs.callPackage ./env/quarto.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\n\nConclusion and How to Use\nNow, users who have cloned the repo for this blog, can simply install nix, and run nix-shell in the root directory of the repo in order for them to get quarto, python, and jupyter, the dependencies I rely on for this project. Because of the way nix, works it is easy to modify my shell.nix, and add more dependencies, like R or more extra language support via juptyer kernels.\nMy usual workflow is to open a terminal in this git repo, and type nix-shell, and then code ., which gives me vscode with quarto (I have the quarto vscode extension installed), python, and jupyter.\nI realized in hindsight that this only works on x86_64 linux, because my derivation works by taking the quarto x86_64 linux binary and packaging it using nix. But I did learn a lot about writing derivations through this, which I can apply to other things.\n\n\nTry 2, creating a multi architechture and multi OS package\nVery frustrating, quarto has no compilation instructions in the readme. However, thankfully, use github actions to do testing, and builds of the applications, which means that the build steps are technically public, just not immediately apparent.\n\nGithub actions works by reading a .yml file for the instuctions on what do do, and this file can be found in the github actions menu, as workflow file.\nI don’t need the whole thing, however, just the tarball building steps.\n\n\nShow steps\n\nobs:\n  configure:\n    runs-on: ubuntu-latest\n    outputs:\n      version: ${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number}}\n      version_base: ${{steps.config.outputs.version}}\n      tag_name: v${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number }}\n      release: v${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number }}\n      changes: ${{ steps.config.outputs.changes }}\n    if: github.event_name != 'schedule' || (github.event_name == 'schedule' && github.repository == 'quarto-dev/quarto-cli')\n    steps:\n      - name: Install libc6-div\n        run: sudo apt-get install libc6-dev\n\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: config\n        id: config\n        run: |\n          source ./configuration\n          CHANGES=\n          # CHANGES=$(git log $(git describe --tags --abbrev=0)..HEAD --oneline)\n          # Escape \\n, \\r to preserve multiline variable\n          # See https://github.community/t/set-output-truncates-multiline-strings/16852/2\n          # CHANGES=\"${CHANGES//'%'/'%25'}\"\n          # CHANGES=\"${CHANGES//$'\\n'/'%0A'}\"\n          # CHANGES=\"${CHANGES//$'\\r'/'%0D'}\"\n          # echo \"changes=$CHANGES\" &gt;&gt; $GITHUB_OUTPUT\n          QUARTO_BUILD_NUMBER=$(($QUARTO_BUILD_RUN_OFFSET + $GITHUB_RUN_NUMBER))\n          echo \"version=$QUARTO_VERSION\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"changes=$CHANGES\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"build_number=$QUARTO_BUILD_NUMBER\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: News\n          path: ./news/changelog-${{steps.config.outputs.version}}.md\n\n  make-source-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Make Tarball\n        run: |\n          tar -zcvf  quarto-${{needs.configure.outputs.version}}.tar.gz *\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Source\n          path: ./quarto-${{needs.configure.outputs.version}}.tar.gz\n\n  make-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Configure\n        run: |\n          ./configure.sh\n      - name: Prepare Distribution\n        run: |\n          pushd package/src/\n          ./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --log-level info\n          popd\n      - name: Make Tarball\n        run: |\n          pushd package/\n          mv pkg-working quarto-${{needs.configure.outputs.version}}\n          tar -cvf  quarto-${{needs.configure.outputs.version}}-linux-amd64.tar quarto-${{needs.configure.outputs.version}}\n          gzip quarto-${{needs.configure.outputs.version}}-linux-amd64.tar\n          mv quarto-${{needs.configure.outputs.version}} pkg-working\n          popd\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Deb Zip\n          path: ./package/quarto-${{needs.configure.outputs.version}}-linux-amd64.tar.gz\n\n  make-arm64-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Configure\n        run: |\n          ./configure.sh\n      - name: Prepare Distribution\n        run: |\n          pushd package/src/\n          ./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --arch aarch64 --log-level info\n          popd\n      - name: Make Tarball\n        run: |\n          pushd package/\n          mv pkg-working quarto-${{needs.configure.outputs.version}}\n          tar -cvf  quarto-${{needs.configure.outputs.version}}-linux-arm64.tar quarto-${{needs.configure.outputs.version}}\n          gzip quarto-${{needs.configure.outputs.version}}-linux-arm64.tar\n          mv quarto-${{needs.configure.outputs.version}} pkg-working\n          popd\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Deb Arm64 Zip\n          path: ./package/quarto-${{needs.configure.outputs.version}}-linux-arm64.tar.gz\n\nSo to convert github actions into somethign easier to read:\nAMD64 build:\nsource ./configuration\n./configure.sh\npushd package/src/ # pushd is like a more advnaced version of cd\n/quarto-bld prepare-dist --set-version ${{needs.configure.outputs.   version}} --log-level info \npopd\npushd package/\nmv pkg-working quarto-${{needs.configure.outputs.version}}\ntar -cvf  quarto-${{needs.configure.outputs.version}}-linux-amd64.tar quarto-${{needs.configure.outputs.version}}\ngzip quarto-${{needs.configure.outputs.version}}-linux-amd64.tar\nmv quarto-${{needs.configure.outputs.version}} pkg-working\npopd\nAnd then based on the github actions, the generated tarball is located at ./package/quarto-${{needs.configure.outputs.version}}-linux-amd64.tar.gz\nARM64 build (very similar steps):\n./configure.sh\npushd package/src/\n./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --arch aarch64 --log-level info\npopd\npushd package/\nmv pkg-working quarto-${{needs.configure.outputs.version}}\ntar -cvf  quarto-${{needs.configure.outputs.version}}-linux-arm64.tar quarto-${{needs.configure.outputs.version}}\ngzip quarto-${{needs.configure.outputs.version}}-linux-arm64.tar\nmv quarto-${{needs.configure.outputs.version}} pkg-working\npopd\nAnd then based on the github actions, the generated file is located at ./package/quarto-${{needs.configure.outputs.version}}-linux-arm64.tar.gz\nI downloaded these files from releases, and they appear to be extremely similar. The quarto “binary” is a bash script that includes a check for architechture. So the only difference is in the configure step, which seems to download a different architechture.\nHowever, when experimenting with trying to compile quarto on my phone (arm) using termux, I get an error. deno: cannot execute: required file not found. So far, googling this error has gotten me nowhere. I may have to create an arm vm to test later."
  }
]