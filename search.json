[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Why schools should be less aggressive with content blocking\n\n\n\n\n\n\n\nenglish\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/blocking/index.html",
    "href": "blog/posts/blocking/index.html",
    "title": "Why schools should be less aggressive with content blocking",
    "section": "",
    "text": "I love experimenting with, and managing internet servers. Over the years, I have learned real world skills from my personal projects, as shown by the role I play my the computer science classes here at del norte. I do things, like create guides or fix broken one’s.\nBut when I try to access my server on school wifi, I get met with the below image.\n\nNot managed. My server is not blocked because it contains dangerous or harmful content, it’s blocked because my school’s (and so many others) uses a whitelist system. A whitelist means that everything is blocked by default, and things must explicitly be allowed through the firewall.\nNow I have a way around this. I can use a VPN, or similar software to get around the content filtering restrictions In addition to that, I can simply work on my home wifi, where there are no such restrictions. For me, getting access to my project based learning environments are easy.\nBut for students, who only use chromebooks, they cannot get around these restrictions. Chromebooks, the managed devices distributed by our school and so many others, are completely locked down. The chromebook itself implements the content blocking software, rather than the WiFi.\nThis means that, if a student were working at home on a school chromebook that was their only device, they would be denied resources that I have access to, purely because I have my own laptop. Information, research material, learning material, troubleshooting guides, and things like my digital playgrounds are locked away from those students.\nIt gets worse. Because the restrictions on the chromebooks prevent them from installing software, they cannot install the necessary tools needed for our computer science classes here at Del Norte. And unlike so many other computer based courses at Poway USD, the CompSci class does not provide it’s own computers for use. I personally know someone who wanted to take computer science, but was forced to drop out because they could not obtain their own device.\nObviously, this is unfair. The point of chromebooks is to offer students who are unable to obtain their own device for whatever reason the ability to have a computer to work on so they can be on equal footing with those who can afford their own devices. But what’s the purpose if they don’t actually offer students the ability to participate in the same activities and classes that students with more resources — wealth — have?\nChromebooks should be, at the very least, usable for all classes. But the only just thing is that students with only school provided chromebooks are given access to the exact same resources as those who can afford to buy their own device.\nThe reason why schools can and do provide chromebooks and other internet services is that they are cheap — Cheaper than they are normally. In America, there exists a program called e-rate, which is designed to make technology more accessible to public institutions, by providing discounts or potentially free products and services. The government subsidizes the cost.\nSadly, this program does not seem to cover computers (see section about internet access), which is probably why schools opt for chromeboks, the cheapest, lowest end device. Although it should be noted, that Google and Apple do have their own deals for offering discounts for bulk purchases for schools and other public institutions.\nAs great as e-rate is (it’s why the WiFi on school and college campus is so fast, if you know how to utilize it), it comes with caveats and conditions.\nThe same way the federal government holds funding for certain services over the head of state and local governments, the federal government holds the eligibility for e-rate over the head of schools and libraries. They must meet a certain requirements. One of those conditions is following CIPA, the Children’s Internet Protection Act.\nIn the beginning of this article, I linked an image of what our school’s block screen looks like. Quoting below:\n\nThe site you have requested has been blocked because it does not comply with the filtering requirements as described by the Children’s Internet Protection Act (CIPA).\n\nSo, when you click on CIPA, it takes you to the law itself. The law is very short, very vague, and the really important parts can fit comfortably into this article:\n\nSchools must implement a policy addressing these things. It does not say exactly what policies they must implement, or does it define materials harmful to minors, or even define “hacking”.\nBecause of the way the law is vaguely worded, schools are obligated to implement as aggressively as a content blocking policy as they can, including the complete prevention of installing software on chromebooks. Because the truth of the matter is, if someone can install VSCode (Programming application we use at Del Norte), then they can potentially install censorship circumvention software.\nIt’s very clear what is happening here. The school cannot meet both obligations, one to prevent students from accessing resources, and another to make resources accessible to students. Becuase not attempting to block everything they can leads to losses on the e-rate discounts, Del Norte breaks it’s promise to it’s students to provide resources to them. Because to this institution, and many more, money is more important than the education of their students.\nNow, it’s not like I am personally not doing anything about this. In this blog, I have another post, about creating a system that will let students access. In addition to simply setting this system up, I am optimizing it, to make it cheaper and more accessible to students, especially those with less resources.\nOf course, as amazing as what I am deploying is, allowing students to get access to a fully featured linux desktop from their browser, it is also flawed — it also acts as a censorship circumvention software. Inside the system they have access to, users are given unfiltered content. Because of this, there is a possibility that schools would be obligated to block this as well.\nUltimately, my software is cool, equipping at least our computer science class the ability to be done on nothing but chromebooks, it is but a band-aid for the real problem — the laws that force schools further limit access to digital resources for students who already are lacking resources.\nIt does them little good. Off the top of my head, I know of several different ways to get around these content blocker, even on chromebooks. These blocking measures certainly do have an effect for general purpose use, but against a dedicated student, they are ultimately ineffective.\nFor some context, CIPA went into effect in 2001. The first iPhone came out in 2005. The law’s intentions are nice, but it pretty clearly wasn’t created with the foreknowledge that every student (who could afford one) would have a device in their pocket capable of circumventing pretty much all of the content blocking restrictions.\nBecause of this, despite the law being written to “protect” everybody, it only actually affects one group of people. Those who can only rely on school provided devices.\nThis is a form of institutional oppression of those who are in a lower socioeconomic class. Even though chromebooks are easily capable of installing the necessary software for computer science courses, laws in place force administrators to prevent students from doing so.\nThis is unjust. The restrictions on these laws should be lessened to enable students who already have a lesser access to such resources an equal access. If parents can be trusted to monitor their kid’s internet usage on a personal device, like a phone or macbook, why can’t they be trusted to do the same with a school chromebook?"
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "Urestricted Wifi/Hotspot as long as you have one unlimited device\n\n\n\n\n\n\n\nlinux\n\n\n\n\nAs long as you have one device with no restrictions, you can make this become many\n\n\n\n\n\n\nJun 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nNginx proxy manager\n\n\n\n\n\n\n\nlinux\n\n\ndevops\n\n\nnginx\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nHow to get a subdomain from duckdns\n\n\n\n\n\n\n\nlinux\n\n\ndevops\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\n\n\n\n\n  \n\n\n\n\nSetting up cockpit\n\n\n\n\n\n\n\naws\n\n\nec2\n\n\ndocker\n\n\nlinux\n\n\n\n\ncockpit is a gui to manage linux servers.\n\n\n\n\n\n\nSep 30, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guides/cockpit-setup/index.html",
    "href": "guides/cockpit-setup/index.html",
    "title": "Setting up cockpit",
    "section": "",
    "text": "What is cockpit (and similar softwares)\nAmazon lets us have free servers via EC2. The typical way to manage servers is either by sshing in, or using the cloud shell that Amazon (and Oracle) give. However, there are alternative ways to manage servers. One extremely popular example is pteradactyl, a webpage based gui to manage game (usually minecraft). It lets you download game servers as docker containers, run them, stop them, and maybe manage some basic settings, All the things a casual who just wants video games may need. But when I created a free Oracle server, I wanted something more. By this point, I was an experienced linux user, and I wanted more advanced features. So I searched for a more advanced server management tool, like people use on real servers, and I found cockpit.\nCockpit comes with many benefits. The two things I really like however, are that it’s terminal is not laggy at all, unlike the amazon ec2 cloud terminal, and it also offers a gui to manage docker containers.\n\n\nThe installation process\nThe installation process is simple:\nsudo apt install cockpit\nTo start the server, run:\nsudo systemctl enable --now cockpit\nThis sets the cockpit server to start on boot, and it starts it now.\nHowever, the firewall must open ports to allow the cockpit server through. This opens the default ports for the cockpit server. It should be noted that not every version fo linux uses ufw as a firewall, some use other firewalls with different management commands.\nsudo ufw allow 9090\nAnother important thing is to set the password for the default “ubuntu” user account so that you can login to cockpit.\nsudo passwd ubuntu\nIt will ask for the new password twice, not showing what you are typing.\nReboot the computer for the server to start properly, however, this won’t work as the virtual private cloud must have its ports open. I had to do this when I set up cockpit on my Oracle server, so I knew the gist of the steps.\n\n\nOpening EC2’s VPC ports (Also necessary if you want to host servers on ports other than 22, 80, or 443)\nFirst, go to your EC2 vps, where you would normally click connect from, and click on the link under vpc:\n\nThis should bring you up to a screen like this:\n\nClick the security tab, bringing you to a screen like this:\n\nAnd then click on security groups, bringing you to a screen like this:\n\nAnd then click on the “edit inbound rules”\nFinally, you should get something like this:\n\nAdd an item that matches what I have in the third row. That opens the port to allow cockpits server to escape. You may also need to use this page to open other ports if you are hosting servers on nonstandard ports."
  },
  {
    "objectID": "guides/duckdns/index.html",
    "href": "guides/duckdns/index.html",
    "title": "How to get a subdomain from duckdns",
    "section": "",
    "text": "Why?\nPeople can’t register for freenom consistently, and it can take time to get a domain from the one’s we have as a class. Duckdns allows people to create their own free domain, extremely easily, and nearly instantly.\n\n\nRegistration and Setup\n\nThis is the registration page. I really like duckdns because you only need a github account to login, which we already have.\n\nAfter you login, you will see this. You can get your own subdomain, and then set your ip address manually.\nTo find the ip address of your server, run this command on your AWS server:\ncurl ifconfig.me\nIf the command curl is not found, install it using apt.\nThen, you can manually input your server’s ip address into the duckdns website.\nIn your nginx confiuration file, make sure you set your nginx configuration file to be your domain name.\nserver {\n    listen 80;\n    listen [::]:80;\n    server_name [yoursubdomain].duckdns.org;\n\n    location / {\n        proxy_pass http://localhost:8087; # Make sure this matches the port your docker-compose.yml is set to\n        # Simple requests\n        if ($request_method ~* \"(GET|POST)\") {\n                add_header \"Access-Control-Allow-Origin\"  *;\n        }\n\n        # Preflight requests\n        if ($request_method = OPTIONS ) {\n                add_header \"Access-Control-Allow-Origin\"  *;\n                add_header \"Access-Control-Allow-Methods\" \"GET, POST, OPTIONS, HEAD\";\n                add_header \"Access-Control-Allow-Headers\" \"Authorization, Origin, X-Requested-With, Content-Type, Accept\";\n                return 200;\n        }\n    }\n}"
  },
  {
    "objectID": "guides/npm/index.html",
    "href": "guides/npm/index.html",
    "title": "Nginx proxy manager",
    "section": "",
    "text": "What is NPM and why do I want to use it?\nNginx proxy manager (npm, but not the node one) is a web based frontend for nginx that automatically also configures letsencrypt, similar to how certbot does it. It makes nginx much easier to use. Rather than writing config files, you can just click around, which is much easier. For a high school computer science class, I think NPM is better, because it doesn’t have the complexity of npm (less potential for accidental failurs), but still teaches people about ports mapping, encryption, and other necessary skills.\nIn addition to that, with npm, even if someone does create a bad config, only their server goes down. With npm, a bad nginx config leads to the whole server going down. That is… not optimal.\n\n\nInstallation and Setup\nInstallation is simpleish.\nFirst, create a docker network for usage with our docker containers (step from here). Because these are our school projects, I will call that network nighthawks\ndocker network create nighthawks\nCreate a folder called npm, and put a docker-compose.yml in it (basic compose file from here):\nversion: '3'\nservices:\n  app:\n    image: 'jc21/nginx-proxy-manager:latest'\n    restart: unless-stopped\n    ports:\n      - '80:80'\n      - '81:81'\n      - '443:443'\n    volumes:\n      - ./data:/data\n      - ./letsencrypt:/etc/letsencrypt\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\ndocker-compose up -d and you’re good to go. It should be noted that you need to have ports 443 and 80 unused by anything else, like Nginx proper. So if you are running nginx, stop it first before you up NPM.\nNPM does need to have port 81 accessible.\nYou can either use a reverse proxy, or open up the port to be accessible from the internet.\nIf you want the port to be accessible from the internet, you might have a firewall of some kind, so just open that. And if you are using one of the big cloud providers (aws, azure, oracle), then you also might have to configure security control groups, as that acts as an extra firewall for those server types. See my cockpit guide on how to do this with AWS.\nIf you want to do a reverse proxy, just use npm to do it: Use proxy post to connect http://npm_app_1:81 to a domain name.\nNow, to configure npm, just access the web interface at https://[domainname/ip]:81\n\n\nUsage\nYou may notice above, in the section about using a reverse to expose npm, I use the docker container name, rather than a port. That’s the amazing part of npm. As long as your docker containers are on the same network, all you need is a hostname and the used port. You don’t even need to expose ports in your docker-compose.yml\nSo rather than the docker-compose.yml we use in our deployment guide\nWe can use:\nversion: '3'\nservices:\n      web:\n              image: flask_port_v1\n              build: .\n              #ports: # ports section not needed\n                     # - \"8086:8080\"\n              volumes:\n                      - persistent_volume:/app/volumes\nvolumes:\npersistent_volume:\n  driver: local\n  driver_opts:\n    o: bind\n    type: none\n    device: /home/ubuntu/flask_portfolio/volumes\n    # replace just flask_portfolio\n\nnetworks:\n  default:\n    external: true\n    name: nighthawks\nAnd then you can simply expose http://flask_port_v1_web_1:8080 to the world!"
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html",
    "href": "guides/unrestricted-tethering/index.html",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "",
    "text": "So I was on a plane ride recently, and the plane gave free, unlimited wifi, to one device, as long as you were using a T-Mobile data plan.\nI used my phone for a bit, but then I wanted to use my computer.\nOkay, so I’m writing this later, after I had written the guide. Apparently, the plan for the boat was 4 devices per person, not total. And even though I had wifi, when I tested USB tethering, it just worked. Either my phone does the proxying automatically or something, because it shouldn’t have worked according to reports of people I’ve talked to online.\nBut after looking at some configs on my computer, It does look like my phone creates a virtual network which my computer connects to, and proxies all traffic."
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html#why",
    "href": "guides/unrestricted-tethering/index.html#why",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "",
    "text": "So I was on a plane ride recently, and the plane gave free, unlimited wifi, to one device, as long as you were using a T-Mobile data plan.\nI used my phone for a bit, but then I wanted to use my computer.\nOkay, so I’m writing this later, after I had written the guide. Apparently, the plan for the boat was 4 devices per person, not total. And even though I had wifi, when I tested USB tethering, it just worked. Either my phone does the proxying automatically or something, because it shouldn’t have worked according to reports of people I’ve talked to online.\nBut after looking at some configs on my computer, It does look like my phone creates a virtual network which my computer connects to, and proxies all traffic."
  },
  {
    "objectID": "guides/unrestricted-tethering/index.html#how",
    "href": "guides/unrestricted-tethering/index.html#how",
    "title": "Urestricted Wifi/Hotspot as long as you have one unlimited device",
    "section": "How",
    "text": "How\nThere is probably an equivalent set of steps you can take to get this setup using iSH, the iPhone terminal emulator, and iPhone USB tethering, as this process is universal, but this guide is written for Android specifically, as that is what I have.\nFirstly, tether your phone using USB tethering. Simply plug your android device into your computer, and then in the settings, you can usually find the usb tethering option in the tethering and mobile hotspot options, wireless and networks, or usb preferences sections of settings. Test if this works. I should have tested, instead of trying to do everything in this guide first, because the security of United airlines Wifi seems to be less than that of the cell carrier of that one reddit user who said that simply USB tethering didn’t allow them to get around their carrier hotspot data cap for other devices.\nInstall termux. Termux is a terminal emulator for android, which gives access to many linux utilities. Yyou can get it from F-droid, or the Github Releases. The version in the Google Play store is outdated, and not recommended.\nIn termux, first update the system:\npkg update\nThen, install the necessary package. We are going to be using a Socks5 Proxy for this.\npkg install microsocks\nNow run the command ip a You should get an output similar to this\nmoonpie@localhost:~/vscode/moonpiedumplings.github.io&gt; ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: wlan0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default qlen 1000\n    link/ether f6:41:0a:25:44:c5 brd ff:ff:ff:ff:ff:ff permaddr 0c:dd:24:ca:bb:f1\n    altname wlo1\n    altname wlp0s20f3\n3: usb0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UNKNOWN group default qlen 1000\n    link/ether 2a:56:99:ae:d0:6f brd ff:ff:ff:ff:ff:ff\n    altname enp0s20f0u1\n    inet 192.168.67.6/24 brd 192.168.67.255 scope global dynamic noprefixroute usb0\n       valid_lft 3573sec preferred_lft 3573sec\n    inet6 fe80::96de:a3e6:3915:6283/64 scope link noprefixroute \n       valid_lft forever preferred_lft forever\nAlthough this is ran from my computer which is currently tethered using this method, so your output won’t be the same. You will lack a usb0 part, and you wlan0 will have an inet value. Note this value, as this is the IP address of your phone on the wifi you are connected to.\nNow, for security purposes, it’s important to ensure that the proxy requires authentication, otherwise anyone can connect to it.\nmicrosocks -u username -p password -1 # This command requires a username and password to authenticate once, then whitelists, the connecting ip, so that it doesn’t have to authenticate.\nTo authenticate the proxy to put your device on the whitelist, you can use any method to connect to a socks5, authenticated proxy, such as curl --socks5 user:password@phoneip:1080 google.com, on your computer or connecting device.\nThen, you can connect to the proxy using other means. For example, firefox has a built in option to connect to socks5 proxies, although it does not support authentication.\n\nWith this, the traffic from your browser will go through your phone, enabling you to get around the one-device restriction.\nAlternatively, on linux, there is usually a proxy options with this setting, to enable you to set a socks5 proxy system wide.\nHere is this option in my KDE system settings.\n\nIf you have no way to authenticate a socks5 proxy, then you can run an unauthenticated proxy. Simply run microsocks, and connect the same way.\n\n\n\n\n\n\nWarning\n\n\n\nThis method works by creating an unauthenticated socks5 proxy server. Although not apparent to the average user, anyone with scanning utilities on the same network can see this server, and connect to it.\nIf they connect, then no private data of yours will be put in jepoardy.\nHowever, if they do connect they will be able to eat up your bandwidth, potentially slowing your connection down, or they could do illegal things and you could be held liable for them, as it is your connection.\nI’m going to see if there is some way to bind this only to the tethered device.\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen authenticating to a socks5 server, the passwords are sent over the network in plaintext. Although if you connect over USB tethering, no network should be exposed to the wireless network, if you are connecting directly over the wireless network, then people would be able to sniff your password and connect to your socks5 server.\nI might research if there is a way to limit the connections in advance, but it might not be possible with such a small and lightweight server application.\n\n\nAirplane wifi, and later on when I get to it, boat wifi, are both really slow. Don’t expect much speed from this setup when you are already limited. But I suspect this works great for those who live in areas without internet, but are on cellular data plans that let them created limited hotspots or do limited tethering."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hi, I’m Jeffrey Fonseca",
    "section": "",
    "text": "Over the summer before my freshman year of high school, I took a course at palomar college, CSNT 110, hardware and OS fundamentals.\nI first learned about the internals of computers during a summer course I took at Palomar college online before my freshman year of highschool — CSNT 110/ OS and Hardware Fundamentals.\nDuring my freshman year, when my laptop starting slowing down, I switched to a linux based operating system, in an attempt to make it faster, using knowledge I had obtained from that course.\nSince then, I have been tinkering, playing with software, and doing personal projects. However, when I was taking AP Computer Science Principles, during my senior year of high school, my teacher criticized me for being a maverick. I didn’t bother with the group projects, working on my own projects, although I contributed back to the group often, but I didn’t document what I do.\nIn response, I created this website. It isn’t fancy, because frontend/website design isn’t my preferred work. But for it does what I desire for it’s purposes, and since then, I’ve been documenting my work, to create a portfolio for myself.\nThe projects documents my progress in my personal projects.\nBlog are my writings and ramblings.\nPlayground is my shorter term experiments and testings.\nGuides are guides I’ve written to help other people. For example, I created the duckdns guide because I was tired of helping people through that myself."
  },
  {
    "objectID": "playground.html",
    "href": "playground.html",
    "title": "Playground",
    "section": "",
    "text": "Dorking around\n\n\n\n\n\n\n\ncybersecurity\n\n\n\n\nZoomeye is pretty interesting\n\n\n\n\n\n\nJun 12, 2023\n\n\n\n\n\n\n  \n\n\n\n\nA very clever crypto scam\n\n\n\n\n\n\n\ncybersecurity\n\n\n\n\nFree bitcoin? Probably not.\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nExperiments with Running python in the browser\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "playground/crypto-scam/index.html",
    "href": "playground/crypto-scam/index.html",
    "title": "A very clever crypto scam",
    "section": "",
    "text": "So I was browsing around, and I found this public pastebin.\n\nklovenierm6@193.233.202.76 where the password is bDBShj\nIt seemed like a crypto wallet reset password? But why post this in a pastebin? It seemed suspicious, but I decided to log in.\n\nWow. $10000 worth of Bitcoin, right there for me to take. It seems to good to be true.\nSo, I spun up a wallet and attempted to deposit the money into it.\n\nWow. In order to withdraw the money first, to an unverified account, I first have to deposit around $2000 worth of Bitcoin into their account.\nObviously, this is a scam. It’s a fairly common pattern, “Here’s free money, but first, send me some money.” But this is such an interesting technological twist, I couldn’t not blog about it.\nBut I decided to experiment a little further.\n~ ❯ nmap -sV 193.233.202.76\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-06-11 12:46 PDT\nNmap scan report for vm.lan (193.233.202.76)\nHost is up (0.21s latency).\nNot shown: 995 closed tcp ports (conn-refused)\nPORT      STATE    SERVICE       VERSION\n22/tcp    open     ssh           OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n2222/tcp  open     ssh           OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n5900/tcp  filtered vnc\n5901/tcp  filtered vnc-1\n16992/tcp filtered amt-soap-http\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\nNmap is a port scanning utility that tells you what people are running on their server.\nSo I attempted to connect to their other services. I tried to connect to both vnc servers first, but for both, I got a server not found error. This is probably because those services aren’t up for public usage, as shown by the filtered state they return in the nmap scan. I suspect that they have a firewall that only allows for certain IP addresses or something like that.\nI also tried to connect to the ssh service on port 2222, but it kicked me out saying I needed a public key.\nOooh, a scan later in the day (around 4-5 hours later) is differerent.\n~ ❯ nmap -sV 193.233.202.76\nStarting Nmap 7.94 ( https://nmap.org ) at 2023-06-11 22:00 PDT\nNmap scan report for vm.lan (193.233.202.76)\nHost is up (0.21s latency).\nNot shown: 996 closed tcp ports (conn-refused)\nPORT     STATE    SERVICE VERSION\n22/tcp   open     ssh     OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n2222/tcp open     ssh     OpenSSH 8.4p1 Debian 5+deb11u1 (protocol 2.0)\n5900/tcp filtered vnc\n5901/tcp filtered vnc-1\nService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernel\n\nService detection performed. Please report any incorrect results at https://nmap.org/submit/ .\nNmap done: 1 IP address (1 host up) scanned in 32.93 seconds\nThere appear to have taken the amt-soap-http service offline. Good security protocol, getting rid of uneeded services, minimizing the attack surface, but do they really need those vnc servers?\nI decided to do a little more searching.\nI first searched google, and only the same pastebin came up, which has since been taken offline.\nI did find something interesting by searching on github:\n\nBut upon going through that repo, it was simply someone’s pastebin archiving repo. And in addition to that, the servers in those that I found, I have since been taken offline."
  },
  {
    "objectID": "playground/dorking/index.html",
    "href": "playground/dorking/index.html",
    "title": "Dorking around",
    "section": "",
    "text": "So I recently saw Maia Crimew’s (in?)famous blog post, How to completely own an airline in 3 easy steps. In this, she describes how she managed to get ahold of confedential data from major airlines, using little technological knowlege, or actual hacking.\nSh used the zoomeye internet search engine, somewhat an analogue of google, except rather than searching all websites, it searches all internet connected devices — including insecure ones.\nI was inspired by her, to do my own “dorking,” or search engine based hacking, trying to find vulnerable public services. She searched for public jenkins servers, which is a build and deployment system, that seems to be able to leak secrets if not configured correctly.\nI decided to search for something even easier. I searched for xterm, a browser based terminal. It took some tinkering with the terms, but eventually I found some publicly exposed servers. People had left an xterm session running, sometimes even with root privileges enabled.\nI decided to search further. I searched guacamole, a web based connector to remote desktop protocol sessions to see if anyone had left any exposed."
  },
  {
    "objectID": "playground/interactive-python-tutorial/index.html",
    "href": "playground/interactive-python-tutorial/index.html",
    "title": "Experiments with Running python in the browser",
    "section": "",
    "text": "My goal is an in browser python editor I can embed into a blog.\nCode is ran client side, so don’t try to crash any server or anything like that.\n\n\n                        \n                    \nAwww, the input function doesn’t seem to work.\nShift + enter/return to evaluate code of the below.\n\nThis kinda works, but also has issues. Input fucntion still doesn’t work.\nShift + enter/return to evaluate below. Or double click.\n\n   import numpy as np\n   np.random.rand(5)\n\nThis is cool, but the code isn’t editable. It just resets itself for some reason.\nMaybe this will work?\n\n\n  Open the project Untitled Project in LiveCodes\n\n\n\n\n\n\n\nyeet = input(\"test\")\n\nprint(yeet)"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Packaging Openstack on Nixos\n\n\n\n\n\n\n\nnix\n\n\nhci\n\n\n\n\n\n\n\n\n\n\n\nJun 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPackaging quarto using nix\n\n\n\n\n\n\n\nnix\n\n\nquarto\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCompiling KasmVNC on NixOS\n\n\n\n\n\n\n\nlinux\n\n\ndevops\n\n\nnix\n\n\nkasm\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nKasmweb setup\n\n\n\n\n\n\n\nnix\n\n\nlinux\n\n\ndevops\n\n\nubuntu\n\n\nkasm\n\n\n\n\n\n\n\n\n\n\n\nJan 26, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/compiling-kasmvnc-on-nixos/index.html",
    "href": "projects/compiling-kasmvnc-on-nixos/index.html",
    "title": "Compiling KasmVNC on NixOS",
    "section": "",
    "text": "This is a living document, for the time being. Rather than a complete blogpost, this is a tracker of my progress, as well as a quick reference I can come back to.\n\nWhat is this?\nKasmweb is a software to create remote desktops, that exist within docker containers, and allow users to access them, all from a browser based GUI.\nKasmvnc is the VNC server part of kasm web, a custom fork of previous existing features, enhanced with more performance, and most importantly, a web native setup. Incompatible with existing VNC protocols, the only way to access Kasmvnc is through the http/https serive it offers, the web based VNC ui.\nKasmVNC is an amazing piece of software, but development for it is not truly, fully public. This is present in their build system. Their build system is a series of bash scripts, that call docker containers, which call more bash scripts, to finally compile the software, and package it, all in one.\nAs part of the scripts they have to compile kasm, lots of static linking happens. They do this because not all distros package the most updated, performant versions of the libraries that kasmvnc uses.\nBut Nixos, the distro I want to package KasmVNC for, does. In addition to that, it is completely incompatible with the hacked together build system that kasm uses. In order to package KasmVNC for Nixos, I must reverse engineer their build system, and bit by bit, port it to NixOS.\n\n\nNixos Build System\nHow the nixos build system works. I will do later, but here I will document, step by step, how nixos builds a package.\n\n\nKasmVNC’s Build System\nReverse engineering KasmVNC’s build system.\nThe below is what I neeed for the compiliation commands to work. I don’t know where Kasm runs these commands, or similar equivalents, this is just what I’ve figured out.\nPerhaps I need to only run make in the KasmVNC/unix directory?\n\ngit clone https://github.com/TigerVNC/tigervnc\n\ncd tigervnc\n\ngit clone https://github.com/kasmtech/KasmVNC\n\ncd KasmVNC\n\ncp ..vncserver .vncserver # this sets up build environment. I will still need to check if every single one of these things are necessary, but this works for now\nThe build script can be found here\nBut from this build script, I don’t think all of it is necessary. Below, I will extract what commands are actually needed, from all the fluff, cruft, and hacks.\n\ncmake -D CMAKE_BUILD_TYPE=RelWithDebInfo . -DBUILD_VIEWER:BOOL=OFF \\\n  -DENABLE_GNUTLS:BOOL=OFF\n\nmake\nBuilds end up in KasmVNC/unix/\nExcept the preliminary builds don’t work. They error:\n~/vscode/tigervnc/KasmVNC/unix master ?1 ❯ ./vncserver \n\nCan't locate List/MoreUtils.pm in @INC (you may need to install the List::MoreUtils module) (@INC contains: /usr/lib/perl5/5.36/site_perl /usr/share/perl5/site_perl /usr/lib/perl5/5.36/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5/5.36/core_perl /usr/share/perl5/core_perl) at ./vncserver line 38.\nBEGIN failed--compilation aborted at ./vncserver line 38.\nThe above is probably because a perl library is missing. After attempting to install the missing library using pacman -S perl-list-moreutils I get a different error.\n\n~/vscode/tigervnc/KasmVNC/unix master ?1 ❯ ./vncserver \n\nCan't locate KasmVNC/CliOption.pm in @INC (you may need to install the KasmVNC::CliOption module) (@INC contains: /usr/lib/perl5/5.36/site_perl /usr/share/perl5/site_perl /usr/lib/perl5/5.36/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5/5.36/core_perl /usr/share/perl5/core_perl) at ./vncserver line 42.\nBEGIN failed--compilation aborted at ./vncserver line 42.\nObviously, this won’t work. I must figure out why KasmVNC pacakges perl packages, where it puts them by default, and how to package them for Nix.\nAlright, the vncserver command appears to be in the git repo, and rather than being a binary, it is a perl wrapper script to start an xvnc server. from the script:\nuse KasmVNC::CliOption;\nuse KasmVNC::ConfigKey;\nuse KasmVNC::PatternValidator;\nuse KasmVNC::EnumValidator;\nuse KasmVNC::Config;\nuse KasmVNC::Users;\nuse KasmVNC::TextOption;\nuse KasmVNC::TextUI;\nuse KasmVNC::Utils;\nuse KasmVNC::Logger;\nThese perl modules/libraries can be found in KasmVNC/unix/KasmVNC\nSo that is what is necessary for the vncserver script to run. But is this script really necessary? Based on the names of the perl libraries, this script might not be adding any core functionalities to kasmvnc, only things like additional command line options, or loggers.\nI need to find where this script runs kasmvnc, and also where the actual kasmvnc binary is.\nI think their fork of xvnc is located in KasmVNC/unix/xserver/hw/vnc/xvnc.c.\nBut I get compilation errors:\n\n[nix-shell:~/vscode/tigervnc/KasmVNC]$ make \n[  3%] Built target os\n[ 13%] Built target rdr\n[ 30%] Built target network\n[ 31%] Built target Xregion\n[ 78%] Built target rfb\n[ 80%] Built target tx\n[ 81%] Built target unixcommon\n[ 81%] Linking CXX executable vncconfig\n/nix/store/178vvank67pg2ckr5ic5gmdkm3ri72f3-binutils-2.39/bin/ld: cannot find -lturbojpeg: No such file or directory\ncollect2: error: ld returned 1 exit status\nmake[2]: *** [unix/vncconfig/CMakeFiles/vncconfig.dir/build.make:157: unix/vncconfig/vncconfig] Error 1\nmake[1]: *** [CMakeFiles/Makefile2:610: unix/vncconfig/CMakeFiles/vncconfig.dir/all] Error 2\nmake: *** [Makefile:136: all] Error 2\nI don’t know why this happens. For those who don’t know, make pretty much calls a set of scripts, called Makefiles. I will need to find where in these scripts, the error commands are run.\nWeirdly, I can’t reproduce outside of the build script:\n\n~/vscode/tigervnc/KasmVNC master ?1 ❯ ld -lsdsakdfj\nld: cannot find -lsdsakdfj: No such file or directory\n~/vscode/tigervnc/KasmVNC master ?1 ❯ ld -lturbojpeg\nld: warning: cannot find entry symbol _start; not setting start address\n~/vscode/tigervnc/KasmVNC master ?2 ❯ ld -ljpeg     \nld: warning: cannot find entry symbol _start; not setting start address\n~/vscode/tigervnc/KasmVNC master ?2 ❯ ld -ljpeg-turbo\nld: cannot find -ljpeg-turbo: No such file or directory\n~/vscode/tigervnc/KasmVNC master ?1 ❯ \nI suspect the make scripts have some hacks that change working directory, or otherwise hide my installation of libjpeg.\nWhen commenting out the part of the KasmVNC/Cmakelists.txt file that appears to be related to libjpeg, the error when I run make changes.\n[ 95%] Building CXX object unix/vncconfig/CMakeFiles/vncconfig.dir/vncconfig.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/Surface.cxx:23:10: fatal error: FL/Fl_RGB_Image.H: No such file or directory\n   23 | #include &lt;FL/Fl_RGB_Image.H&gt;\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake[2]: *** [tests/CMakeFiles/fbperf.dir/build.make:104: tests/CMakeFiles/fbperf.dir/__/vncviewer/Surface.cxx.o] Error 1\nmake[2]: *** Waiting for unfinished jobs....\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/Surface_X11.cxx:26:10: fatal error: FL/Fl_RGB_Image.H: No such file or directory\n   26 | #include &lt;FL/Fl_RGB_Image.H&gt;\n      |          ^~~~~~~~~~~~~~~~~~~\ncompilation terminated.\nmake[2]: *** [tests/CMakeFiles/fbperf.dir/build.make:118: tests/CMakeFiles/fbperf.dir/__/vncviewer/Surface_X11.cxx.o] Error 1\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:31:10: fatal error: FL/Fl.H: No such file or directory\n   31 | #include &lt;FL/Fl.H&gt;\n      |          ^~~~~~~~~\nThis is probably missing libraries.\nAfter installing fltk (pacman -S fltk), this error goes away, and I get a new, even harder to comprehend error.\n\n[ 97%] Building CXX object tests/CMakeFiles/decperf.dir/decperf.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx: In constructor ‘PlatformPixelBuffer::PlatformPixelBuffer(int, int)’:\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:64:29: error: ‘uint8_t’ was not declared in this scope\n   64 |   setBuffer(width, height, (uint8_t*)xim-&gt;data,\n      |                             ^~~~~~~\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:38:1: note: ‘uint8_t’ is defined in header ‘&lt;cstdint&gt;’; did you forget to ‘#include &lt;cstdint&gt;’?\n   37 | #include \"PlatformPixelBuffer.h\"\n  +++ |+#include &lt;cstdint&gt;\n   38 | \n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:64:37: error: expected primary-expression before ‘)’ token\n   64 |   setBuffer(width, height, (uint8_t*)xim-&gt;data,\n      |                                     ^\nI suspect I have the wrong version of the tigervnc source code/libraries. I will need to investigate where Kasm’s build system downloads these vncviewer libraries from.\nAfter editing the errored file, to include the library:\nWithin KasmVNC/vncviewer/PlatformPixelBuffer.cxx:\n\n#include &lt;cstdint&gt;\n\nI get a different error.\n\n[ 94%] Building CXX object tests/CMakeFiles/fbperf.dir/__/vncviewer/PlatformPixelBuffer.cxx.o\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx: In constructor ‘PlatformPixelBuffer::PlatformPixelBuffer(int, int)’:\n/home/moonpie/vscode/tigervnc/KasmVNC/vncviewer/PlatformPixelBuffer.cxx:66:3: error: ‘setBuffer’ was not declared in this scope; did you mean ‘setbuffer’?\n   66 |   setBuffer(width, height, (uint8_t*)xim-&gt;d﻿tion. \nI think it’s likely that I have the wrong version of tigervnc or something. I will try to see how Kasm’s build scripts set this up.\nOkay, I will attempt to create a visual graph, documenting each step of KasmVNC’s build system, to build an ubuntu package. I will have hyperlinks to each of the scripts/dockerfiles, or other pieces of the github repo. Currently still working on figuring out how to use Graphviz.\n\nEntrybuild-tarballBuildWWWdockerfile.ubuntu.build\n\n\n\n\n\n\n\n   \n\nEntry\n\n  Entry.  Starts out at KasmVNC/builder     \n\nBuildPackage\n\n  I will run the ‘builder/build-package ubuntu bionic’ command.   This isn’t the only command available, but for an example.     \n\nEntry-&gt;BuildPackage\n\n    \n\nBuildTarball\n\n  build-tarball ubuntu bionic     \n\nBuildPackage-&gt;BuildTarball\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildTarball\n\n  \n\nwww\n\n  if some condition, then   build and run dockerfile.www.build     \n\ndockerbuild\n\n  Build and run the appropiate Dockerfile,   which in this case dockerfile.ubuntu_bionic.build     \n\nwww-&gt;dockerbuild\n\n    \n\nincomplete\n\n ???, currrently in progress.   \n\nBuildTarball\n\n  build-tarball     \n\nBuildTarball-&gt;www\n\n    \n\ndockerbuild-&gt;incomplete\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildWWW\n\n  \n\nbuildwwwsh\n\n  ENTRYPOINT [ “/src/build_www.sh” ]     \n\nCOPY kasmweb/ /src/www/\n\n COPY kasmweb/ /src/www/   \n\nCOPY builder/build_www.sh /src/\n\n COPY builder/build_www.sh /src/   \n\nCOPY kasmweb/ /src/www/-&gt;COPY builder/build_www.sh /src/\n\n    \n\nWORKDIR /src/www\n\n WORKDIR /src/www   \n\nCOPY builder/build_www.sh /src/-&gt;WORKDIR /src/www\n\n    \n\nRUN npm install\n\n RUN npm install   \n\nWORKDIR /src/www-&gt;RUN npm install\n\n    \n\nRUN npm install-&gt;buildwwwsh\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBuildonUbuntu\n\n  \n\nEntry\n\n Entry here. This entire phase happens in a docker container   \n\ndevpendencies\n\n Install some dependencies. Notably, tightvncserver   \n\nEntry-&gt;devpendencies\n\n    \n\nMakeinstalls\n\n  build and install webp and libjpeg turbo     \n\ndevpendencies-&gt;Makeinstalls\n\n    \n\nInstall some more libs\n\n Install some more libs   \n\nMakeinstalls-&gt;Install some more libs\n\n    \n\nbuildsh\n\n  build.sh  This is the command the built docker container runs     \n\nInstall some more libs-&gt;buildsh\n\n   \n\n\n\n\n\n\n\n\n\n\n\nNote about dependencies install\n\n\n\n\n\nFor some reason they use multiple build phases for the same step of installing packages. In addition to that, they don’t clean the apt cache between build stages.\n\n\n\n\n\n\n\n\n\nNote about webp and libjpeg-turbo\n\n\n\n\n\nThey make and install — in the build dockerfile. I will end up skipping this step, as nix packages these, but why do they do that?\n\n\n\n\n\n\nI will expand on this, and organize it further. But based on these beginnings, kasmvnc appears to be a perl script that either starts a sepreate webserver or serves a webserver (if there is a perl native way to do this?), which appears to be based on novnc, while it also starts the vnc server, which is based on tigervnc.\nHowever, I am still confused about one thing: Where does it download the vnc server source code.\n\n\nThe easy way\nAfter I packaged quarto, I realized that I can actually package the kasmvnc binary using nix. I have decided to do this for now.\nHere is first attempt for this. Now, kasmvnc’s packaging system is weird, in that they do not offer a binary tarball for their packages. So, I have decided to convert their alpine package into something I can use, because based on a cursory look into all the packages, it seems to be the easiest to package for Nix/Nixos.\n\n\nFirst try!\n\n{\n    stdenv,\n    lib,\n    fetchurl,\n    makeWrapper,\n} :\n\nstdenv.mkDerivation rec {\n  pname = \"kasmvnc\";\n  version = \"1.1.0\";\n  src = fetchurl {\n    url = \"https://github.com/kasmtech/KasmVNC/releases/download/v${version}/kasmvnc.alpine_317_x86_64.tgz\";\n    sha256 = \"sha256-j/3PUwBd8XygBKYfFdAwN15cwxDPf3vbEwbLy1laxSU=\";\n  };\n\n  nativeBuildInputs = [\n  ];\n\n  patches = [\n  ];\n\n  postPatch = ''\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n  '';\n\n  installPhase = ''\n      runHook preInstall\n\n      mkdir -p $out/bin $out/share $out/man $out/etc $out/lib\n\n      echo here\n      ls\n      ls local/bin\n\n      mv local/etc/* $out/etc\n      mv local/share/* $out/share\n      mv local/man/* $out/man\n      mv local/lib/* $out/lib\n      mv local/bin/* $out/bin\n\n      runHook preInstall\n  '';\n\n  meta = with lib; {\n    description = \"Kasmvnc\";\n    longDescription = ''\n        Long description here\n    '';\n    homepage = \"\";\n    changelog = \"https://github.com/kasmtech/KasmVNC/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ moonpiedumplings ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nThis is my first attempt at a derivation. I’ve converted this one from the quarto derivation I built, which is also on my blog.\nI’m installing it using a simple shell.nix, that uses the nix callPackage function to build the kasmvnc nix package, and make it available in my current shell.\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n    kasmvnc = pkgs.callPackage ./kasmvnctest.nix {};\nin\n    pkgs.mkShell {\n        packages = [ kasmvnc ];\n    }\nTo run kasmvnc, I run the vncserver command.\nHowever, I get an error."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html",
    "href": "projects/openstack-on-nixos/index.html",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Hyperconverged infrastrucucture, is when multiple aspects of computing can all be managed from the same platform. This is usually done with virtualization, like virtualized servers, or virtualized storage.\nPromox Virtual Environment is one of the most popular examples of this for homelabbers, or people who manage their own servers for personal use. It offers a web based interface to configure virtual machines, virtualized storage, and clustering. When researching what software to use to manage my server, I considered proxmox.\nProxmox is based on debian linux, and is very tightly integrated into that ecosystem. It is nearly impossible to run proxmox on any other linux distro, and I disliked this inflexibility.\nOpenstack is an open source, public and private cloud solution, containing hyperconverged infrastructure, and more. It’s used when people don’t want to rely on external cloud solutions, like Amazon Web Services (AWS). For example, a university may decide that it is cheaper to manage and maintain their own cloud than to rely on AWS.\nOpenstack is massive, consisting of multiple components that must be installed and configured independently of eachother, yet set up to work with eachother. Becuase of this, openstack is usually deployed as configuration as code. The two most popular solutions, from my research, openstack-ansible, and kolla-ansible, work by deploying containerized, preconfigured installs of openstack, that connect to the bare metal portions of the system through standardized API’s like libvirt, which are easy to configure on the base system using ansible. On the other hand, because openstack is much more complex, it is easy to simply make a preconfigured container image, and distribute it out for people to use."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#what-is-hyperconverged-infrastrucuture",
    "href": "projects/openstack-on-nixos/index.html#what-is-hyperconverged-infrastrucuture",
    "title": "Packaging Openstack on Nixos",
    "section": "",
    "text": "Hyperconverged infrastrucucture, is when multiple aspects of computing can all be managed from the same platform. This is usually done with virtualization, like virtualized servers, or virtualized storage.\nPromox Virtual Environment is one of the most popular examples of this for homelabbers, or people who manage their own servers for personal use. It offers a web based interface to configure virtual machines, virtualized storage, and clustering. When researching what software to use to manage my server, I considered proxmox.\nProxmox is based on debian linux, and is very tightly integrated into that ecosystem. It is nearly impossible to run proxmox on any other linux distro, and I disliked this inflexibility.\nOpenstack is an open source, public and private cloud solution, containing hyperconverged infrastructure, and more. It’s used when people don’t want to rely on external cloud solutions, like Amazon Web Services (AWS). For example, a university may decide that it is cheaper to manage and maintain their own cloud than to rely on AWS.\nOpenstack is massive, consisting of multiple components that must be installed and configured independently of eachother, yet set up to work with eachother. Becuase of this, openstack is usually deployed as configuration as code. The two most popular solutions, from my research, openstack-ansible, and kolla-ansible, work by deploying containerized, preconfigured installs of openstack, that connect to the bare metal portions of the system through standardized API’s like libvirt, which are easy to configure on the base system using ansible. On the other hand, because openstack is much more complex, it is easy to simply make a preconfigured container image, and distribute it out for people to use."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#what-is-nixos",
    "href": "projects/openstack-on-nixos/index.html#what-is-nixos",
    "title": "Packaging Openstack on Nixos",
    "section": "What is Nixos?",
    "text": "What is Nixos?\nNixos is an operating system that uses the nix package manager to install packages, but also the nix language for configuration. Because of this, it is a form of configuration as code.\nFor example, I used nix’s ability to create a shell environment to create a shell environment with quarto on linux\nI asked, and searched around, looking for if there was a Nixos way to set up something like proxmox or openstack.\nI first asked, but then I realized that I could search github for the nix programming language, and keywords I desired. I did so, and I found someone’s lxdware configurations\n\n\nShow someone else’s configs for lxdware on nixos\n\n{ config, pkgs, lib, ... }: {\n  systemd.services.docker-create-network-lxdware = {\n    enable = true;\n    description = \"Create lxdware docker network\";\n    path = [ pkgs.docker ];\n    serviceConfig = {\n      Type = \"oneshot\";\n      RemainAfterExit = \"yes\";\n      ExecStart = pkgs.writeScript \"docker-create-network-lxdware\" ''\n        #! ${pkgs.runtimeShell} -e\n        ${pkgs.docker}/bin/docker network create lxdware || true\n      '';\n    };\n    after = [ \"network-online.target\" ];\n    wantedBy = [ \"multi-user.target\" ];\n  };\n\n  virtualisation.oci-containers.containers.\"lxdware\" = {\n    autoStart = true;\n    image = \"docker.io/lxdware/dashboard:latest\";\n    volumes = [ \"/services/lxdware/lxdware:/var/lxdware\" ];\n    dependsOn = [ \"create-network-lxdware\" ];\n    extraOptions = [\n      # networks\n      \"--network=lxdware\"\n      # labels\n      \"--label\"\n      \"traefik.enable=true\"\n      \"--label\"\n      \"traefik.docker.network=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.rule=Host(`lxd.local.bspwr.com`)\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.entrypoints=websecure\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls=true\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.tls.certresolver=letsencrypt\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.service=lxdware\"\n      \"--label\"\n      \"traefik.http.routers.lxdware.middlewares=local-allowlist@file, default@file\"\n      \"--label\"\n      \"traefik.http.services.lxdware.loadbalancer.server.port=80\"\n    ];\n  };\n}\n\nLxdware is a web based frontend for LXD, a type of hyperconverged infrastructure. LXD is a daemon, or background process, for managing containers (ran via LXC), virtual machines, and to an extent, virtualized storage. It appealed to me, when I was searching for a hyperconverged infrastructure solution for my home lab.\n\nIt’s feature rich, and mature. However, I dislike the particular implementation used in the above configuration. They ran lxdware, in a docker container. This works, and probably works well, but this isn’t a very nixos way of doing things. Nix offers reproducibility, so docker isn’t needed, and is generally frowned upon because it brings some disadvantages. I wanted to configure lxdware using nix myself.\nI later looked at openstack, because I wanted to see if there was an ideal way to configure this with nixos. Nix makes it so easy to configure so many other services, just a few lines of nix code in the configuration.nix file to set up webservers, or other services.\nIn fact, people even discussed this in a thread posted on the Nixos discourse forums\n\nIs there anyone actively working on being able to run an OpenStack cloud using NixOS? Shouldn’t “we” be able to do what the Kayobe project does but without the Ansible stuff? https://docs.openstack.org/kayobe/latest/configuration/reference/kolla-ansible.html 9\n\nOne person replied, saying that they had done a bit, but work had “rotten since”. And indeed, when I searched around github, I found an old project, in a github repo titled nixstack\nThis person, or group of people, had packaged openstack to be easy to enable. But as I read through the code, I realized that the app is from the era of python2, so this code is really old."
  },
  {
    "objectID": "projects/openstack-on-nixos/index.html#it-begins",
    "href": "projects/openstack-on-nixos/index.html#it-begins",
    "title": "Packaging Openstack on Nixos",
    "section": "It begins",
    "text": "It begins\nSo I decided to package it myself. At least, that’s my goal. It seems to be a massive project, but one user already did it, when I asked around on the discord.\n\nThey claimed they will open source it soon-ish, but right now, none of their work is public.\nSo I decided to do it myself. I have the old templates as my guides, and I know it is possible because someone else has done it.\nAfter going through the code for nixstack, I experienced a pleasant suprise — openstack is purely a python application, and this means that it can be packages by nix’s python packaging ecosystem.\nBecause openstack is purely a python application, all the dependencies are declared in the requirements.txt, located in the root of the git repo of each openstack component.\nNixpkgs very large, but I don’t know if they have all of the python packages. Luckily, I found a newer project, pip2nix, which seems to have been first created at around the same time the openstack on nix projects stopped development.\nI should be able to convert a requirements.txt into something nix can install natively, directly from the pypi repos, without having to worry about packages not being packages in nixpkgs.\nSo I started by cloning the openstack keystone repository from github: https://github.com/openstack/keystone\nI install and setup pip2nix and attempt to use it. It first complains about python36 being too old, but after I switch to the python39 version of pip2nix:\n[nix-shell:~/vscode/keystone]$ pip2nix generate .\nProcessing /home/moonpie/vscode/keystone\n... # lots of extraneous output\npip2nix generate seems to take the same inputs as pip, so to install a python package from the current working directory, you could usually do pip install ., but here I do a pip2nix generate . instead. This generates a python-packages.nix file with all the python packages, including openstack keystone.\nI played around with trying to install openstack keystone in a nix-shell environment.\n{\n        pkgs ? import &lt;nixpkgs&gt; {}\n} : \nwith import ./python-packages.nix {inherit pkgs;};\nlet\n        keystone-nix = callPackage keystone;\nin\npkgs.mkShell {\n        packages = [ keystone-nix ];\n}\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       error: function 'anonymous lambda' called without required argument 'fetchurl'\n\n       at /home/moonpie/vscode/keystone/python-packages.nix:4:1:\n\n            3|\n            4| { pkgs, fetchurl, fetchgit, fetchhg }:\n             | ^\n            5|\n\nI remember getting a similar error when I was trying to package quarto, and I was told to use callPackage. I don’t think I am using callPackage correctly here.\nI tried tinkering with some code that I got from searching github\n{\n    pkgs ? import &lt;nixpkgs&gt; {}\n} : \nwith pkgs;\nlet\n  python = python3;\n  pythonPackages = python.pkgs;\n\n  # generated with `pip2nix generate -r requirements.txt`\n  generatePipPackages = import ./python-packages.nix {\n    inherit pkgs;\n    inherit (pkgs) fetchurl fetchgit fetchhg;\n  };\n\n  pipPackages =  generatePipPackages pipPackages pythonPackages;\nin\npkgs.mkShell {\n    packages = with pipPackages; [ keystone ];\n}\nBut this errors:\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … while calling the 'derivationStrict' builtin\n\n         at /builtin/derivation.nix:9:12: (source not available)\n\n       … while evaluating derivation 'nix-shell'\n         whose name attribute is located at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:303:7\n\n       … while evaluating attribute 'nativeBuildInputs' of derivation 'nix-shell'\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/stdenv/generic/make-derivation.nix:347:7:\n\n          346|       depsBuildBuild              = lib.elemAt (lib.elemAt dependencies 0) 0;\n          347|       nativeBuildInputs           = lib.elemAt (lib.elemAt dependencies 0) 1;\n             |       ^\n          348|       depsBuildTarget             = lib.elemAt (lib.elemAt dependencies 0) 2;\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: attribute 'setuptools' missing\n\n       at /home/moonpie/vscode/keystone/python-packages.nix:106:7:\n\n          105|     propagatedBuildInputs = [\n          106|       self.\"setuptools\"\n             |       ^\n          107|       self.\"six\"\n\nI don’t really know why this errors.\nSo I did some more research, and looked at the pip2nix docs (which I was suprised they had because nix isn’t the best documented). According to that, you can use pip2nix scaffold --package projectName to generate a default.nix which nix-shell will read and convert into a shell environment for you.\nSo I did that. And I got the same error as above. I then did some searching on github for a similar issue, and found one issue where a replier said that the user had to add Cython to the requirements.txt, otherwise pip2nix would not add it to the package set. So I tried adding setuptools to the end of the requirements.txt, and pip2nix pretty much just ignored it, and didn’t add it to the generated python-packages.nix file. Why?\nSo I decied to add it to the generated python-packages.nix file manually, and I got an error, but that was probably because I had added setuptools = pkgs.python310Packages.setuptools when python39 was the version in use. This is because I didn’t realize nix packaged python39, I thought they only had 310/311, however, upon later trying to regenerate packages, I now get a completely different error.\n\n\nShow error\n\nmoonpie@localhost:~/vscode/keystone&gt; nix-shell\nerror:\n       … in the left operand of the update (//) operator\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/lib/fixed-points.nix:69:64:\n\n           68|   #\n           69|   extends = f: rattrs: self: let super = rattrs self; in super // f self super;\n             |                                                                ^\n           70|\n\n       … in the left operand of the update (//) operator\n\n         at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/lib/fixed-points.nix:69:64:\n\n           68|   #\n           69|   extends = f: rattrs: self: let super = rattrs self; in super // f self super;\n             |                                                                ^\n           70|\n\n       (stack trace truncated; use '--show-trace' to show the full trace)\n\n       error: function 'anonymous lambda' called with unexpected argument 'self'\n\n       at /nix/store/q300rswsxpr2kkng9azzsbfi9m8fdg50-nixpkgs/nixpkgs/pkgs/development/interpreters/python/passthrufun.nix:37:6:\n\n           36|     # - applies overrides from `packageOverrides` and `pythonPackagesOverlays`.\n           37|     ({ pkgs, stdenv, python, overrides }: let\n             |      ^\n           38|       pythonPackagesFun = import ./python-packages-base.nix {\n\nI literally changed nothing. Except for adding setuptools to the start of the requirements.txt, but I still got a differing error than the one I got yesterday.\nI recloned the keystone repo (reverting back to the default requirements.txt file), and regenerated the nix files using pip2nix, however I still get the same error."
  },
  {
    "objectID": "projects/quarto-via-nix/index.html",
    "href": "projects/quarto-via-nix/index.html",
    "title": "Packaging quarto using nix",
    "section": "",
    "text": "Skip to the conclusion if you want to know how to use the efforts of my hard work, although it only works on x6_64 linux.\n\nWhat is quarto?\nTo explain quarto, I first have to explain jupyter notebooks, and quarto’s predecessor, fastpages.\nFastpages is a blogging platform built on jekyll, a static site generator. Static sites are websites that do not connect to a backend server, the user’s browser does all the rendering. Static site generators do all the hard work of creating these, by converting a very human readable format, like markdown, to pretty looking html, which is what browsers render.\nFastpages adds onto the features of jekyll, by adding support for jupyter notebooks. Jupyter is a technology that allows users to combine code, multimedia, and text into a single document, for any purposes that one might use it. It’s usually popular for data science, as code is used to generate diagrams, but I really like it for testing snippets of code, as you can have multiple pieces of code in one document, and then run and debug them independently of eachother. Fastpagess can convert all of this, even automatically running code and creating any interactive elements, and then putting it all up on the internet as a static site.\nFastpages is also deprecated. On the github page, which is archived, it recommends you switch to quarto.\nQuarto adds more features on top of fastpages, while also removing some features. Fastpages is primarily designed for blogging, whereas quarto also has support for generating books, pdf’s, and websites.\nHowever, quarto lacks some features, as it uses it’s own static site generator, rather than jekyll. The biggest and most noticable one, is the Liquid template language that my computer science teacher uses to dynamically render his schedule page.\nBut for my purposes, quarto works fine. This blogpost you are reading, was generated using quarto.\n\n\nWhat is nix?\nNix is multiple things. Nix is a linux distribution, an package repository, a package manager, a programming language, and a configuration as code system.\nRight now, I am trying to use it as a package manager — specifically, to give myself the quarto tool.\nI’ve selected nix because it focuses on reproducible builds, across Mac, and Linux and x86_64, and arm64. This enables a multitude of devices to get packages with an identical configuration to me.\n\n\nUsing Nix\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno ];\n    }\nThis is a sample shell.nix file. If you run the nix-shell command line tool while in the same working directory, or using the filename as an argument, it will use this bit of nix code to create an shell environment for you.\nNix is a functional programming langauge. Unlike a language like python or java, where everything is an object, in nix, everything is a function. The colon : declares the arguments for a function. The ? declares a default argument for the variable, pkgs, to be used in the function. This is important, because without this declarion, the program does now know where to get packages from.\nThe above shell.nix works great. However, it installs an older version of quarto, 1.2, as only an older version of quarto is packaged in the nixpkgs repository. I want the newest version, 1.3.\nHere is the code used to create the quarto package, called a derivation:\n\n\nShow derivation\n\n\n{ stdenv\n, lib\n, pandoc\n, esbuild\n, deno\n, fetchurl\n, nodePackages\n, rWrapper\n, rPackages\n, extraRPackages ? []\n, makeWrapper\n, python3\n, extraPythonPackages ? ps: with ps; []\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"quarto\";\n  version = \"1.2.475\";\n  src = fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-oyKjDlTKt2fIzirOqgNRrpuM7buNCG5mmgIztPa28rY=\";\n  };\n\n  nativeBuildInputs = [\n    makeWrapper\n  ];\n\n  patches = [\n    ./fix-deno-path.patch\n  ];\n\n  postPatch = ''\n    # Compat for Deno &gt;=1.26\n    substituteInPlace bin/quarto.js \\\n      --replace 'Deno.setRaw(stdin.rid, ' 'Deno.stdin.setRaw(' \\\n      --replace 'Deno.setRaw(Deno.stdin.rid, ' 'Deno.stdin.setRaw('\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n    wrapProgram $out/bin/quarto \\\n      --prefix PATH : ${lib.makeBinPath [ deno ]} \\\n      --prefix QUARTO_PANDOC : ${pandoc}/bin/pandoc \\\n      --prefix QUARTO_ESBUILD : ${esbuild}/bin/esbuild \\\n      --prefix QUARTO_DART_SASS : ${nodePackages.sass}/bin/sass \\\n      --prefix QUARTO_R : ${rWrapper.override { packages = [ rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n      --prefix QUARTO_PYTHON : ${python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n  '';\n\n  installPhase = ''\n      runHook preInstall\n\n      mkdir -p $out/bin $out/share\n\n      rm -r bin/tools\n\n      mv bin/* $out/bin\n      mv share/* $out/share\n\n      runHook preInstall\n  '';\n\n  meta = with lib; {\n    description = \"Open-source scientific and technical publishing system built on Pandoc\";\n    longDescription = ''\n        Quarto is an open-source scientific and technical publishing system built on Pandoc.\n        Quarto documents are authored using markdown, an easy to write plain text format.\n    '';\n    homepage = \"https://quarto.org/\";\n    changelog = \"https://github.com/quarto-dev/quarto-cli/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ mrtarantoga ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nI don’t want to bore you with details, but in short, it downloads an older version of quarto than the newest.\n\n\nPackaging quarto\nNix is very poorly documented. The recommended way of getting help with nix is to ask for help on the discord. So that is what I did. The first thing I asked was how to get a newer version of quarto:\nI started out by asking how to update the version of the quarto package. I started out to do so on my own, by cloning the nixpkgs github repo, and attempting to build nixpkgs, but I couldn’t figure out how to build it at first, which is when I asked.\n\nWhich didn’t work, because I did not want to download the whole nixpkgs. I wanted to store the nix derivation to build. The answer: use an ovveride:\nI was then told to use the overrideAttrs function, which overrides specific attributes, essentially variables, of the derivation, another type of function, used to build the program.\nMy first attempt was not too good.\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\nA simple shell.nix that replaced the version attribute of quarto. I shared this excitedly, thinking I had figured this out on my own, only to be told that this change was purely cosmetic, and the new version of quarto wasn’t actually installed. And they were right.\n[nix-shell:~/vscode/quartotest]$ which quarto\n/nix/store/9qy0kpll3r755c6i0717405dilhffdrd-quarto-1.3.361/bin/quarto\nIt looks right, until you check deeper:\n[nix-shell:~/vscode/quartotest]$ quarto --version\n1.2.475\nSo a deeper override was needed. I needed to override the src attribute, which determines where to download the files used to package the application.\n\n\nSecond attempt!\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n            src = fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-VEYUEI4xzQPXlyTbCThAW2npBCZNPDJ5x2cWnkNz7RE=\";\n  };\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nBut this errored as well.\n~/vscode/quartotest master !4 ?1 ❯ nix-shell                                                                                                                         2m 56s\nerror: undefined variable 'fetchurl'\n\n       at /home/moonpie/vscode/quartotest/shell.nix:5:19:\n\n            4|             version = \"1.3.361\";\n            5|             src = fetchurl {\n             |                   ^\n            6|     url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-lin\nI was confused? Why did this error? I had copied exactly what was in the derivation used to build the package?\nLater, I figured out why. When a package is built, the dependencies are declared in the beginning of the package:\n{ \n  Dependencies_Here\n} : stdenv.mkDerivation.restofpackage\nIn nix, every single thing is a function. When creation a function in nix, the curly brackets before the function declare the arguments that the function will take:\nfunction = {arg1, arg2} : functionhere\nBut when creating a package, this syntax plays another role. The arguments of the function act as a dependency list, by declaring what packages are necessary to build the derivation. This prevents the build from being tainted by anything that is not explicitly declared. However, because overrides are not the same as derivations, they act differently.\nBut to get around this error when using the override function:\n\n\nSuccess!\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \nquarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n            version = \"1.3.361\";\n            src = pkgs.fetchurl {\n    url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n    sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n  };\n        });\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nAnd it worked:\n[nix-shell:/tmp/test]$ quarto --version\n1.3.361\nExcept it didn’t. When I actually tried to render my project:\n[nix-shell:~/vscode/quartotest]$ quarto render\n[1/4] about.qmd\nCould not find data file templates/styles.citations.html\nSince this file couldn’t be found on my system, I tried to find it on the internet.\nAnd find it I did, in the data-files section of the information about the pandoc 3.1 package\ndata-files:\n                 -- templates\n                 data/templates/styles.html\n                 data/templates/styles.citations.html\nFirst, I checked what version of the pandoc that Nix had in their repositories. They only had 2.1.9, which was too old for the version of quarto I had.\nBut just in case, I asked on the github discussions page for quarto. And yes, quarto 1.3, the version I wanted, did require pandoc 3.0, which nix did not have packaged.\nExcept it did, although the package wasn’t in the dependencies. So I first tried to install it independently, using the nix-shell -p package tool\n\n\nExcept I got an error\n\n~/vscode/test ❯ nix-shell -p haskellPackages.pandoc_3_1_2\nthis derivation will be built:\n  /nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv\nbuilding '/nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv'...\nsetupCompilerEnvironmentPhase\nBuild with /nix/store/4wjl91hrizxghwqy18a1337gq2y9mh40-ghc-9.2.7.\nunpacking sources\nunpacking source archive /nix/store/7ncxphrr3nff9jb3j4w9ksl6ggznqhm6-pandoc-3.1.2.tar.gz\nsource root is pandoc-3.1.2\nsetting SOURCE_DATE_EPOCH to timestamp 1000000000 of file pandoc-3.1.2/xml-light/Text/Pandoc/XML/Light/Types.hs\npatching sources\ncompileBuildDriverPhase\nsetupCompileFlags: -package-db=/build/tmp.yaRUozaznX/setup-package.conf.d -j16 +RTS -A64M -RTS -threaded -rtsopts\n[1 of 1] Compiling Main             ( /nix/store/4mdp8nhyfddh7bllbi7xszz7k9955n79-Setup.hs, /build/tmp.yaRUozaznX/Main.o )\nLinking Setup ...\nconfiguring\nconfigureFlags: --verbose --prefix=/nix/store/7983f3r6gpgvf17dn1k2c05wma708xdn-pandoc-3.1.2 --libdir=$prefix/lib/$compiler --libsubdir=$abi/$libname --datadir=/nix/store/zdc55i48g6hpbwckiwk6s6iraf30hh99-pandoc-3.1.2-data/share/ghc-9.2.7 --with-gcc=gcc --package-db=/build/tmp.yaRUozaznX/package.conf.d --ghc-options=-j16 +RTS -A64M -RTS --disable-split-objs --enable-library-profiling --profiling-detail=exported-functions --disable-profiling --enable-shared --disable-coverage --enable-static --disable-executable-dynamic --enable-tests --disable-benchmarks --enable-library-vanilla --disable-library-for-ghci --ghc-option=-split-sections -f-trypandoc --extra-lib-dirs=/nix/store/4g9phbpakh51bbw2n391vipz9r5z56kw-ncurses-6.4/lib --extra-lib-dirs=/nix/store/mnq0hqsqivdbaqzmzc287l0z9zw8dp15-libffi-3.4.4/lib --extra-lib-dirs=/nix/store/0ssnwyy41aynhav7jr4dz1y01lfzi86f-gmp-with-cxx-6.2.1/lib\nUsing Parsec parser\nConfiguring pandoc-3.1.2...\nSetup: Encountered missing or private dependencies:\ndoctemplates &gt;=0.11 && &lt;0.12,\ngridtables &gt;=0.1 && &lt;0.2,\njira-wiki-markup &gt;=1.5.1 && &lt;1.6,\nmime-types &gt;=0.1.1 && &lt;0.2,\npandoc-types &gt;=1.23 && &lt;1.24,\ntexmath &gt;=0.12.7 && &lt;0.13\n\nerror: builder for '/nix/store/63pnk32wsdczfk3nkl071w9y69yy5wmi-pandoc-3.1.2.drv' failed with exit code 1\n\nExcept all the dependencies that the error message wanted, existed in nixpkgs.\nHere’s doctemplates\nHere’s gridtables\nAnd so on. It was like, even though pandoc required these packages, it couldn’t see them. They all had the format packagename_version, as opposed to simply packagename, which would be an older package.\nSo I asked on discord, again.\n\nThis one user, NobbZ, helps people so much that people joke that he is the documentation.\nI tried their solution, and it didn’t work. Apparently, this solution was designed for the newer feature of nix, flakes, which I wasn’t using.\nBut with some adjustment, I managed to figure out how to use the override feature on my own, with the same solution that NobbZ sent me seconds later:\n\n\nTrying to get override working\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n   /* pandoc = pkgs.haskellPackages.callCabal2nix \"pandoc\" (fetchTarball {\n        url = \"https://github.com/jgm/pandoc/archive/refs/tags/3.1.2.tar.gz\";\n        sha256 = \"1h928w4ghbxg5whq7d9nkrfll2abvmbkc45adfgv35rfhcpkiiv9\";\n    }) {};*/\n    doctemplates = pkgs.haskellPackages.doctemplates_0_11;\n    gridtables = pkgs.haskellPackages.gridtables_0_1_0_0;\n    jira-wiki-markup = pkgs.haskellPackages.jira-wiki-markup_1_5_1;\n    mime-types = pkgs.haskellPackages.mime-types_0_1_1_0;\n    pandoc-types = pkgs.haskellPackages.pandoc-types_1_23;\n    texmath = pkgs.haskellPackages.texmath_0_12_7_1;\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.override {inherit doctemplates gridtables jira-wiki-markup mime-types pandoc-types texmath;};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter pandoc deno mkpasswd ];\n    }\n\nExcept this errors:\nWarning:\n    This package indirectly depends on multiple versions of the same package. This is very likely to cause a compile failure.\n      package http-client (http-client-0.7.13.1-52kzOBAMbxmJrzoQZgatPf) requires mime-types-0.1.0.9-Gdz1G1mhqziCfo3C8KZHz7\n      package pandoc (pandoc-3.1.2) requires mime-types-0.1.1.0-4FUch8wD40c6kQtGdyJOSM\n      package texmath (texmath-0.12.7.1-BbrGid5okuSI4hfeGBAcF8) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package commonmark-pandoc (commonmark-pandoc-0.2.1.3-OwUzhyyJ0cDzxfYXzbAci) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package citeproc (citeproc-0.8.1-LP74PTBZCEoHiNCfXfUYdM) requires pandoc-types-1.22.2.1-1cCcarshT2W3DaxppqWytd\n      package pandoc (pandoc-3.1.2) requires pandoc-types-1.23-AC7tSm0fcRIGMZsmro9kaK\n      package pandoc (pandoc-3.1.2) requires pandoc-types-1.23-AC7tSm0fcRIGMZsmro9kaK\n** abort because of serious configure-time warning from Cabal\nerror: builder for '/nix/store/ibawyigbdn9bs1gs9hc0mgzqraqfxhy0-pandoc-3.1.2.drv' failed with exit code 1\nEssentially, a dependency error. Doctemplates also required a package that wasn’t under the default of packagename, but rather packagename_version.\nAnother user proceeded to chime in with their solution:\n\n\nSee cdepillabout’s solution\n\nlet\n  pkgs = import &lt;nixpkgs&gt; {};\n\n  pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n    doctemplates = hprev.doctemplates_0_11;\n    gridtables = hprev.gridtables_0_1_0_0;\n    jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n    mime-types = hprev.mime-types_0_1_1_0;\n    pandoc-types = hprev.pandoc-types_1_23;\n    texmath = hprev.texmath_0_12_7_1;\n  });\nin\n  pkgs.mkShell {\n    packages = [pandoc];\n  }\n\nAnd this worked! Except it didn’t.\n[nix-shell:~/vscode/quartotest]$ pandoc\nbash: pandoc: command not found\n\n[nix-shell:~/vscode/quartotest]$ which pandoc\nwhich: no pandoc in (/nix/store/kbcrs84s1x8yd5bp1nq6q6ihda8nd2lp-bash-interactive-5.2-p15/bin:/nix/store/a9q4y7vw1fgs990bs5mpd3p50mc0iz27-python3-3.10.11/bin:/nix/store/nh8iz5l2zn5nbk19qxdw575a5fhfcajw-quarto-1.3.361/bin:/nix/store/ar2lzr4kr4pi1zgx3w8hl6fkny3bql53-python3.10-notebook-6.5.2/bin:/nix/store/ai5lxg5vzjsfk9zkyn65ndq81na2mm5c-python3.10-babel-2.12.1/bin:/nix/store/95cxzy2hpizr23343b8bskl4yacf4b3l-python3-3.10.11/bin:/nix/store/5ii8sm9yh01ny05bl1wjdv6pkdjb8bw0-python3.10-jupyter-core-5.2.0/bin:/nix/store/pkgr71n4dy7h9lp00paf6k3llfa95ig0-python3.10-Send2Trash-1.8.1b0/bin:/nix/store/x1kk4hlx0zl12igvr6v0pk2cq2720fbh-python3.10-jupyter_client-8.0.3/bin:/nix/store/9icvaw0dgk7258m564xlh513nz6xis1m-python3.10-nbformat-5.7.3/bin:/nix/store/6svh49hf9pq5hwavgyb642v5a0pjnn4a-python3.10-jsonschema-4.17.3/bin:/nix/store/15jn0r39wg0ripjzjfxj9arcv53qxck9-python3.10-nbclassic-0.5.2/bin:/nix/store/jjy30kw6pw2mq54ig6lrm84ds91a9snf-python3.10-ipython-8.11.0/bin:/nix/store\nApparently, in pandoc 3.0, the binary and the library have been split into two seperate packages. In nixpkgs, the library can be found in the haskellPackages.pandoc_3_1_2, and the binary can be found in haskellPackages.pandoc-cli.\nnix-shell -p haskellPackages.pandoc-cli\n\nSetup: Encountered missing or private dependencies:\ndoctemplates &gt;=0.11 && &lt;0.12, pandoc &gt;=3.0\n...\nerror: builder for '/nix/store/hzzqnffj08r9qc0xi3b4ydi7w91dn4m0-pandoc-server-0.1.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/arbq7vgw539xyd4l0y5x3jyhhra30v91-pandoc-cli-0.1.1.drv' failed to build\nThe pandoc-cli package is broken, for the exact same error that the pandoc library won’t compile for.\n\n\nSo I try my own override:\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n        doctemplates = hprev.doctemplates_0_11;\n        gridtables = hprev.gridtables_0_1_0_0;\n        jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n        mime-types = hprev.mime-types_0_1_1_0;\n        pandoc-types = hprev.pandoc-types_1_23;\n        texmath = hprev.texmath_0_12_7_1;\n      });\n    pandoc-cli = pkgs.haskellPackages.pandoc-cli.overrideScope (hfinal: hprev: {\n        hslua-core = hprev.hslua-core_2_3_1;\n        lua = hprev.lua_2_3_1;\n    });\nin\n    pkgs.mkShell {\n        packages = [ pkgs.python310Full quarto pkgs.jupyter pandoc pandoc-cli pkgs.deno ];\n    }\n\nBut this also errors:\n\n\nShow error\n\nUsing Parsec parser\nConfiguring tasty-hslua-1.0.2...\n\nSetup: Encountered missing or private dependencies:\nhslua-core &gt;=2.0 && &lt;2.3\n\nerror: builder for '/nix/store/nmhz8xnc24xw86q573v515n3q8m9l0y5-tasty-hslua-1.0.2.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/8q4ni7s2am50xbbbkcdjk85szyvq3jk8-hslua-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/hcysycpqkqv4kd3qmkwzyi7pkaqszqyy-hslua-marshalling-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/h4c32n03d305njipsiw4rzc8rq52l2bc-hslua-packaging-2.2.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/y655vw1bdq8a9j818k16y7228nlsf86y-hslua-cli-1.4.1.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/xkkp4cj1yfwjpczc6k7y08gxdqdbfh4n-pandoc-2.19.2.drv' failed to build\nerror: 1 dependencies of derivation '/nix/store/sc44mnc1ngxfxi7h3f6qrrvnvldla4w3-pandoc-cli-0.1.1.drv' failed to build\n~/vscode/quartotest master !4 ?9 ❯                                                                                        \n\nBut the hslua-core version I want was packaged in nixpkgs, similar to doctemplates, or gridtables. So I did a further override.\n\n\nShow/hide\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet\n    quarto = pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n    });\n    pandoc = pkgs.haskellPackages.pandoc_3_1_2.overrideScope (hfinal: hprev: {\n        doctemplates = hprev.doctemplates_0_11;\n        gridtables = hprev.gridtables_0_1_0_0;\n        jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n        mime-types = hprev.mime-types_0_1_1_0;\n        pandoc-types = hprev.pandoc-types_1_23;\n        texmath = hprev.texmath_0_12_7_1;\n        tasty-hslua = hprev.tasty-hslua_1_1_0;\n        hslua-marshalling = hprev.hslua-marshalling_2_3_0;\n        hslua-aeson = hprev.hslua-aeson_2_3_0_1;\n        hslua = hprev.hslua_2_3_0;\n      });\n    pandoc-cli = pkgs.haskellPackages.pandoc-cli.overrideScope (hfinal: hprev: {\n        hslua-core = hprev.hslua-core_2_3_1;\n        lua = hprev.lua_2_3_1;\n        tasty-hslua = hprev.tasty-hslua_1_1_0;\n        hslua-marshalling = hprev.hslua-marshalling_2_3_0;\n        hslua-aeson = hprev.hslua-aeson_2_3_0_1;\n        hslua = hprev.hslua_2_3_0;\n    });\nin\n    pkgs.mkShell {\n        packages = [ pkgs.python310Full quarto pkgs.jupyter pandoc pandoc-cli pkgs.deno ];\n    }\n\nWhich still errors:\nUsing Parsec parser\nConfiguring hslua-typing-0.1.0...\n\nSetup: Encountered missing or private dependencies:\nhslua-core &gt;=2.3 && &lt;2.4, hslua-marshalling &gt;=2.3 && &lt;2.4\n\nerror: builder for '/nix/store/kbmfxjy0ycwwg6r6zsp9q9v1pfkmggnw-hslua-typing-0.1.0.drv' failed with exit code 1\nerror: 1 dependencies of derivation '/nix/store/nj4smnyrkaf52qx98r1wa0r1gdnjbwxk-hslua-2.3.0.drv' failed to build\nI quickly realized, that the updated set of haskell packages in nixpkgs, is broken all the way down. I found a relevant github issue. In this issue, somene had modified the derivation of the haskellPackages, to get pandoc-cli to work.\nI used their fork of nixpkgs to give myself pandoc-cli.\n~/vscode/quartotest master +3 !1 ❯ nix-shell -p haskellPackages.pandoc-cli -I nixpkgs=https://github.com/seam345/nixpkgs/archive/89e6e477c8357a087e863db562d2fa8d9fe5ba29.tar.gz\n[nix-shell:~/vscode/quartotest]$ pandoc --version\npandoc 3.1\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\nThis actually worked, and I got pandoc-cli with the 3.0 version of pandoc. However, I couldn’t get quarto to use pandoc-cli rather than the normal pandoc version, so quarto still wasn’t working.\nLater, cdepillabout chimed in again. Here is their solution:\n\n\nShow/hide\n\nlet\n  nixpkgs-src = fetchTarball {\n    # nixpkgs-unstable as of 2023-05-31\n    url = \"https://github.com/NixOS/nixpkgs/archive/58c85835512b0db938600b6fe13cc3e3dc4b364e.tar.gz\";\n    sha256 = \"0bkhaiaczj25s6hji2k9pm248jhfbiaqcfcsfk92bbi7kgzzzpif\";\n  };\n\n  my-overlay = final: prev: {\n\n    pandoc_1_3 =\n      let\n        inherit (final.haskell.lib.compose) disableCabalFlag markUnbroken;\n      in\n      final.lib.pipe\n        final.haskellPackages.pandoc-cli\n        [\n          markUnbroken\n          (disableCabalFlag \"lua\")\n          (p: p.overrideScope (hfinal: hprev: {\n            doctemplates = hprev.doctemplates_0_11;\n            gridtables = hprev.gridtables_0_1_0_0;\n            hslua-cli = null;\n            jira-wiki-markup = hprev.jira-wiki-markup_1_5_1;\n            mime-types = hprev.mime-types_0_1_1_0;\n            pandoc = hprev.pandoc_3_1_2;\n            pandoc-lua-engine = null;\n            pandoc-server = markUnbroken hprev.pandoc-server;\n            pandoc-types = hprev.pandoc-types_1_23;\n            texmath = hprev.texmath_0_12_7_1;\n          }))\n        ];\n\n    quarto_1_3 =\n      let\n        quarto-version = \"1.3.361\";\n      in\n      (final.quarto.override { pandoc = final.pandoc_1_3; }).overrideAttrs (oldAttrs: {\n        version = quarto-version;\n        src = final.fetchurl {\n          url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${quarto-version}/quarto-${quarto-version}-linux-amd64.tar.gz\";\n          sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n      });\n  };\n\n  pkgs = import nixpkgs-src { overlays = [ my-overlay ]; };\n\nin\n\npkgs.quarto_1_3\n\nAlthough I modified it a bit, to be\n...\nin\npkgs.mkShell {\n  packages = [pkgs.quarto_1_3];\n}\nBecause it is the mkShell package that creates the shell environment.\nThis solution works, and gets me quarto 1.3, and also replaces the default dependency of quarto, on pandoc 2.1.9, with one on pandoc-cli.\n[nix-shell:~/vscode/quartotest]$ quarto pandoc --version\npandoc 3.1.2\nFeatures: +server -lua\nScripting engine: none\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\nHowever, it has a caveat. It compiles pandoc without lua support, as those packages where the ones that were broken in nixpkgs.\nBut apparently, quarto needs lua suppport.\n[nix-shell:~/vscode/quartotest]$ quarto render\n[1/9] about.qmd\nThis version of pandoc has been compiled without Lua support.\nSo yeah. That doesn’t work.\nBut meanwhile, the quarto team responded to a question I asked on their github discussion page. I had asked if quarto requires either the pandoc binary, or the pandoc library.\nThey replied, and said that a pandoc binary, is actually included inside the tarball and the packages they have created. As part of the build system, the package is fairly self reliant, not needing much in terms of external dependencies.\nSo that’s what I did.\nI modified the derivation so that it uses the built in pandoc, rather than replacing it with an external one.\n\n\nShow/hide\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        patches = [];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : ${pkgs.nodePackages.sass}/bin/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nAnd this works:\n[nix-shell:/tmp/test]$ quarto pandoc --version\npandoc 3.1.1\nFeatures: +server +lua\nScripting engine: Lua 5.4\nUser data directory: /home/moonpie/.local/share/pandoc\nCopyright (C) 2006-2023 John MacFarlane. Web:  https://pandoc.org\nThis is free software; see the source for copying conditions. There is no\nwarranty, not even for merchantability or fitness for a particular purpose.\n\n[nix-shell:/tmp/test]$ quarto render\n[1/4] about.qmd\n[2/4] posts/post-with-code/index.qmd\n[3/4] posts/welcome/index.qmd\n[4/4] index.qmd\n\nOutput created: _site/index.html\n\n\n[nix-shell:/tmp/test]$ \nExcept not really:\n[nix-shell:/tmp/test]$ quarto check\n\n[✓] Checking versions of quarto binary dependencies...\n      Pandoc version 3.1.1: OK\nERROR: TypeError: Invalid Version: 1.62.1 compiled with dart2js 2.19.6\n\nTypeError: Invalid Version: 1.62.1 compiled with dart2js 2.19.6\n    at new SemVer (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:48564:19)\n    at Range.test (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:48974:23)\n    at satisfies (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:49191:18)\n    at checkVersion (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104009:14)\n    at checkVersions (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104035:5)\n    at async check (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:103989:13)\n    at async Command.fn (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:104212:5)\n    at async Command.execute (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:8437:13)\n    at async quarto (file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:127540:5)\n    at async file:///nix/store/bbb70ala41gczl37hmcfy1fx6dldw57l-quarto-1.3.361/bin/quarto.js:127558:9\nSo I did some experimenting. First, I replaced all the dependencies with the versions that came with the pandoc package, and then I didn’t get this error with the quarto check command, which checks the installation of quarto, python, pandoc, and R. I then removed the dependencies on by one, to see which one broke it. It ended up being dart-sass.\nSo I tried again:\n\n\nShow shell.nix\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        patches = [];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python3\n        '';\n        installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nAnd this works. The only thing that goes wrong is it gives me a warning when I use the preview function.\nWARNING: Specified QUARTO_PYTHON '/nix/store/xs35q9yb940cxsy1y0qcs84239zmd2jn-python3-3.10.11-env/bin/python3:/bin/python' does not exist.\nI’ve found no wayh to get rid of this warning, and since it is just a warning, I will ignore it. Here is my new shell.nix.\n\n\nShow shell.nix\n\n\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\n{ pkgs ? import &lt;nixpkgs&gt; {} } :\nlet \n    pandoc = null;\n\n    extraRPackages = [];\n    extraPythonPackages = ps: with ps; [];\n\n\n    quarto = (pkgs.quarto.overrideAttrs (oldAttrs: rec {\n        version = \"1.3.361\";\n        src = pkgs.fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n        buildInputs = with pkgs; [ python3 jupyter ];\n        preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${pkgs.lib.makeBinPath [ pkgs.deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${pkgs.esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${pkgs.rWrapper.override { packages = [ pkgs.rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${pkgs.python3}/bin/python3\n        '';\n        installPhase = ''\n            echo \"this is the quarto python ${pkgs.python3.withPackages (ps: with ps; [ jupyter ipython ] ++ (extraPythonPackages ps))}/bin/python\"\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n    })).override {inherit pandoc extraPythonPackages extraRPackages;};\nin\n    pkgs.mkShell {\n\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\nHowever, this is messy. Fitting an entire set of overrides into a single shell.nix file is definitely not the neatest way to do this. And there are some other flaws, like things that aren’t necessary as a dependency. Fortunately, there is a neater way.\nThe nix callPackage function allows for a nix function to call it’s own derivation. Rather than using an override, I can write my own derivation and use the callPackage fucnction to call upon it.\nHere is my derivation:\n\n\nShow derivation:\n\n{ stdenv\n, lib\n, esbuild\n, deno\n, fetchurl\n, nodePackages\n, rWrapper\n, rPackages\n, extraRPackages ? []\n, makeWrapper\n, python3\n, extraPythonPackages ? ps: with ps; []\n}:\n\nstdenv.mkDerivation rec {\n  pname = \"quarto\";\n  version = \"1.3.361\";\n        src = fetchurl {\n            url = \"https://github.com/quarto-dev/quarto-cli/releases/download/v${version}/quarto-${version}-linux-amd64.tar.gz\";\n            sha256 = \"sha256-vvnrIUhjsBXkJJ6VFsotRxkuccYOGQstIlSNWIY5nuE=\";\n        };\n\n  nativeBuildInputs = [\n    makeWrapper\n  ];\n\n  patches = [\n    ./fix-deno-path.patch\n  ];\n\n  postPatch = ''\n    # Compat for Deno &gt;=1.26\n    substituteInPlace bin/quarto.js \\\n      --replace 'Deno.setRaw(stdin.rid, ' 'Deno.stdin.setRaw(' \\\n      --replace 'Deno.setRaw(Deno.stdin.rid, ' 'Deno.stdin.setRaw('\n  '';\n\n  dontStrip = true;\n\n  preFixup = ''\n            wrapProgram $out/bin/quarto \\\n            --prefix PATH : ${lib.makeBinPath [ deno ]} \\\n            --prefix QUARTO_PANDOC : $out/bin/tools/pandoc \\\n            --prefix QUARTO_ESBUILD : ${esbuild}/bin/esbuild \\\n            --prefix QUARTO_DART_SASS : $out/bin/tools/dart-sass/sass \\\n            --prefix QUARTO_R : ${rWrapper.override { packages = [ rPackages.rmarkdown ] ++ extraRPackages; }}/bin/R \\\n            --prefix QUARTO_PYTHON : ${python3}/bin/python3\n    '';\n\n  installPhase = ''\n            runHook preInstall\n\n            mkdir -p $out/bin $out/share\n\n            mv bin/* $out/bin\n            mv share/* $out/share\n\n            runHook preInstall\n            '';\n\n  meta = with lib; {\n    description = \"Open-source scientific and technical publishing system built on Pandoc\";\n    longDescription = ''\n        Quarto is an open-source scientific and technical publishing system built on Pandoc.\n        Quarto documents are authored using markdown, an easy to write plain text format.\n    '';\n    homepage = \"https://quarto.org/\";\n    changelog = \"https://github.com/quarto-dev/quarto-cli/releases/tag/v${version}\";\n    license = licenses.gpl2Plus;\n    maintainers = with maintainers; [ mrtarantoga ];\n    platforms = [ \"x86_64-linux\" ];\n    sourceProvenance = with sourceTypes; [ binaryNativeCode binaryBytecode ];\n  };\n}\n\nAnd here is the shell.nix that summons this package:\nlet\n    pkgs = import &lt;nixpkgs&gt; {};\n    quarto = pkgs.callPackage ./env/quarto.nix {};\nin\n    pkgs.mkShell {\n        packages = with pkgs; [ python310Full quarto jupyter ];\n    }\n\n\nConclusion and How to Use\nNow, users who have cloned the repo for this blog, can simply install nix, and run nix-shell in the root directory of the repo in order for them to get quarto, python, and jupyter, the dependencies I rely on for this project. Because of the way nix, works it is easy to modify my shell.nix, and add more dependencies, like R or more extra language support via juptyer kernels.\nMy usual workflow is to open a terminal in this git repo, and type nix-shell, and then code ., which gives me vscode with quarto (I have the quarto vscode extension installed), python, and jupyter.\nI realized in hindsight that this only works on x86_64 linux, because my derivation works by taking the quarto x86_64 linux binary and packaging it using nix. But I did learn a lot about writing derivations through this, which I can apply to other things.\n\n\nTry 2, creating a multi architechture and multi OS package\nVery frustrating, quarto has no compilation instructions in the readme. However, thankfully, use github actions to do testing, and builds of the applications, which means that the build steps are technically public, just not immediately apparent.\n\nGithub actions works by reading a .yml file for the instuctions on what do do, and this file can be found in the github actions menu, as workflow file.\nI don’t need the whole thing, however, just the tarball building steps.\n\n\nShow steps\n\nobs:\n  configure:\n    runs-on: ubuntu-latest\n    outputs:\n      version: ${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number}}\n      version_base: ${{steps.config.outputs.version}}\n      tag_name: v${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number }}\n      release: v${{steps.config.outputs.version}}.${{ steps.config.outputs.build_number }}\n      changes: ${{ steps.config.outputs.changes }}\n    if: github.event_name != 'schedule' || (github.event_name == 'schedule' && github.repository == 'quarto-dev/quarto-cli')\n    steps:\n      - name: Install libc6-div\n        run: sudo apt-get install libc6-dev\n\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n\n      - name: config\n        id: config\n        run: |\n          source ./configuration\n          CHANGES=\n          # CHANGES=$(git log $(git describe --tags --abbrev=0)..HEAD --oneline)\n          # Escape \\n, \\r to preserve multiline variable\n          # See https://github.community/t/set-output-truncates-multiline-strings/16852/2\n          # CHANGES=\"${CHANGES//'%'/'%25'}\"\n          # CHANGES=\"${CHANGES//$'\\n'/'%0A'}\"\n          # CHANGES=\"${CHANGES//$'\\r'/'%0D'}\"\n          # echo \"changes=$CHANGES\" &gt;&gt; $GITHUB_OUTPUT\n          QUARTO_BUILD_NUMBER=$(($QUARTO_BUILD_RUN_OFFSET + $GITHUB_RUN_NUMBER))\n          echo \"version=$QUARTO_VERSION\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"changes=$CHANGES\" &gt;&gt; $GITHUB_OUTPUT\n          echo \"build_number=$QUARTO_BUILD_NUMBER\" &gt;&gt; $GITHUB_OUTPUT\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: News\n          path: ./news/changelog-${{steps.config.outputs.version}}.md\n\n  make-source-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Make Tarball\n        run: |\n          tar -zcvf  quarto-${{needs.configure.outputs.version}}.tar.gz *\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Source\n          path: ./quarto-${{needs.configure.outputs.version}}.tar.gz\n\n  make-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Configure\n        run: |\n          ./configure.sh\n      - name: Prepare Distribution\n        run: |\n          pushd package/src/\n          ./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --log-level info\n          popd\n      - name: Make Tarball\n        run: |\n          pushd package/\n          mv pkg-working quarto-${{needs.configure.outputs.version}}\n          tar -cvf  quarto-${{needs.configure.outputs.version}}-linux-amd64.tar quarto-${{needs.configure.outputs.version}}\n          gzip quarto-${{needs.configure.outputs.version}}-linux-amd64.tar\n          mv quarto-${{needs.configure.outputs.version}} pkg-working\n          popd\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Deb Zip\n          path: ./package/quarto-${{needs.configure.outputs.version}}-linux-amd64.tar.gz\n\n  make-arm64-tarball:\n    runs-on: ubuntu-latest\n    needs: [configure]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Configure\n        run: |\n          ./configure.sh\n      - name: Prepare Distribution\n        run: |\n          pushd package/src/\n          ./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --arch aarch64 --log-level info\n          popd\n      - name: Make Tarball\n        run: |\n          pushd package/\n          mv pkg-working quarto-${{needs.configure.outputs.version}}\n          tar -cvf  quarto-${{needs.configure.outputs.version}}-linux-arm64.tar quarto-${{needs.configure.outputs.version}}\n          gzip quarto-${{needs.configure.outputs.version}}-linux-arm64.tar\n          mv quarto-${{needs.configure.outputs.version}} pkg-working\n          popd\n      - name: Upload Artifact\n        uses: actions/upload-artifact@v3\n        with:\n          name: Deb Arm64 Zip\n          path: ./package/quarto-${{needs.configure.outputs.version}}-linux-arm64.tar.gz\n\nSo to convert github actions into somethign easier to read:\nAMD64 build:\nsource ./configuration\n./configure.sh\npushd package/src/ # pushd is like a more advnaced version of cd\n/quarto-bld prepare-dist --set-version ${{needs.configure.outputs.   version}} --log-level info \npopd\npushd package/\nmv pkg-working quarto-${{needs.configure.outputs.version}}\ntar -cvf  quarto-${{needs.configure.outputs.version}}-linux-amd64.tar quarto-${{needs.configure.outputs.version}}\ngzip quarto-${{needs.configure.outputs.version}}-linux-amd64.tar\nmv quarto-${{needs.configure.outputs.version}} pkg-working\npopd\nAnd then based on the github actions, the generated tarball is located at ./package/quarto-${{needs.configure.outputs.version}}-linux-amd64.tar.gz\nARM64 build (very similar steps):\n./configure.sh\npushd package/src/\n./quarto-bld prepare-dist --set-version ${{needs.configure.outputs.version}} --arch aarch64 --log-level info\npopd\npushd package/\nmv pkg-working quarto-${{needs.configure.outputs.version}}\ntar -cvf  quarto-${{needs.configure.outputs.version}}-linux-arm64.tar quarto-${{needs.configure.outputs.version}}\ngzip quarto-${{needs.configure.outputs.version}}-linux-arm64.tar\nmv quarto-${{needs.configure.outputs.version}} pkg-working\npopd\nAnd then based on the github actions, the generated file is located at ./package/quarto-${{needs.configure.outputs.version}}-linux-arm64.tar.gz\nI downloaded these files from releases, and they appear to be extremely similar. The quarto “binary” is a bash script that includes a check for architechture. So the only difference is in the configure step, which seems to download a different architechture.\nHowever, when experimenting with trying to compile quarto on my phone (arm) using termux, I get an error. deno: cannot execute: required file not found. So far, googling this error has gotten me nowhere. I may have to create an arm vm to test later."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html",
    "href": "projects/setting-up-kasm/index.html",
    "title": "Kasmweb setup",
    "section": "",
    "text": "School chromebooks cannot be used for computer science. Due to a content blocking setup that is more restrictive than the Great Firewall of China, school chromebooks cannot install the necessary digital tools for software development.\nCurrently, you must have your own device to be able to participate in computer science classes. Students who are unable to obtain their own device for whatever reason are denied participation.\nAlthough getting the chromebooks unlocked to be able to install software is a complex, potentially legal problem, there are alternatives.\nKasm is a remote desktop software. It runs a computer on a remote server, that can be accessed through a browser, or a chromebook. Because there are no restrictions on what can be installed when using Kasm, it makes it possible to use development tools on a chromebook. This blog post is me optimizing kasm so that it is more resource efficient, and also enabling software development tools to be easily installed on it.\nRoadblocks/steps:"
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "href": "projects/setting-up-kasm/index.html#turns-out-memory-deduplication-is-on-by-default-for-docker-containers",
    "title": "Kasmweb setup",
    "section": "Turns out, memory deduplication is on by default for docker containers",
    "text": "Turns out, memory deduplication is on by default for docker containers\nGithub issue where someone asked about this. The documents linked were very unclear, so I’ll break it down.\nIf you are using overlayfs or aufs, you have memory deduplication. If you are using other storage drivers, you sacrifice memory for more i/o (write/read) performance.\nFrom here:\n\nOn my ubuntu virtual machine, and the AWS ubuntu machine we are working on, Overlay2 is the storage driver:"
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#kernel-same-page-merging",
    "href": "projects/setting-up-kasm/index.html#kernel-same-page-merging",
    "title": "Kasmweb setup",
    "section": "Kernel Same page merging",
    "text": "Kernel Same page merging\nPreviously, I tried instructions from here: https://wiki.openvz.org/KSM_(kernel_same-page_merging). However, I noticed only a minimal space saved using the LD_PRELOAD steps. Not useful.\nI then tried cachyos fork of uksmd: https://github.com/CachyOS/uksmd, a daemon to go through userspace tasks and dedupe them.\nOnly works with a kernel that has the pmadv_ksm() syscall. Exists in most kernels optimized for desktop usage, like linux-zen, linux-liqourix, or pf-kernel (the original creators of uksmd)\nTo check if your currently running kernel has the feature:\n\non Archlinux, check if the files sys_enter_pmadv_ksm and sys_exit_pmadv_ksm exist in /sys/kernel/debug/tracing/events/syscalls (default does not have this feature, but linux-zen does)\non Ubuntu check if lines containing pmadv exist in the file /proc/kallsyms\n\n\n\n\nuksmstats\n\n\nHalf a gig of ram saved on a normal desktop. Expect to see much more when multiple almost identical docker containers are launched. Very useful. It saves a lot of ram. However, there might be a better way for docker, without jumping through hoops.\nDoes cost a miniscule amount of cpu power, but we have more cpu power and less ram on our servers.\nTo install uksmd on ubuntu, you need to switch kernels.\n\nCompiling UKSMD\nSteps to do so on Ubuntu 22 (you must have switched kernels):\nsudo apt-get install debhelper build-essential dh-make meson pkg-config libprocps-dev libcap-ng-dev # I think it can either be pkg-conf or pkg-config\ngit clone https://github.com/insilications/uksmd-clr\nRename the directory to be something compatible with below steps, like uksmd-1 before you cd into it.\nFollow steps from here\ndh_make --createorig\ndh_auto_configure --buildsystem=meson\ndpkg-buildpackage -rfakeroot -us -uc -b\nThe debian package will be build in the directory above the source directory.\nInstall your debian package!\nIf you want the uksmdstats command for monitoring purposes, you can only get it from the cachyos github (or make your own, it’s just a shell script).\nsudo curl https://raw.githubusercontent.com/CachyOS/uksmd/master/uksmdstats -o /usr/bin/uksmdstats\n\n\nSwitching Kernels\nstatus: complete\ncurl 'https://liquorix.net/install-liquorix.sh' | sudo bash from the liqourix kernel website\nI checked if liqourix has the necessary features, and yes it does.\n\nSetting the Default Kernel\nstatus: researching\nReddit post where I ask how to set default kernel in grub\nOn that reddit post, I talk about some flawed solutions I have found. Mainly they don’t seem to be truly persistent, not surviving kernel updates, updates of grub, or installation of new kernels. This endeavor is pretty risky, because a broken grub means we will have no choice but to delete our aws and start over. I need a 100% solution.\nAfter grilling chatgpt through 3 wrong answers, which chatgpt presented with the absolute confidence that an AI has, it finally presented me a solution that seems like it doesn’t have a risk of breaking the AWS system we are working on.\n\nI need to make some adjustments, but I should be able to select for the term “liqourix” while removing the term “recovery” to select the correct kernel (but not the recovery kernel) with complete consistency even through kernel updates, grub updates, or installation of new kernels.\nI searched around for how to do this, but I eventually gave up and asked chatgpt again, getting this:\n\nMy 20_linux_xen is not the same as what chatgpt wants, so I asked again, and it gave me this code to put in /etc/default/grub:\n# Set the default menu entry based on the title of the menu entry\n# that contains the word \"liqourix\" while ignoring the term \"recovery\"\nGRUB_DEFAULT=\"$(grep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}')\"\ngrep -E -o '^menuentry.*liqourix.*' /boot/grub/grub.cfg | grep -v 'recovery' | head -1 | awk -F\\' '{print $2}'\nI will test this in a virtual machine.\n\n\n\nWeaker alternative: ksm_preload\nstatus: won’t be used\ngit clone https://github.com/binfess/ksm_preload\ncmake .\nmake\nsudo make install\nI added LD_PRELOAD=/usr/local/share/ksm_preload/libksm_preload.so to the file /etc/environment\nI haven’t tested the above, but I saw very minimal space saved, only about 0.11 **megabytes* saved, on my desktop. Tests on my sample server are similarly discouraging:\n\nThe above was with 2 kasm sessions open. Nearly useless. In addition to that, uksmd seems to make this completely obsolete."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#zram",
    "href": "projects/setting-up-kasm/index.html#zram",
    "title": "Kasmweb setup",
    "section": "zram",
    "text": "zram\nstatus: immplemented on my personal systems, but not on the ubuntu vm yet.\nTo install zram, sudo apt install systemd-zram-generator\nThen, you can configure zram by editing /etc/systemd/zram-generator.conf\nThis works, but apparently, things in a swap file aren’t deduplicated by uksmd. Rather, zram handles it’s own deduplication, with the compression algorithms. Becuase it must hash pages, this can potentially lead to more cpu usage.\nBecause I don’t know about this, I made yet another reddit post asking about this topic."
  },
  {
    "objectID": "projects/setting-up-kasm/index.html#zswap",
    "href": "projects/setting-up-kasm/index.html#zswap",
    "title": "Kasmweb setup",
    "section": "zswap",
    "text": "zswap\nStatus: Partially done, but dropped in favor of zswap\n\nzswap is disabled by default on my ubuntu virtual machine. Odd that both are disabled by default.\nParameters for zswap can be found in /sys/module/zswap/parameters/\nTo set parameters at boot, use kernel boot paremeters, like zswap.enabled=1 zswap.compressor=lz4 zswap.max_pool_percent=20 zswap.zpool=z3fold\nI will need to tinker to see what is the most optimized zswap setup\nzswap has it’s own memory deduplication feature, which is enabled by default on both my ubuntu vm and the aws ubuntu server:\nSee above, my comments about zswap’s memory deduplication feature."
  }
]